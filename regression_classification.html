<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Regression and classification | Applied Geodata Science</title>
  <meta name="description" content="This course prepares you to benefit from the general data richness in environmental and geo-sciences." />
  <meta name="generator" content="bookdown 0.31 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Regression and classification | Applied Geodata Science" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This course prepares you to benefit from the general data richness in environmental and geo-sciences." />
  <meta name="github-repo" content="stineb/agsd" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Regression and classification | Applied Geodata Science" />
  
  <meta name="twitter:description" content="This course prepares you to benefit from the general data richness in environmental and geo-sciences." />
  

<meta name="author" content="Benjamin Stocker (lead), Koen Hufkens (contributing), Pepa Aran (contributing), Pascal Schneider (contributing)" />


<meta name="date" content="2023-02-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="open_science.html"/>
<link rel="next" href="supervised_ml_i.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied Geodata Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this book</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#target-readers-of-this-book"><i class="fa fa-check"></i><b>0.1</b> Target readers of this book</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="0.2" data-path="introduction.html"><a href="introduction.html#what-is-applied-geodata-science"><i class="fa fa-check"></i><b>0.2</b> What is Applied Geodata Science?</a></li>
<li class="chapter" data-level="0.3" data-path="introduction.html"><a href="introduction.html#data-science-workflow"><i class="fa fa-check"></i><b>0.3</b> The (data) science workflow</a></li>
<li class="chapter" data-level="0.4" data-path="introduction.html"><a href="introduction.html#data-rich-era"><i class="fa fa-check"></i><b>0.4</b> Why now?</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="getting_started.html"><a href="getting_started.html"><i class="fa fa-check"></i><b>1</b> Getting started</a>
<ul>
<li class="chapter" data-level="1.1" data-path="getting_started.html"><a href="getting_started.html#learning-objectives"><i class="fa fa-check"></i><b>1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="1.2" data-path="getting_started.html"><a href="getting_started.html#tutorial"><i class="fa fa-check"></i><b>1.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="getting_started.html"><a href="getting_started.html#working-with-r-and-rstudio"><i class="fa fa-check"></i><b>1.2.1</b> Working with R and RStudio</a></li>
<li class="chapter" data-level="1.2.2" data-path="getting_started.html"><a href="getting_started.html#r-objects"><i class="fa fa-check"></i><b>1.2.2</b> R objects</a></li>
<li class="chapter" data-level="1.2.3" data-path="getting_started.html"><a href="getting_started.html#r-scripts"><i class="fa fa-check"></i><b>1.2.3</b> R scripts</a></li>
<li class="chapter" data-level="1.2.4" data-path="getting_started.html"><a href="getting_started.html#workspace-management"><i class="fa fa-check"></i><b>1.2.4</b> Workspace management</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="getting_started.html"><a href="getting_started.html#exercises"><i class="fa fa-check"></i><b>1.3</b> Exercises</a></li>
<li class="chapter" data-level="1.4" data-path="getting_started.html"><a href="getting_started.html#solutions"><i class="fa fa-check"></i><b>1.4</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="programming_primers.html"><a href="programming_primers.html"><i class="fa fa-check"></i><b>2</b> Programming primers</a>
<ul>
<li class="chapter" data-level="2.1" data-path="programming_primers.html"><a href="programming_primers.html#learning-objectives-1"><i class="fa fa-check"></i><b>2.1</b> Learning objectives</a></li>
<li class="chapter" data-level="2.2" data-path="programming_primers.html"><a href="programming_primers.html#tutorial-1"><i class="fa fa-check"></i><b>2.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="programming_primers.html"><a href="programming_primers.html#libraries"><i class="fa fa-check"></i><b>2.2.1</b> Libraries</a></li>
<li class="chapter" data-level="2.2.2" data-path="programming_primers.html"><a href="programming_primers.html#programming-basics"><i class="fa fa-check"></i><b>2.2.2</b> Programming basics</a></li>
<li class="chapter" data-level="2.2.3" data-path="programming_primers.html"><a href="programming_primers.html#working-with-data-frames"><i class="fa fa-check"></i><b>2.2.3</b> Working with data frames</a></li>
<li class="chapter" data-level="2.2.4" data-path="programming_primers.html"><a href="programming_primers.html#where-to-find-help"><i class="fa fa-check"></i><b>2.2.4</b> Where to find help</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="programming_primers.html"><a href="programming_primers.html#exercises-1"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
<li class="chapter" data-level="2.4" data-path="programming_primers.html"><a href="programming_primers.html#solutions-1"><i class="fa fa-check"></i><b>2.4</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data_wrangling.html"><a href="data_wrangling.html"><i class="fa fa-check"></i><b>3</b> Data wrangling</a>
<ul>
<li class="chapter" data-level="3.1" data-path="data_wrangling.html"><a href="data_wrangling.html#learning-objectives-2"><i class="fa fa-check"></i><b>3.1</b> Learning objectives</a></li>
<li class="chapter" data-level="3.2" data-path="data_wrangling.html"><a href="data_wrangling.html#tutorial-2"><i class="fa fa-check"></i><b>3.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="data_wrangling.html"><a href="data_wrangling.html#example-data"><i class="fa fa-check"></i><b>3.2.1</b> Example data</a></li>
<li class="chapter" data-level="3.2.2" data-path="data_wrangling.html"><a href="data_wrangling.html#required-libraries"><i class="fa fa-check"></i><b>3.2.2</b> Required libraries</a></li>
<li class="chapter" data-level="3.2.3" data-path="data_wrangling.html"><a href="data_wrangling.html#tidyverse"><i class="fa fa-check"></i><b>3.2.3</b> Tidyverse</a></li>
<li class="chapter" data-level="3.2.4" data-path="data_wrangling.html"><a href="data_wrangling.html#tabular-data"><i class="fa fa-check"></i><b>3.2.4</b> Tabular data</a></li>
<li class="chapter" data-level="3.2.5" data-path="data_wrangling.html"><a href="data_wrangling.html#variable-selection"><i class="fa fa-check"></i><b>3.2.5</b> Variable selection</a></li>
<li class="chapter" data-level="3.2.6" data-path="data_wrangling.html"><a href="data_wrangling.html#time-objects"><i class="fa fa-check"></i><b>3.2.6</b> Time objects</a></li>
<li class="chapter" data-level="3.2.7" data-path="data_wrangling.html"><a href="data_wrangling.html#variable-re--definition"><i class="fa fa-check"></i><b>3.2.7</b> Variable (re-) definition</a></li>
<li class="chapter" data-level="3.2.8" data-path="data_wrangling.html"><a href="data_wrangling.html#axes-of-variation"><i class="fa fa-check"></i><b>3.2.8</b> Axes of variation</a></li>
<li class="chapter" data-level="3.2.9" data-path="data_wrangling.html"><a href="data_wrangling.html#tidy-data"><i class="fa fa-check"></i><b>3.2.9</b> Tidy data</a></li>
<li class="chapter" data-level="3.2.10" data-path="data_wrangling.html"><a href="data_wrangling.html#aggregating-data"><i class="fa fa-check"></i><b>3.2.10</b> Aggregating data</a></li>
<li class="chapter" data-level="3.2.11" data-path="data_wrangling.html"><a href="data_wrangling.html#data-cleaning"><i class="fa fa-check"></i><b>3.2.11</b> Data cleaning</a></li>
<li class="chapter" data-level="3.2.12" data-path="data_wrangling.html"><a href="data_wrangling.html#combining-relational-data"><i class="fa fa-check"></i><b>3.2.12</b> Combining relational data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="data_wrangling.html"><a href="data_wrangling.html#extra-material"><i class="fa fa-check"></i><b>3.3</b> Extra material</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="data_wrangling.html"><a href="data_wrangling.html#functional-programming-i"><i class="fa fa-check"></i><b>3.3.1</b> Functional programming I</a></li>
<li class="chapter" data-level="3.3.2" data-path="data_wrangling.html"><a href="data_wrangling.html#strings"><i class="fa fa-check"></i><b>3.3.2</b> Strings</a></li>
<li class="chapter" data-level="3.3.3" data-path="data_wrangling.html"><a href="data_wrangling.html#functional-programming-ii"><i class="fa fa-check"></i><b>3.3.3</b> Functional programming II</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="data_wrangling.html"><a href="data_wrangling.html#exercises-2"><i class="fa fa-check"></i><b>3.4</b> Exercises</a></li>
<li class="chapter" data-level="3.5" data-path="data_wrangling.html"><a href="data_wrangling.html#solutions-2"><i class="fa fa-check"></i><b>3.5</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data_vis.html"><a href="data_vis.html"><i class="fa fa-check"></i><b>4</b> Data visualisation</a>
<ul>
<li class="chapter" data-level="4.1" data-path="data_vis.html"><a href="data_vis.html#learning-objectives-3"><i class="fa fa-check"></i><b>4.1</b> Learning objectives</a></li>
<li class="chapter" data-level="4.2" data-path="data_vis.html"><a href="data_vis.html#required-packages"><i class="fa fa-check"></i><b>4.2</b> Required packages</a></li>
<li class="chapter" data-level="4.3" data-path="data_vis.html"><a href="data_vis.html#tutorial-3"><i class="fa fa-check"></i><b>4.3</b> Tutorial</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="data_vis.html"><a href="data_vis.html#the-grammar-of-graphics"><i class="fa fa-check"></i><b>4.3.1</b> The grammar of graphics</a></li>
<li class="chapter" data-level="4.3.2" data-path="data_vis.html"><a href="data_vis.html#every-data-has-its-representation"><i class="fa fa-check"></i><b>4.3.2</b> Every data has its representation</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="data_vis.html"><a href="data_vis.html#exercises-3"><i class="fa fa-check"></i><b>4.4</b> Exercises</a></li>
<li class="chapter" data-level="4.5" data-path="data_vis.html"><a href="data_vis.html#solutions-3"><i class="fa fa-check"></i><b>4.5</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data_variety.html"><a href="data_variety.html"><i class="fa fa-check"></i><b>5</b> Data variety</a>
<ul>
<li class="chapter" data-level="5.1" data-path="data_variety.html"><a href="data_variety.html#learning-objectives-4"><i class="fa fa-check"></i><b>5.1</b> Learning objectives</a></li>
<li class="chapter" data-level="5.2" data-path="data_variety.html"><a href="data_variety.html#tutorial-4"><i class="fa fa-check"></i><b>5.2</b> Tutorial</a></li>
<li class="chapter" data-level="5.3" data-path="data_variety.html"><a href="data_variety.html#files-and-file-formats"><i class="fa fa-check"></i><b>5.3</b> Files and file formats</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="data_variety.html"><a href="data_variety.html#file-extensions"><i class="fa fa-check"></i><b>5.3.1</b> File extensions</a></li>
<li class="chapter" data-level="5.3.2" data-path="data_variety.html"><a href="data_variety.html#file-formats"><i class="fa fa-check"></i><b>5.3.2</b> File formats</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="data_variety.html"><a href="data_variety.html#meta-data"><i class="fa fa-check"></i><b>5.4</b> Meta-data</a></li>
<li class="chapter" data-level="5.5" data-path="data_variety.html"><a href="data_variety.html#spatial-data-representation"><i class="fa fa-check"></i><b>5.5</b> Spatial data representation</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="data_variety.html"><a href="data_variety.html#raster-data-model"><i class="fa fa-check"></i><b>5.5.1</b> Raster data model</a></li>
<li class="chapter" data-level="5.5.2" data-path="data_variety.html"><a href="data_variety.html#vector-data-model"><i class="fa fa-check"></i><b>5.5.2</b> Vector data model</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="data_variety.html"><a href="data_variety.html#data-sources"><i class="fa fa-check"></i><b>5.6</b> Data sources</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="data_variety.html"><a href="data_variety.html#direct-downloads"><i class="fa fa-check"></i><b>5.6.1</b> Direct downloads</a></li>
<li class="chapter" data-level="5.6.2" data-path="data_variety.html"><a href="data_variety.html#apis"><i class="fa fa-check"></i><b>5.6.2</b> APIs</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="data_variety.html"><a href="data_variety.html#exercises-4"><i class="fa fa-check"></i><b>5.7</b> Exercises</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="data_variety.html"><a href="data_variety.html#files-and-file-formats-1"><i class="fa fa-check"></i><b>5.7.1</b> Files and file formats</a></li>
<li class="chapter" data-level="5.7.2" data-path="data_variety.html"><a href="data_variety.html#api-use"><i class="fa fa-check"></i><b>5.7.2</b> API use</a></li>
<li class="chapter" data-level="5.7.3" data-path="data_variety.html"><a href="data_variety.html#get-1"><i class="fa fa-check"></i><b>5.7.3</b> GET</a></li>
<li class="chapter" data-level="5.7.4" data-path="data_variety.html"><a href="data_variety.html#dedicated-library"><i class="fa fa-check"></i><b>5.7.4</b> Dedicated library</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="data_variety.html"><a href="data_variety.html#solutions-4"><i class="fa fa-check"></i><b>5.8</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="code_mgmt.html"><a href="code_mgmt.html"><i class="fa fa-check"></i><b>6</b> Code management</a>
<ul>
<li class="chapter" data-level="6.1" data-path="code_mgmt.html"><a href="code_mgmt.html#learning-objectives-5"><i class="fa fa-check"></i><b>6.1</b> Learning objectives</a></li>
<li class="chapter" data-level="6.2" data-path="code_mgmt.html"><a href="code_mgmt.html#tutorial-5"><i class="fa fa-check"></i><b>6.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="code_mgmt.html"><a href="code_mgmt.html#git-and-local-version-control"><i class="fa fa-check"></i><b>6.2.1</b> Git and local version control</a></li>
<li class="chapter" data-level="6.2.2" data-path="code_mgmt.html"><a href="code_mgmt.html#remote-version-control"><i class="fa fa-check"></i><b>6.2.2</b> Remote version control</a></li>
<li class="chapter" data-level="6.2.3" data-path="code_mgmt.html"><a href="code_mgmt.html#location-based-code-management---github-templates"><i class="fa fa-check"></i><b>6.2.3</b> Location based code management - github templates</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="code_mgmt.html"><a href="code_mgmt.html#exercises-5"><i class="fa fa-check"></i><b>6.3</b> Exercises</a></li>
<li class="chapter" data-level="6.4" data-path="code_mgmt.html"><a href="code_mgmt.html#solutions-5"><i class="fa fa-check"></i><b>6.4</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="open_science.html"><a href="open_science.html"><i class="fa fa-check"></i><b>7</b> Open science practices</a>
<ul>
<li class="chapter" data-level="7.1" data-path="open_science.html"><a href="open_science.html#learning-objectives-6"><i class="fa fa-check"></i><b>7.1</b> Learning objectives</a></li>
<li class="chapter" data-level="7.2" data-path="open_science.html"><a href="open_science.html#tutorial-6"><i class="fa fa-check"></i><b>7.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="open_science.html"><a href="open_science.html#project-structure"><i class="fa fa-check"></i><b>7.2.1</b> Project structure</a></li>
<li class="chapter" data-level="7.2.2" data-path="open_science.html"><a href="open_science.html#managing-workflows"><i class="fa fa-check"></i><b>7.2.2</b> Managing workflows</a></li>
<li class="chapter" data-level="7.2.3" data-path="open_science.html"><a href="open_science.html#capturing-your-session-state"><i class="fa fa-check"></i><b>7.2.3</b> Capturing your session state</a></li>
<li class="chapter" data-level="7.2.4" data-path="open_science.html"><a href="open_science.html#capturing-a-system-state"><i class="fa fa-check"></i><b>7.2.4</b> Capturing a system state</a></li>
<li class="chapter" data-level="7.2.5" data-path="open_science.html"><a href="open_science.html#readable-reporting-using-rmarkdown"><i class="fa fa-check"></i><b>7.2.5</b> Readable reporting using Rmarkdown</a></li>
<li class="chapter" data-level="7.2.6" data-path="open_science.html"><a href="open_science.html#data-retention"><i class="fa fa-check"></i><b>7.2.6</b> Data retention</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="open_science.html"><a href="open_science.html#exercises-6"><i class="fa fa-check"></i><b>7.3</b> Exercises</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="open_science.html"><a href="open_science.html#data-and-code-management"><i class="fa fa-check"></i><b>7.3.1</b> Data and code management</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="open_science.html"><a href="open_science.html#references"><i class="fa fa-check"></i><b>7.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regression_classification.html"><a href="regression_classification.html"><i class="fa fa-check"></i><b>8</b> Regression and classification</a>
<ul>
<li class="chapter" data-level="8.1" data-path="regression_classification.html"><a href="regression_classification.html#learning-objectives-7"><i class="fa fa-check"></i><b>8.1</b> Learning objectives</a></li>
<li class="chapter" data-level="8.2" data-path="regression_classification.html"><a href="regression_classification.html#tutorial-7"><i class="fa fa-check"></i><b>8.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="regression_classification.html"><a href="regression_classification.html#types-of-models"><i class="fa fa-check"></i><b>8.2.1</b> Types of models</a></li>
<li class="chapter" data-level="8.2.2" data-path="regression_classification.html"><a href="regression_classification.html#regression"><i class="fa fa-check"></i><b>8.2.2</b> Regression</a></li>
<li class="chapter" data-level="8.2.3" data-path="regression_classification.html"><a href="regression_classification.html#classification"><i class="fa fa-check"></i><b>8.2.3</b> Classification</a></li>
<li class="chapter" data-level="8.2.4" data-path="regression_classification.html"><a href="regression_classification.html#model-evaluation"><i class="fa fa-check"></i><b>8.2.4</b> Model evaluation</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="regression_classification.html"><a href="regression_classification.html#exercises-7"><i class="fa fa-check"></i><b>8.3</b> Exercises</a></li>
<li class="chapter" data-level="8.4" data-path="regression_classification.html"><a href="regression_classification.html#solutions-6"><i class="fa fa-check"></i><b>8.4</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="supervised_ml_i.html"><a href="supervised_ml_i.html"><i class="fa fa-check"></i><b>9</b> Supervised machine learning I</a>
<ul>
<li class="chapter" data-level="9.1" data-path="supervised_ml_i.html"><a href="supervised_ml_i.html#learning-objectives-8"><i class="fa fa-check"></i><b>9.1</b> Learning objectives</a></li>
<li class="chapter" data-level="9.2" data-path="supervised_ml_i.html"><a href="supervised_ml_i.html#tutorial-8"><i class="fa fa-check"></i><b>9.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="supervised_ml_i.html"><a href="supervised_ml_i.html#required-packages-1"><i class="fa fa-check"></i><b>9.2.1</b> Required packages</a></li>
<li class="chapter" data-level="9.2.2" data-path="supervised_ml_i.html"><a href="supervised_ml_i.html#overfitting"><i class="fa fa-check"></i><b>9.2.2</b> Overfitting</a></li>
<li class="chapter" data-level="9.2.3" data-path="supervised_ml_i.html"><a href="supervised_ml_i.html#data-and-the-modelling-challenge"><i class="fa fa-check"></i><b>9.2.3</b> Data and the modelling challenge</a></li>
<li class="chapter" data-level="9.2.4" data-path="supervised_ml_i.html"><a href="supervised_ml_i.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>9.2.4</b> K-nearest neighbours</a></li>
<li class="chapter" data-level="9.2.5" data-path="supervised_ml_i.html"><a href="supervised_ml_i.html#model-formulation"><i class="fa fa-check"></i><b>9.2.5</b> Model formulation</a></li>
<li class="chapter" data-level="9.2.6" data-path="supervised_ml_i.html"><a href="supervised_ml_i.html#data-splitting"><i class="fa fa-check"></i><b>9.2.6</b> Data splitting</a></li>
<li class="chapter" data-level="9.2.7" data-path="supervised_ml_i.html"><a href="supervised_ml_i.html#preprocessing"><i class="fa fa-check"></i><b>9.2.7</b> Pre-processing</a></li>
<li class="chapter" data-level="9.2.8" data-path="supervised_ml_i.html"><a href="supervised_ml_i.html#putting-it-all-together-half-way"><i class="fa fa-check"></i><b>9.2.8</b> Putting it all together (half-way)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="supervised_ml_i.html"><a href="supervised_ml_i.html#exercises-8"><i class="fa fa-check"></i><b>9.3</b> Exercises</a></li>
<li class="chapter" data-level="9.4" data-path="supervised_ml_i.html"><a href="supervised_ml_i.html#solutions-7"><i class="fa fa-check"></i><b>9.4</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="supervised_ml_ii.html"><a href="supervised_ml_ii.html"><i class="fa fa-check"></i><b>10</b> Supervised machine learning II</a>
<ul>
<li class="chapter" data-level="10.1" data-path="supervised_ml_ii.html"><a href="supervised_ml_ii.html#learning-objectives-9"><i class="fa fa-check"></i><b>10.1</b> Learning objectives</a></li>
<li class="chapter" data-level="10.2" data-path="supervised_ml_ii.html"><a href="supervised_ml_ii.html#tutorial-9"><i class="fa fa-check"></i><b>10.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="supervised_ml_ii.html"><a href="supervised_ml_ii.html#training"><i class="fa fa-check"></i><b>10.2.1</b> Model training</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="supervised_ml_ii.html"><a href="supervised_ml_ii.html#exercises-9"><i class="fa fa-check"></i><b>10.3</b> Exercises</a></li>
<li class="chapter" data-level="10.4" data-path="supervised_ml_ii.html"><a href="supervised_ml_ii.html#solutions-8"><i class="fa fa-check"></i><b>10.4</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="neural_nets.html"><a href="neural_nets.html"><i class="fa fa-check"></i><b>11</b> Neural networks</a>
<ul>
<li class="chapter" data-level="11.1" data-path="neural_nets.html"><a href="neural_nets.html#learning-objectives-10"><i class="fa fa-check"></i><b>11.1</b> Learning objectives</a></li>
<li class="chapter" data-level="11.2" data-path="neural_nets.html"><a href="neural_nets.html#tutorial-10"><i class="fa fa-check"></i><b>11.2</b> Tutorial</a></li>
<li class="chapter" data-level="11.3" data-path="neural_nets.html"><a href="neural_nets.html#exercises-10"><i class="fa fa-check"></i><b>11.3</b> Exercises</a></li>
<li class="chapter" data-level="11.4" data-path="neural_nets.html"><a href="neural_nets.html#solutions-9"><i class="fa fa-check"></i><b>11.4</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="interpretable_ml.html"><a href="interpretable_ml.html"><i class="fa fa-check"></i><b>12</b> Interpretable machine learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="interpretable_ml.html"><a href="interpretable_ml.html#learning-objectives-11"><i class="fa fa-check"></i><b>12.1</b> Learning objectives</a></li>
<li class="chapter" data-level="12.2" data-path="interpretable_ml.html"><a href="interpretable_ml.html#tutorial-11"><i class="fa fa-check"></i><b>12.2</b> Tutorial</a></li>
<li class="chapter" data-level="12.3" data-path="interpretable_ml.html"><a href="interpretable_ml.html#exercises-11"><i class="fa fa-check"></i><b>12.3</b> Exercises</a></li>
<li class="chapter" data-level="12.4" data-path="interpretable_ml.html"><a href="interpretable_ml.html#solutions-10"><i class="fa fa-check"></i><b>12.4</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied Geodata Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression_classification" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Regression and classification<a href="regression_classification.html#regression_classification" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><strong>Chapter lead author: Pepa Aran</strong></p>
<p>Contents:</p>
<ul>
<li>Difference between regression and classification</li>
<li><a href="https://github.com/stineb/esds_book-gitlab/blob/e27fbc390fa302bc99be3d1796c4e75dc6ee1c41/06_supervised_ml_1.Rmd#L84">Linear regression</a></li>
<li>Logistic regression</li>
<li><a href="https://github.com/stineb/esds_book-gitlab/blob/e27fbc390fa302bc99be3d1796c4e75dc6ee1c41/07_supervised_ml_2.Rmd#L236">Regression metrics</a></li>
<li><a href="https://github.com/stineb/esds_book-gitlab/blob/e27fbc390fa302bc99be3d1796c4e75dc6ee1c41/07_supervised_ml_2.Rmd#L345">Classification metrics</a>, <a href="https://bookdown.org/max/FES/measuring-performance.html#class-metrics">another link</a></li>
<li>Comparing models (AIC, …)</li>
<li>Detecting outliers: identification via distributions [hist(), boxplot(), qqnorm()], multivariate (Cook’s Distance to get influential values)</li>
<li>Feature selection, <a href="https://github.com/stineb/esds_book-gitlab/blob/master/08_variable_selection.Rmd">stepwise regression</a>, multi-colinearity (<a href="http://www.sthda.com/english/articles/39-regression-model-diagnostics/160-multicollinearity-essentials-and-vif-in-r/">vif</a>)</li>
</ul>
<div id="learning-objectives-7" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Learning objectives<a href="regression_classification.html#learning-objectives-7" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>After completing this tutorial, you will be able to:</p>
<ul>
<li>Understand the basics of regression and classification models</li>
<li>Fit linear and logistic regression models in R</li>
<li>Choose and calculate relevant model performance metrics</li>
<li>Evaluate and compare regression models</li>
<li>Detect data outliers</li>
<li>Select best predictive variables</li>
</ul>
</div>
<div id="tutorial-7" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Tutorial<a href="regression_classification.html#tutorial-7" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="regression_classification.html#cb298-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load necessary packages</span></span>
<span id="cb298-2"><a href="regression_classification.html#cb298-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb298-3"><a href="regression_classification.html#cb298-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb298-4"><a href="regression_classification.html#cb298-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb298-5"><a href="regression_classification.html#cb298-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lubridate)</span>
<span id="cb298-6"><a href="regression_classification.html#cb298-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb298-7"><a href="regression_classification.html#cb298-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb298-8"><a href="regression_classification.html#cb298-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(broom)</span></code></pre></div>
<div id="types-of-models" class="section level3 hasAnchor" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Types of models<a href="regression_classification.html#types-of-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Models try to explain relationships between variables through a mathematical formulation, particularly to predict a given target variable using other explanatory variables, also called predictors. Generally, we say that the target variable <span class="math inline">\(Y\)</span> is a function (denoted <span class="math inline">\(f\)</span>) of a set of explanatory variables <span class="math inline">\(X_1, X_2, \dots, X_p\)</span> and some model parameters <span class="math inline">\(\beta\)</span>. Models can be represented as:
<span class="math display">\[Y \sim f(X_1, X_2, \dots, X_p, \beta)\]</span></p>
<p>This is a very general notation and depending on the structure of these components, we get to different modelling approaches.</p>
<p>The first distinction comes from the type of target variable. Whenever <span class="math inline">\(Y\)</span> is a continuous variable, we are facing a <em>regression</em> problem. If <span class="math inline">\(Y\)</span> is categorical, we talk about <em>classification</em>.</p>
<table>
<colgroup>
<col width="30%" />
<col width="34%" />
<col width="34%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Regression</th>
<th>Classification</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Target variable</td>
<td>Continuous</td>
<td>Categorical</td>
</tr>
<tr class="even">
<td>Common models</td>
<td>Linear regression, polynomial regression, kNN, tree-based regression</td>
<td>Logistic regression, kNN, SVM, tree classifiers</td>
</tr>
<tr class="odd">
<td>Metrics</td>
<td>RMSE, <span class="math inline">\(R^2\)</span>, adjusted <span class="math inline">\(R^2\)</span>, AIC, BIC</td>
<td>Accuracy, precision, AUC, F1</td>
</tr>
</tbody>
</table>
</div>
<div id="regression" class="section level3 hasAnchor" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Regression<a href="regression_classification.html#regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we will introduce the most basic regression model: linear regression. We’ll explain how to fit the model with R, how to include categorical predictors and polynomial terms. Finally, several performance metrics for regression models are presented.</p>
<div id="linear-regression" class="section level4 hasAnchor" number="8.2.2.1">
<h4><span class="header-section-number">8.2.2.1</span> Linear regression<a href="regression_classification.html#linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Theory</strong></p>
<p>Let’s start with the simplest model: linear regression. You probably have studied linear regression from a statistical perspective, here we will take a data-fitting approach.</p>
<p>For example, we can try to explain the relationship between GPP and short wave radiation, like in the <a href="data_vis.html#data_vis">visualisation tutorial</a>. The figure below shows a cloud of data points, and a straight line predicting GPP based on observed shortwave radiation values.</p>
<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb299-1"><a href="regression_classification.html#cb299-1" aria-hidden="true" tabindex="-1"></a><span class="co"># read and format data from Ch 3</span></span>
<span id="cb299-2"><a href="regression_classification.html#cb299-2" aria-hidden="true" tabindex="-1"></a>hhdf <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(<span class="st">&quot;./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN.csv&quot;</span>)</span></code></pre></div>
<pre><code>## Rows: 52608 Columns: 20
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## dbl  (18): TA_F, SW_IN_F, LW_IN_F, VPD_F, PA_F, P_F, WS_F, GPP_NT_VUT_REF, N...
## dttm  (2): TIMESTAMP_START, TIMESTAMP_END
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb301-1"><a href="regression_classification.html#cb301-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2023</span>)</span>
<span id="cb301-2"><a href="regression_classification.html#cb301-2" aria-hidden="true" tabindex="-1"></a>gg1 <span class="ot">&lt;-</span> hhdf <span class="sc">|&gt;</span></span>
<span id="cb301-3"><a href="regression_classification.html#cb301-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sample_n</span>(<span class="dv">2000</span>) <span class="sc">|&gt;</span>  <span class="co"># to reduce the dataset</span></span>
<span id="cb301-4"><a href="regression_classification.html#cb301-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> SW_IN_F, <span class="at">y =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb301-5"><a href="regression_classification.html#cb301-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.75</span>, <span class="at">alpha=</span><span class="fl">0.4</span>) <span class="sc">+</span></span>
<span id="cb301-6"><a href="regression_classification.html#cb301-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb301-7"><a href="regression_classification.html#cb301-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Shortwave radiation (W m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;)&quot;</span>)), </span>
<span id="cb301-8"><a href="regression_classification.html#cb301-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;GPP (gC m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;s&quot;</span><span class="sc">^-</span><span class="dv">1</span>, <span class="st">&quot;)&quot;</span>))) <span class="sc">+</span></span>
<span id="cb301-9"><a href="regression_classification.html#cb301-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span>
<span id="cb301-10"><a href="regression_classification.html#cb301-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb301-11"><a href="regression_classification.html#cb301-11" aria-hidden="true" tabindex="-1"></a>segment_points <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x0 =</span> <span class="dv">332</span>, <span class="at">y0 =</span> <span class="fl">3.65</span>, <span class="at">y_regr =</span> <span class="fl">8.77</span>)</span>
<span id="cb301-12"><a href="regression_classification.html#cb301-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb301-13"><a href="regression_classification.html#cb301-13" aria-hidden="true" tabindex="-1"></a>gg1 <span class="sc">+</span></span>
<span id="cb301-14"><a href="regression_classification.html#cb301-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> x0, <span class="at">y =</span> y0, <span class="at">xend =</span> x0, <span class="at">yend =</span> y_regr), </span>
<span id="cb301-15"><a href="regression_classification.html#cb301-15" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> segment_points,</span>
<span id="cb301-16"><a href="regression_classification.html#cb301-16" aria-hidden="true" tabindex="-1"></a>               <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="fl">1.2</span>, <span class="at">alpha =</span> <span class="fl">0.8</span>)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-150-1.png" width="672" /></p>
<p>We want to find the best straight line that approximates a cloud of data points. For this, we assume a linear relationship between a single explanatory variable <span class="math inline">\(X\)</span> and our target <span class="math inline">\(Y\)</span>: <span class="math display">\[
Y_i \sim \beta_0 + \beta_1 X_i, \;\;\; i = 1, 2, ...n \;,
\]</span> where <span class="math inline">\(Y_i\)</span> is the i-th observation of the target variable, and <span class="math inline">\(X_i\)</span> is the i-th value of the (single) predictor variable. <span class="math inline">\(n\)</span> is the number of observations we have and <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are constant coefficients (model parameters). We call <span class="math inline">\(\beta_0\)</span> the intercept and <span class="math inline">\(\beta_1\)</span> the slope of the regression line. Generally, <span class="math inline">\(\hat{Y}\)</span> denotes the model prediction.</p>
<p>Fitting a linear regression is finding the values for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> such that, on average over all points, the distance between the line at <span class="math inline">\(X_i\)</span>, that is <span class="math inline">\(\beta_0 + \beta_1 X_i\)</span> (in blue), and the observed value <span class="math inline">\(Y_i\)</span>, is as small as possible. Mathematically, this is minimizing the sum of the square errors, that is:
<span class="math display">\[ \min_{\beta_0, \beta_1} \sum_i (Y_i - \beta_0 - \beta_1 X_i)^2 .\]</span></p>
<p>This linear model can be used to make predictions on new data, which are obtained by <span class="math inline">\(\hat{Y}_\text{new} = \beta_0 + \beta_1 X_\text{new}\)</span>. When the new data comes from the same distribution as the data used to fit the regression line, this should be a good prediction.</p>
<p>It’s not hard to imagine that the univariate linear regression can be generalized to a multivariate linear regression, where we assume that the target variable is a linear combination of <span class="math inline">\(p\)</span> predictor variables:
<span class="math display">\[Y \sim \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \; ... \; + \beta_p X_p \;.\]</span>
Note that here, <span class="math inline">\(X_1, \dots, X_p\)</span> and <span class="math inline">\(Y\)</span> are vectors of length corresponding to the number of observations in our data set (<span class="math inline">\(n\)</span> - as above). Analogously, calibrating the <span class="math inline">\(p+1\)</span> coefficients <span class="math inline">\(\beta_0, \beta_1, \beta_2, ..., \beta_p\)</span> is to minimize the sum of square errors
<span class="math inline">\(\min_{\beta} \sum_i (Y_i - \hat{Y}_i)^2\)</span>.</p>
<p>While the regression is a line in two-dimensional space for the univariate case, it is a plane in three-dimensional space for bi-variate regression, and hyperplanes in higher dimensions.</p>
<p><strong>Implementation in R</strong></p>
<p>To fit a univariate linear regression model in R, we can use the <code>lm()</code> function. Already in Chapter <a href="data_wrangling.html#data_wrangling">3</a>, we made linear models by doing:</p>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb303-1"><a href="regression_classification.html#cb303-1" aria-hidden="true" tabindex="-1"></a><span class="co"># numerical variables only, remove NA</span></span>
<span id="cb303-2"><a href="regression_classification.html#cb303-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> hhdf <span class="sc">%&gt;%</span></span>
<span id="cb303-3"><a href="regression_classification.html#cb303-3" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span><span class="fu">starts_with</span>(<span class="st">&quot;TIMESTAMP&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb303-4"><a href="regression_classification.html#cb303-4" aria-hidden="true" tabindex="-1"></a>  tidyr<span class="sc">::</span><span class="fu">drop_na</span>()</span>
<span id="cb303-5"><a href="regression_classification.html#cb303-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb303-6"><a href="regression_classification.html#cb303-6" aria-hidden="true" tabindex="-1"></a><span class="co"># fit univariate linear regression</span></span>
<span id="cb303-7"><a href="regression_classification.html#cb303-7" aria-hidden="true" tabindex="-1"></a>linmod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> SW_IN_F, <span class="at">data =</span> df)</span></code></pre></div>
<p>Here, <code>GPP_NT_VUT_REF</code> is <span class="math inline">\(Y\)</span>, and <code>SW_IN_F</code> is <span class="math inline">\(X\)</span>. We can include multiple predictors for a multivariate regression, for example as:</p>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="regression_classification.html#cb304-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit multivariate linear regression</span></span>
<span id="cb304-2"><a href="regression_classification.html#cb304-2" aria-hidden="true" tabindex="-1"></a>linmod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> SW_IN_F <span class="sc">+</span> VPD_F <span class="sc">+</span> TA_F, <span class="at">data =</span> df)</span></code></pre></div>
<p>or all available features in our data set (all columns other than <code>GPP_NT_VUT_REF</code> in <code>df</code>) as:</p>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb305-1"><a href="regression_classification.html#cb305-1" aria-hidden="true" tabindex="-1"></a>linmod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> ., <span class="at">data =</span> df)</span></code></pre></div>
<p><code>linmod*</code> is now a model object of class <code>"lm"</code>. It is a list containing the following components:</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="regression_classification.html#cb306-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ls</span>(linmod1)</span></code></pre></div>
<pre><code>##  [1] &quot;assign&quot;        &quot;call&quot;          &quot;coefficients&quot;  &quot;df.residual&quot;  
##  [5] &quot;effects&quot;       &quot;fitted.values&quot; &quot;model&quot;         &quot;qr&quot;           
##  [9] &quot;rank&quot;          &quot;residuals&quot;     &quot;terms&quot;         &quot;xlevels&quot;</code></pre>
<p>Enter <code>?lm</code> in the console for a complete documentation of these components and other details of the linear model implementation.</p>
<p>R offers a set of generic functions that work with this type of object. The following returns a human-readable report of the fit. Here the <em>residuals</em> are the difference between the observed target values and the predicted values.</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="regression_classification.html#cb308-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(linmod1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = GPP_NT_VUT_REF ~ SW_IN_F, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -38.699  -2.092  -0.406   1.893  35.153 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.8732273  0.0285896   30.54   &lt;2e-16 ***
## SW_IN_F     0.0255041  0.0001129  225.82   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.007 on 41299 degrees of freedom
## Multiple R-squared:  0.5525, Adjusted R-squared:  0.5525 
## F-statistic: 5.099e+04 on 1 and 41299 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We can also extract coefficients <span class="math inline">\(\beta\)</span> with</p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="regression_classification.html#cb310-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(linmod1)</span></code></pre></div>
<pre><code>## (Intercept)     SW_IN_F 
##  0.87322728  0.02550413</code></pre>
<p>and the residual sum of squares (which we wanted to minimize) with</p>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb312-1"><a href="regression_classification.html#cb312-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">residuals</span>(linmod1)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 1035309</code></pre>
<p>Although <code>summary()</code> provides a nice, human-readable output, you may find it unpractical to work with. A set of relevant statistical quantities are returned in a tidy format using <code>tidy()</code> from the <code>broom</code> package:</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="regression_classification.html#cb314-1" aria-hidden="true" tabindex="-1"></a>broom<span class="sc">::</span><span class="fu">tidy</span>(linmod1)</span></code></pre></div>
<pre><code>## # A tibble: 2 × 5
##   term        estimate std.error statistic   p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   0.873   0.0286        30.5 1.25e-202
## 2 SW_IN_F       0.0255  0.000113     226.  0</code></pre>
<p><strong>Model advantages and concerns</strong></p>
<p>An advantage of linear regression is that the coefficients provide information that is straight-forward to interpret. We’ve seen above, that <code>GPP_NT_VUT_REF</code> increases by 0.0255 for a unit increase in <code>SW_IN_F</code>. Of course, the units of the coefficients depend on the units of <code>GPP_NT_VUT_REF</code> and <code>SW_IN_F</code>. This has the advantage that the data does not need to be normalised. That is, a linear regression model with the same predictive skills can be found, irrespective of whether <code>GPP_NT_VUT_REF</code> is given in g C m<span class="math inline">\(^{-2}\)</span>s<span class="math inline">\(^{-1}\)</span> or in kg C m<span class="math inline">\(^{-2}\)</span>s<span class="math inline">\(^{-1}\)</span>.</p>
<p>Another advantage of linear regression is that it’s much less prone to overfit than other algorithms. But this can also be a disadvantage, since linear regression can be rather under-fitting. It’s not able to capture non-linearities in the observed relationship and, as we’ll see later in this chapter, it exhibits a poorer performance than more complex models (e.g. polynomial regression) also on the validation data set.</p>
<p>A further limitation is that least squares regression requires <span class="math inline">\(n&gt;p\)</span>. In words: the number of observations must be greater than the number of predictors. If this is not given, one can resort to step-wise forward regression, where predictors are sequentially added based on which predictor adds the most additional information at each step. You’ll encounter stepwise regression in the Application session 8.</p>
<p>When multiple predictors are linearly correlated, then linear regression cannot discern individual effects and individual predictors may appear statistically insignificant when they would be significant if covarying predictors were not included in the model. Such instability can get propagated to predictions. Again, stepwise regression can be used to remedy this problem. However, when one predictor covaries with multiple other predictors, this may not work. For many applications in environmental sciences, we deal with limited numbers of predictors. We can use our own knowledge to examine potentially problematic covariations and make an informed pre-selection rather than throwing all predictors we can possibly think of at our models. Such a pre-selection can be guided by the model performance on a validation data set (more on that below).</p>
<p>An alternative strategy is to use <em>dimension reduction</em> methods. Principal Component regression reduces the data to capture only the complementary axes along which our data varies and therefore collapses covarying predictors into a single one that represents their common axis of variation. Partial Least Squares regression works similarly but modifies the principal components so that they are maximally correlated to the target variable. You can read more on their implementation in R <a href="https://bradleyboehmke.github.io/HOML/linear-regression.html#PCR">here</a>.</p>
</div>
<div id="regression-on-categorical-variables" class="section level4 hasAnchor" number="8.2.2.2">
<h4><span class="header-section-number">8.2.2.2</span> Regression on categorical variables<a href="regression_classification.html#regression-on-categorical-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In the <a href="data_vis.html#vis_regr_cat">regression within categories</a> section of <a href="data_vis.html#data_vis">Chapter 5</a>, we saw that when we separate the data into sub-plots, hidden patterns emerge. This information is very relevant for modeling, because it can be included in our regression model. It is crucial to spend enough time exploring the data before you start modeling, because it helps to understand the fit and output of the model, but also to create models that capture the relationships between variables better.</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="regression_classification.html#cb316-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create month category</span></span>
<span id="cb316-2"><a href="regression_classification.html#cb316-2" aria-hidden="true" tabindex="-1"></a>df_cat <span class="ot">&lt;-</span> hhdf <span class="sc">|&gt;</span></span>
<span id="cb316-3"><a href="regression_classification.html#cb316-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">MONTH =</span> lubridate<span class="sc">::</span><span class="fu">month</span>(TIMESTAMP_START)) <span class="sc">|&gt;</span></span>
<span id="cb316-4"><a href="regression_classification.html#cb316-4" aria-hidden="true" tabindex="-1"></a>  tidyr<span class="sc">::</span><span class="fu">drop_na</span>() <span class="sc">|&gt;</span></span>
<span id="cb316-5"><a href="regression_classification.html#cb316-5" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(MONTH, GPP_NT_VUT_REF, SW_IN_F)</span></code></pre></div>
<p>So far, we have only used continuous variables as explanatory variables in a linear regression. It is also possible to use categorical variables. To do this in R, such variables cannot be of class <code>numeric</code>, otherwise the <code>lm()</code> function treats them as continuous variables. For example, although the variable <code>NIGHT</code> is categorical with values <code>0</code> and <code>1</code>, the model <code>linmod3</code> treats it as a number. We must make sure that categorical variables have class <code>character</code> or, even better, <code>factor</code>.</p>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb317-1"><a href="regression_classification.html#cb317-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fix class of categorical variables</span></span>
<span id="cb317-2"><a href="regression_classification.html#cb317-2" aria-hidden="true" tabindex="-1"></a>df_cat <span class="ot">&lt;-</span> df_cat <span class="sc">|&gt;</span></span>
<span id="cb317-3"><a href="regression_classification.html#cb317-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">MONTH =</span> <span class="fu">as.factor</span>(MONTH))</span></code></pre></div>
<p>Now we can fit the linear model again:</p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="regression_classification.html#cb318-1" aria-hidden="true" tabindex="-1"></a>linmod_cat <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> MONTH <span class="sc">+</span> SW_IN_F, <span class="at">data =</span> df_cat)</span>
<span id="cb318-2"><a href="regression_classification.html#cb318-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(linmod_cat)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = GPP_NT_VUT_REF ~ MONTH + SW_IN_F, data = df_cat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -36.212  -2.346  -0.223   2.200  34.416 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.6146109  0.0893693  18.067  &lt; 2e-16 ***
## MONTH2      -1.8105447  0.1294675 -13.985  &lt; 2e-16 ***
## MONTH3      -2.8800172  0.1264177 -22.782  &lt; 2e-16 ***
## MONTH4      -2.5667281  0.1278097 -20.082  &lt; 2e-16 ***
## MONTH5      -0.0288745  0.1273491  -0.227 0.820631    
## MONTH6       0.4614556  0.1298069   3.555 0.000378 ***
## MONTH7       0.1697514  0.1283830   1.322 0.186100    
## MONTH8       1.2942463  0.1231252  10.512  &lt; 2e-16 ***
## MONTH9       0.5140562  0.1165474   4.411 1.03e-05 ***
## MONTH10     -0.4807082  0.1152536  -4.171 3.04e-05 ***
## MONTH11     -1.3370277  0.1159059 -11.535  &lt; 2e-16 ***
## MONTH12     -1.2634451  0.1151530 -10.972  &lt; 2e-16 ***
## SW_IN_F      0.0246420  0.0001169 210.810  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.865 on 41288 degrees of freedom
## Multiple R-squared:  0.5776, Adjusted R-squared:  0.5775 
## F-statistic:  4704 on 12 and 41288 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>In the fit summary, you can observe that, there are <code>MONTH2</code> to <code>MONTH12</code> parameters. <code>MONTH</code> is a factor which can take 12 different values: <code>1</code> to <code>12</code>. <code>lm()</code> uses one of the factor level as the reference, in this case <code>1</code>, and fits an intercept for the other categories. The result is a set of parallel regression lines, one for each different month.</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="regression_classification.html#cb320-1" aria-hidden="true" tabindex="-1"></a>df_cat <span class="sc">|&gt;</span></span>
<span id="cb320-2"><a href="regression_classification.html#cb320-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">MONTH_NAME =</span> lubridate<span class="sc">::</span><span class="fu">month</span>(<span class="fu">as.integer</span>(MONTH), <span class="at">label =</span> <span class="cn">TRUE</span>)) <span class="sc">|&gt;</span></span>
<span id="cb320-3"><a href="regression_classification.html#cb320-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> SW_IN_F, <span class="at">y =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb320-4"><a href="regression_classification.html#cb320-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb320-5"><a href="regression_classification.html#cb320-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">formula =</span> y <span class="sc">~</span> x <span class="sc">+</span> <span class="dv">0</span>, <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb320-6"><a href="regression_classification.html#cb320-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;SW&quot;</span>, <span class="at">y =</span> <span class="st">&quot;GPP&quot;</span>) <span class="sc">+</span></span>
<span id="cb320-7"><a href="regression_classification.html#cb320-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>MONTH_NAME) <span class="sc">+</span></span>
<span id="cb320-8"><a href="regression_classification.html#cb320-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-162-1.png" width="672" /></p>
<p>In the grid image, we can observe that GPP does not increase with SW at the same rate every month. For example, the increase in GPP is less steep in February than in September. To model this, we should consider a variable slope parameter for each month or category. In R, this is implemented by including an <em>interaction</em> term <code>MONTH:SW_IN_F</code> in the regression formula, like this:</p>
<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb321-1"><a href="regression_classification.html#cb321-1" aria-hidden="true" tabindex="-1"></a>linmod_inter <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> MONTH <span class="sc">+</span> SW_IN_F <span class="sc">+</span> MONTH<span class="sc">:</span>SW_IN_F, <span class="at">data =</span> df_cat)</span>
<span id="cb321-2"><a href="regression_classification.html#cb321-2" aria-hidden="true" tabindex="-1"></a><span class="co"># equivalently: lm(GPP_NT_VUT_REF ~ MONTH * SW_IN_F, data = df_cat)</span></span>
<span id="cb321-3"><a href="regression_classification.html#cb321-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(linmod_inter)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = GPP_NT_VUT_REF ~ MONTH + SW_IN_F + MONTH:SW_IN_F, 
##     data = df_cat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -28.891  -2.113  -0.420   1.892  34.029 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      2.0449603  0.0944991  21.640  &lt; 2e-16 ***
## MONTH2          -1.5386938  0.1369424 -11.236  &lt; 2e-16 ***
## MONTH3          -1.5249304  0.1365863 -11.165  &lt; 2e-16 ***
## MONTH4          -1.0050639  0.1396023  -7.199 6.15e-13 ***
## MONTH5          -0.4502367  0.1412720  -3.187  0.00144 ** 
## MONTH6          -1.2559057  0.1474257  -8.519  &lt; 2e-16 ***
## MONTH7          -0.8440097  0.1446838  -5.833 5.47e-09 ***
## MONTH8          -0.2188300  0.1346734  -1.625  0.10419    
## MONTH9          -1.3407190  0.1269387 -10.562  &lt; 2e-16 ***
## MONTH10         -0.9991456  0.1235627  -8.086 6.32e-16 ***
## MONTH11         -1.2124373  0.1230946  -9.850  &lt; 2e-16 ***
## MONTH12         -1.0724209  0.1210819  -8.857  &lt; 2e-16 ***
## SW_IN_F          0.0158600  0.0008758  18.110  &lt; 2e-16 ***
## MONTH2:SW_IN_F  -0.0030373  0.0011518  -2.637  0.00837 ** 
## MONTH3:SW_IN_F  -0.0058229  0.0009713  -5.995 2.05e-09 ***
## MONTH4:SW_IN_F  -0.0038333  0.0009469  -4.048 5.17e-05 ***
## MONTH5:SW_IN_F   0.0087370  0.0009305   9.389  &lt; 2e-16 ***
## MONTH6:SW_IN_F   0.0135219  0.0009172  14.743  &lt; 2e-16 ***
## MONTH7:SW_IN_F   0.0110791  0.0009182  12.066  &lt; 2e-16 ***
## MONTH8:SW_IN_F   0.0151014  0.0009317  16.209  &lt; 2e-16 ***
## MONTH9:SW_IN_F   0.0180496  0.0009297  19.415  &lt; 2e-16 ***
## MONTH10:SW_IN_F  0.0097277  0.0009761   9.966  &lt; 2e-16 ***
## MONTH11:SW_IN_F -0.0011415  0.0010932  -1.044  0.29640    
## MONTH12:SW_IN_F -0.0099745  0.0012972  -7.689 1.52e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.593 on 41277 degrees of freedom
## Multiple R-squared:  0.6237, Adjusted R-squared:  0.6234 
## F-statistic:  2974 on 23 and 41277 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="polynomial-regression" class="section level4 hasAnchor" number="8.2.2.3">
<h4><span class="header-section-number">8.2.2.3</span> Polynomial regression<a href="regression_classification.html#polynomial-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Furthermore, the relationships between variables may be non-linear. In the previous example, we see that the increase in GPP saturates as shortwave radiation grows, which suggests that the true relationship could be represented by a curve. There are many regression methods that fit this kind of relationship, like polynomial regression, <a href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/loess">LOESS</a> (local polynomial regression fitting), etc.</p>
<p>Let’s fit a simple quadratic regression model, just for the month of August. For this we use the <code>poly()</code> function which constructs orthogonal polynomials of a given degree:</p>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb323-1"><a href="regression_classification.html#cb323-1" aria-hidden="true" tabindex="-1"></a>quadmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> <span class="fu">poly</span>(SW_IN_F, <span class="dv">2</span>), </span>
<span id="cb323-2"><a href="regression_classification.html#cb323-2" aria-hidden="true" tabindex="-1"></a>              <span class="at">data =</span> df_cat <span class="sc">|&gt;</span></span>
<span id="cb323-3"><a href="regression_classification.html#cb323-3" aria-hidden="true" tabindex="-1"></a>                <span class="fu">filter</span>(MONTH <span class="sc">==</span> <span class="dv">8</span>))</span>
<span id="cb323-4"><a href="regression_classification.html#cb323-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(quadmod)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = GPP_NT_VUT_REF ~ poly(SW_IN_F, 2), data = filter(df_cat, 
##     MONTH == 8))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -26.367  -2.055  -0.253   1.801  32.375 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)          7.13084    0.07944   89.77   &lt;2e-16 ***
## poly(SW_IN_F, 2)1  447.25113    4.61907   96.83   &lt;2e-16 ***
## poly(SW_IN_F, 2)2 -151.08797    4.61907  -32.71   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.619 on 3378 degrees of freedom
## Multiple R-squared:  0.7556, Adjusted R-squared:  0.7555 
## F-statistic:  5223 on 2 and 3378 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>In the following plot, you can see how the model fit for GPP in August improves as we consider higher degree polynomials:</p>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb325-1"><a href="regression_classification.html#cb325-1" aria-hidden="true" tabindex="-1"></a>df_cat <span class="sc">|&gt;</span></span>
<span id="cb325-2"><a href="regression_classification.html#cb325-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(MONTH <span class="sc">==</span> <span class="dv">8</span>) <span class="sc">|&gt;</span></span>
<span id="cb325-3"><a href="regression_classification.html#cb325-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> SW_IN_F, <span class="at">y =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb325-4"><a href="regression_classification.html#cb325-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.4</span>) <span class="sc">+</span></span>
<span id="cb325-5"><a href="regression_classification.html#cb325-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">formula =</span> y <span class="sc">~</span> x, <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">&quot;lm&quot;</span>), <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb325-6"><a href="regression_classification.html#cb325-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">formula =</span> y <span class="sc">~</span> <span class="fu">poly</span>(x, <span class="dv">2</span>), <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, </span>
<span id="cb325-7"><a href="regression_classification.html#cb325-7" aria-hidden="true" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">&quot;poly2&quot;</span>), <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb325-8"><a href="regression_classification.html#cb325-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">formula =</span> y <span class="sc">~</span> <span class="fu">poly</span>(x, <span class="dv">3</span>), <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, </span>
<span id="cb325-9"><a href="regression_classification.html#cb325-9" aria-hidden="true" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">&quot;poly3&quot;</span>), <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb325-10"><a href="regression_classification.html#cb325-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">formula =</span> y <span class="sc">~</span> <span class="fu">poly</span>(x, <span class="dv">4</span>), <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, </span>
<span id="cb325-11"><a href="regression_classification.html#cb325-11" aria-hidden="true" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">&quot;poly4&quot;</span>), <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb325-12"><a href="regression_classification.html#cb325-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;SW&quot;</span>, <span class="at">y =</span> <span class="st">&quot;GPP&quot;</span>, <span class="at">color =</span> <span class="st">&quot;Regression&quot;</span>) <span class="sc">+</span></span>
<span id="cb325-13"><a href="regression_classification.html#cb325-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-165-1.png" width="672" /></p>
</div>
<div id="metrics-for-regression" class="section level4 hasAnchor" number="8.2.2.4">
<h4><span class="header-section-number">8.2.2.4</span> Metrics for regression<a href="regression_classification.html#metrics-for-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We have explored several regression models to predict GPP based on SW. Now you may wonder how to choose one of them as your final analysis. Overall, we want to find the simplest model that best explains the data. We seek to find a balance between fitting the data well and generalizing to new data, which can be accomplished by reducing the complexity (for now, the number of parameters) of the model.</p>
<p>Visual inspection of the model fit is a good start, but can become uninformative when the number of explanatory variables grows. So how can we measure model quality? In this section, we present some commonly used <em>metrics</em> to assess and compare regression models.</p>
<ul>
<li><p><strong>MSE</strong>: The mean squared error is defined, as its name suggests, as:
<span class="math display">\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^n (Y_i - \hat{Y_i})^2
\]</span>
It measures the magnitude of the errors, and is minimized to fit a linear regression or, as we will see in <a href="supervised_ml_i.html#supervised_ml_i">Chapter 9</a>, during model training when used as a loss function. Note that since it scales with the square of the errors, the MSE is particularly sensitive to large errors in single points (including outliers).</p></li>
<li><p><strong>RMSE</strong>: The root mean squared error is, as its name suggests, the root of the MSE:
<span class="math display">\[
\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{\frac{1}{n} \sum_{i=1}^n (Y_i - \hat{Y_i})^2}
\]</span>
Like the MSE, the RMSE also measures accuracy (the magnitude of the errors) and is minimized during model training. By taking the square root of mean square errors, the RMSE is in the same units as the data <span class="math inline">\(Y\)</span> and is less sensitive to outliers as the MSE.</p></li>
<li><p><strong><span class="math inline">\(R^2\)</span></strong>: The <em>coefficient of determination</em> measures how close the fitted values are to the true target, and describes the proportion of variation in <span class="math inline">\(Y\)</span> that is captured by modeled values <span class="math inline">\(\hat{Y}\)</span>. Note that the denominator in the formula below is the variance of <span class="math inline">\(Y\)</span>, which is actually the MSE of a model that just predicts the average of <span class="math inline">\(Y\)</span> (using no explanatory variable <span class="math inline">\(X\)</span>). So <span class="math inline">\(R^2\)</span> is basically measuring how much better than just taking the mean of <span class="math inline">\(Y\)</span> our model is. It is traditionally defined as:
<span class="math display">\[
R^2 = 1 - \frac{\sum_i (Y_i - \hat{Y}_i)^2}{\sum_i (Y_i - \bar{Y})^2}
\]</span>
In this case, the goal is to maximize the metric, thus trying the explain as much variation in <span class="math inline">\(Y\)</span> as possible. In contrast to the MSE and RMSE, <span class="math inline">\(R^2\)</span> measures goodness of fit and gives a quantity between 0 and 1 that takes into consideration how variable the target is. We can actually write <span class="math inline">\(R^2 = 1 - \frac{MSE}{\hat{Var}(Y)}\)</span>. A perfect fit is quantified by <span class="math inline">\(R^2 = 1\)</span> and uninformative models have an <span class="math inline">\(R^2\)</span> approaching zero. For some applications where the data is very noisy, an <span class="math inline">\(R^2\)</span> of 0.6 may already be considered good.</p></li>
<li><p><strong>Pearson’s <span class="math inline">\(r^2\)</span></strong>: The linear correlation between two variables (here <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span>) is measured by the <em>Pearson’s correlation coefficient</em> <span class="math inline">\(r\)</span> and defined by
<span class="math display">\[r_{YZ} = \frac{\sum_i (Y_i - \bar{Y}) (Z_i - \bar{Z}) }{\sqrt{ \sum_i(Y_i-\bar{Y})^2}\sqrt{ \sum_i(Z_i-\bar{Z})^2 } }.\]</span>
The coefficient of determination and Pearson’s correlation are closely related. The square of the Pearson’s correlation between the observed <span class="math inline">\(Y\)</span> and fitted values <span class="math inline">\(\hat{Y}\)</span> coincides with the coefficient of determination for a linear regression (see proof <a href="https://statproofbook.github.io/P/slr-rsq">here</a>). It holds that <span class="math inline">\(R^2 = r_{Y \hat{Y}}^2\)</span>.</p></li>
</ul>
<p>The distinction between uppercase and lowercase nomenclature is often not consistent in the literature. The uppercase <span class="math inline">\(R^2\)</span> is commonly used as coefficient of determination, in the context of comparing observed and predicted values. When the <em>correlation</em> between two different variables in a sample is quantified, the lowercase <span class="math inline">\(r^2\)</span> is commonly used.</p>
<p>The following scatterplots show different synthetic data points and regression lines. For each one, the Pearson correlation between target and predictor is calculated, together with the <span class="math inline">\(R^2\)</span> and RMSE for the corresponding regressions.</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="regression_classification.html#cb326-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb326-2"><a href="regression_classification.html#cb326-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Function that creates scatterplots with regression metrics</span></span>
<span id="cb326-3"><a href="regression_classification.html#cb326-3" aria-hidden="true" tabindex="-1"></a>plot_regression <span class="ot">&lt;-</span> <span class="cf">function</span>(d, title){</span>
<span id="cb326-4"><a href="regression_classification.html#cb326-4" aria-hidden="true" tabindex="-1"></a>  d <span class="sc">|&gt;</span></span>
<span id="cb326-5"><a href="regression_classification.html#cb326-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb326-6"><a href="regression_classification.html#cb326-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.75</span>, <span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb326-7"><a href="regression_classification.html#cb326-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">title =</span> title,</span>
<span id="cb326-8"><a href="regression_classification.html#cb326-8" aria-hidden="true" tabindex="-1"></a>         <span class="at">subtitle =</span> (<span class="fu">paste</span>(<span class="st">&quot;r^2 =&quot;</span>, <span class="fu">round</span>(<span class="fu">cor</span>(d<span class="sc">$</span>x, d<span class="sc">$</span>y), <span class="dv">3</span>),</span>
<span id="cb326-9"><a href="regression_classification.html#cb326-9" aria-hidden="true" tabindex="-1"></a>                           <span class="st">&quot;, R^2 =&quot;</span>, <span class="fu">round</span>(<span class="fu">summary</span>(</span>
<span id="cb326-10"><a href="regression_classification.html#cb326-10" aria-hidden="true" tabindex="-1"></a>                             <span class="fu">lm</span>(y<span class="sc">~</span>x, <span class="at">data =</span> d)</span>
<span id="cb326-11"><a href="regression_classification.html#cb326-11" aria-hidden="true" tabindex="-1"></a>                             )<span class="sc">$</span>r.squared, <span class="dv">3</span>)))) <span class="sc">+</span></span>
<span id="cb326-12"><a href="regression_classification.html#cb326-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_smooth</span>(<span class="at">formula =</span> y <span class="sc">~</span> x, <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb326-13"><a href="regression_classification.html#cb326-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lims</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>), <span class="at">y =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>)) <span class="sc">+</span></span>
<span id="cb326-14"><a href="regression_classification.html#cb326-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_classic</span>()</span>
<span id="cb326-15"><a href="regression_classification.html#cb326-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb326-16"><a href="regression_classification.html#cb326-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb326-17"><a href="regression_classification.html#cb326-17" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">plot_regression</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> x,</span>
<span id="cb326-18"><a href="regression_classification.html#cb326-18" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">y =</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)),</span>
<span id="cb326-19"><a href="regression_classification.html#cb326-19" aria-hidden="true" tabindex="-1"></a>                      <span class="st">&quot;Uncorrelated&quot;</span>)</span>
<span id="cb326-20"><a href="regression_classification.html#cb326-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb326-21"><a href="regression_classification.html#cb326-21" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">plot_regression</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> x,</span>
<span id="cb326-22"><a href="regression_classification.html#cb326-22" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">y =</span> <span class="fl">0.5</span> <span class="sc">*</span> x),</span>
<span id="cb326-23"><a href="regression_classification.html#cb326-23" aria-hidden="true" tabindex="-1"></a>                      <span class="st">&quot;Perfectly linearly correlated&quot;</span>)</span></code></pre></div>
<pre><code>## Warning in summary.lm(lm(y ~ x, data = d)): essentially perfect fit: summary may
## be unreliable</code></pre>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="regression_classification.html#cb328-1" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">&lt;-</span> <span class="fu">plot_regression</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> x,</span>
<span id="cb328-2"><a href="regression_classification.html#cb328-2" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">y =</span> <span class="fl">0.5</span><span class="sc">*</span>x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)),</span>
<span id="cb328-3"><a href="regression_classification.html#cb328-3" aria-hidden="true" tabindex="-1"></a>                      <span class="st">&quot;Low correlation&quot;</span>)</span>
<span id="cb328-4"><a href="regression_classification.html#cb328-4" aria-hidden="true" tabindex="-1"></a>p4 <span class="ot">&lt;-</span> <span class="fu">plot_regression</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> x,</span>
<span id="cb328-5"><a href="regression_classification.html#cb328-5" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">y =</span> <span class="fl">0.5</span><span class="sc">*</span>x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="at">sd =</span> <span class="fl">0.2</span>)),</span>
<span id="cb328-6"><a href="regression_classification.html#cb328-6" aria-hidden="true" tabindex="-1"></a>                      <span class="st">&quot;Higher correlation&quot;</span>)</span>
<span id="cb328-7"><a href="regression_classification.html#cb328-7" aria-hidden="true" tabindex="-1"></a>cowplot<span class="sc">::</span><span class="fu">plot_grid</span>(p1, p2, p3, p4)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-166-1.png" width="672" /></p>
<p>Metrics for correlation like the ones above should not be used as a loss function because they do not penalize biased models. All of them keep improving as we include more variables, where they are actually informative or not.</p>
<p>The <span class="math inline">\(R^2\)</span> always increases when predictors are added to a model. This is particularly critical in the context of machine learning when we compare alternative models that differ by their number of parameters. In other words, the <span class="math inline">\(R^2\)</span> of a model with a large number of predictors tends to give an overconfident estimate of its predictive power. We will introduce cross-validation in <a href="supervised_ml_i.html#supervised_ml_i">Chapter 9</a>, which yields good predictive power. This is the “gold-standard”. But when the number of data points is small, cross validation estimates may not be robust. Without resorting to cross validation, the effect of spuriously improving the evaluation metric by adding uninformative predictors can also be mitigated by penalizing the number of predictors <span class="math inline">\(p\)</span>. Different metrics are available:</p>
<ul>
<li><p><strong>Adjusted <span class="math inline">\(R^2\)</span></strong>: The adjusted <span class="math inline">\(R^2\)</span> discounts values by the number of predictors. It is defined as
<span class="math display">\[
{R}^2_{adj} = 1 - (1-R^2) \; \frac{n-1}{n-p-1} \;,
\]</span>
where <span class="math inline">\(n\)</span> (as before) is the number of observations, <span class="math inline">\(p\)</span> the number of parameters and <span class="math inline">\(R^2\)</span> the usual coefficient of determination. Same as for <span class="math inline">\(R^2\)</span>, the goal is to maximize <span class="math inline">\(R^2_{adj}\)</span>. For a fitted model in R <code>modl</code>, its value is returned by <code>summary(modl)$adj.r.squared</code>.</p></li>
<li><p><strong>AIC</strong>: the Akaike’s Information Criterion is defined in terms of log-likelihood (covered in Quantitative Methoden) but for linear regression it can be written as:
<span class="math display">\[
\text{AIC} = n \log \Big(\frac{\text{SSE}}{n}\Big) + 2(p+2)
\]</span>
where <span class="math inline">\(n\)</span> is the number of observations used for estimation, <span class="math inline">\(p\)</span> is the number of explanatory variables in the model and <span class="math inline">\(SSE\)</span> is the sum of squared errors (<span class="math inline">\(SSE= \sum_i (Y_i-\hat{Y_i})^2\)</span>). Also in this case we have to minimize it and the model with the minimum value of the AIC is often the best model for forecasting. Since it penalizes having many parameters, it will favor less complex models.</p></li>
<li><p><strong>AIC<span class="math inline">\(_c\)</span></strong>: For small values of <span class="math inline">\(n\)</span> the AIC tends to select too many predictors. A bias-corrected version of the AIC is defined as:
<span class="math display">\[
\text{AIC}_c = \text{AIC} + \frac{2(p + 2)(p + 3)}{n-p-3}
\]</span>
Also AIC<span class="math inline">\(_c\)</span> is minimized for an optimal predictive model.</p></li>
<li><p><strong>BIC</strong>: the Schwarz’s Bayesian Information Criterion is defined as
<span class="math display">\[
\text{BIC} = n \log \Big(\frac{\text{SSE}}{n}\Big) + (p+2)  \log(n)
\]</span>
Also in this case our goal is to minimize the BIC. This metric has the feature that if there is a true underlying model, the BIC will select that model given enough data. The BIC tends to select a model with fewer predictors than AIC.</p></li>
</ul>
<p><strong>Implementation in R</strong></p>
<p>Let’s calculate the metrics introduced above for a few of the fitted regression models. Some of these metrics, like <span class="math inline">\(R^2\)</span> and the adjusted <span class="math inline">\(R^2\)</span> are given by the <code>summary()</code> function. Alternatively, the {yardstick} package provides implementations for a few of these metrics, which we compute below:</p>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb329-1"><a href="regression_classification.html#cb329-1" aria-hidden="true" tabindex="-1"></a>compute_regr_metrics <span class="ot">&lt;-</span> <span class="cf">function</span>(linmod){</span>
<span id="cb329-2"><a href="regression_classification.html#cb329-2" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">&lt;-</span> <span class="fu">length</span>(linmod<span class="sc">$</span>coefficients)</span>
<span id="cb329-3"><a href="regression_classification.html#cb329-3" aria-hidden="true" tabindex="-1"></a>  sse <span class="ot">&lt;-</span> <span class="fu">sum</span>(linmod<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb329-4"><a href="regression_classification.html#cb329-4" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">length</span>(linmod<span class="sc">$</span>residuals)</span>
<span id="cb329-5"><a href="regression_classification.html#cb329-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb329-6"><a href="regression_classification.html#cb329-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">round</span>(</span>
<span id="cb329-7"><a href="regression_classification.html#cb329-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">c</span>(<span class="at">mse =</span> <span class="fu">mean</span>(linmod<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb329-8"><a href="regression_classification.html#cb329-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">rmse =</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(linmod<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>)),</span>
<span id="cb329-9"><a href="regression_classification.html#cb329-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">R2 =</span> <span class="fu">summary</span>(linmod)<span class="sc">$</span>r.squared,</span>
<span id="cb329-10"><a href="regression_classification.html#cb329-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">R2_adj =</span> <span class="fu">summary</span>(linmod)<span class="sc">$</span>adj.r.squared,</span>
<span id="cb329-11"><a href="regression_classification.html#cb329-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">r2 =</span> <span class="fu">cor</span>(linmod<span class="sc">$</span>fitted, linmod<span class="sc">$</span>model<span class="sc">$</span>GPP_NT_VUT_REF)<span class="sc">^</span><span class="dv">2</span>,</span>
<span id="cb329-12"><a href="regression_classification.html#cb329-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">AIC =</span> <span class="fu">extractAIC</span>(linmod)[<span class="dv">2</span>],</span>
<span id="cb329-13"><a href="regression_classification.html#cb329-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">AIC_adj =</span> <span class="fu">extractAIC</span>(linmod)[<span class="dv">2</span>] <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>(p<span class="sc">+</span><span class="dv">2</span>)<span class="sc">*</span>(p<span class="sc">+</span><span class="dv">3</span>)<span class="sc">/</span>(n<span class="sc">-</span>p<span class="dv">-3</span>),</span>
<span id="cb329-14"><a href="regression_classification.html#cb329-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">BIC =</span> <span class="fu">BIC</span>(linmod) <span class="co"># this implementation is based on log-likelihood</span></span>
<span id="cb329-15"><a href="regression_classification.html#cb329-15" aria-hidden="true" tabindex="-1"></a>  ), <span class="dv">3</span>)</span>
<span id="cb329-16"><a href="regression_classification.html#cb329-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb329-17"><a href="regression_classification.html#cb329-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb329-18"><a href="regression_classification.html#cb329-18" aria-hidden="true" tabindex="-1"></a>metrics <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="fu">list</span>(linmod1, linmod2, linmod_cat, quadmod), compute_regr_metrics)</span>
<span id="cb329-19"><a href="regression_classification.html#cb329-19" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(metrics) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Linear model&quot;</span>, <span class="st">&quot;Linear model 2&quot;</span>, <span class="st">&quot;Linear + categories&quot;</span>,</span>
<span id="cb329-20"><a href="regression_classification.html#cb329-20" aria-hidden="true" tabindex="-1"></a>                       <span class="st">&quot;Quadratic model&quot;</span>)</span>
<span id="cb329-21"><a href="regression_classification.html#cb329-21" aria-hidden="true" tabindex="-1"></a>metrics</span></code></pre></div>
<pre><code>##         Linear model Linear model 2 Linear + categories Quadratic model
## mse           25.067         24.782              23.664          21.317
## rmse           5.007          4.978               4.865           4.617
## R2             0.553          0.558               0.578           0.756
## R2_adj         0.553          0.558               0.577           0.755
## r2             0.553          0.558               0.578           0.756
## AIC       133058.006     132589.878          130700.234       10350.160
## AIC_adj   133058.007     132589.880          130700.246       10350.177
## BIC       250293.053     249842.182          248030.196       19971.526</code></pre>
</div>
</div>
<div id="classification" class="section level3 hasAnchor" number="8.2.3">
<h3><span class="header-section-number">8.2.3</span> Classification<a href="regression_classification.html#classification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We will introduce a classification problem with a binary target, although it’s easy to generalize to categorical variables with more than two classes. As an example, we will try to classify whether it’s day or night (<code>NIGHT</code>) based on shortwave and longwave radiation measurements (<code>SW_IN_F</code> and <code>LW_IN_F</code>). Let’s look at the data first:</p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb331-1"><a href="regression_classification.html#cb331-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2023</span>)</span>
<span id="cb331-2"><a href="regression_classification.html#cb331-2" aria-hidden="true" tabindex="-1"></a>gg2 <span class="ot">&lt;-</span> hhdf <span class="sc">|&gt;</span></span>
<span id="cb331-3"><a href="regression_classification.html#cb331-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sample_n</span>(<span class="dv">1000</span>) <span class="sc">|&gt;</span>  <span class="co"># to reduce the dataset </span></span>
<span id="cb331-4"><a href="regression_classification.html#cb331-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> SW_IN_F, <span class="at">y =</span> LW_IN_F, <span class="at">color =</span> <span class="fu">as.factor</span>(NIGHT))) <span class="sc">+</span></span>
<span id="cb331-5"><a href="regression_classification.html#cb331-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.75</span>) <span class="sc">+</span></span>
<span id="cb331-6"><a href="regression_classification.html#cb331-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Shortwave radiation (W m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;)&quot;</span>)), </span>
<span id="cb331-7"><a href="regression_classification.html#cb331-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Longwave radiation (W m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;)&quot;</span>))) <span class="sc">+</span></span>
<span id="cb331-8"><a href="regression_classification.html#cb331-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span>
<span id="cb331-9"><a href="regression_classification.html#cb331-9" aria-hidden="true" tabindex="-1"></a>gg2</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-168-1.png" width="672" /></p>
<p>At first sight, it’s easy to see that the short wave radiation is close to zero in the night, while longwave radiation doesn’t seem to differ strongly between daytime and nighttime.</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="regression_classification.html#cb332-1" aria-hidden="true" tabindex="-1"></a>hhdf <span class="sc">|&gt;</span></span>
<span id="cb332-2"><a href="regression_classification.html#cb332-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(NIGHT<span class="sc">==</span><span class="dv">0</span>) <span class="sc">|&gt;</span></span>
<span id="cb332-3"><a href="regression_classification.html#cb332-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull</span>(SW_IN_F) <span class="sc">|&gt;</span></span>
<span id="cb332-4"><a href="regression_classification.html#cb332-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">hist</span>(<span class="at">breaks=</span><span class="dv">30</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-169-1.png" width="672" />
Using this example, we’ll cover logistic regression, its implementation in R and metrics for classification.</p>
<div id="logistic-regression" class="section level4 hasAnchor" number="8.2.3.1">
<h4><span class="header-section-number">8.2.3.1</span> Logistic regression<a href="regression_classification.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Theory</strong></p>
<p>A classification problem is a bit more difficult to write mathematically than a regression problem. Before, the mathematical representation of <code>GPP_NT_VUT_REF ~ SW_IN_F</code> was <code>GPP_NT_VUT_REF</code><span class="math inline">\(\;=\; \beta_0 + \beta_1\)</span><code>SW_IN_F</code>. With the classification model <code>NIGHT ~ SW_IN_F</code>, we cannot just write <code>NIGHT</code><span class="math inline">\(\;=\; \beta_0 + \beta_1\)</span><code>SW_IN_F</code> because <code>NIGHT</code> is not a number. Hence, the categorical variable must be encoded, in this case <code>0</code> means “day” and <code>1</code> means “night”. We already loaded the data into R with this encoding.</p>
<p>The next issue is that a linear model makes continuous predictions in the entire real numbers space <span class="math inline">\((-\inf, \inf)\)</span>, but we want the predictions to be in <span class="math inline">\(\{0, 1\}\)</span>. We can transform these values to be in <span class="math inline">\([0,1]\)</span> with a <em>link</em> function. For a binary response, it’s common to use a <em>logit</em> link function:
<span class="math display">\[\text{logit}(z) = \frac{\exp(z)}{1+\exp(z)}\]</span></p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb333-1"><a href="regression_classification.html#cb333-1" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">exp</span>(x)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(x)), <span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="at">ylab =</span> <span class="st">&quot;logit(x)&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-170-1.png" width="672" /></p>
<p>Combining a linear model (with any type of predictors, like for regression) and a logit link function, we arrive to a <strong>logistic regression</strong> model:
<span class="math display">\[f(X, \beta) = \text{logit}(\beta_0 + \beta_1 X_1 + ... + \beta_p X_p) = \frac{\exp(\beta_0 + \beta_1 X_1 + ... + \beta_p X_p)}{1 + \exp(\beta_0 + \beta_1 X_1 + ... + \beta_p X_p)}.\]</span>
This predicted value can be understood as the probability of belonging to class 1 (in our example, nighttime). A classification rule is defined such that an observation <span class="math inline">\(X_{new}\)</span> with a predicted probability of belonging to class 1 higher than a given <em>threshold</em> <span class="math inline">\(\tau\)</span> (i.e. <span class="math inline">\(f(X_{new}, \beta) &gt; \tau\)</span>) will be classified as 1; and if the predicted probability is smaller than the threshold, it will be classified as 0.</p>
<p>A logistic regression model results in a linear classification rule. This means that the <span class="math inline">\(p\)</span>-dimensional space will be divided in two by a hyperplane, and the points falling in each side of the hyperplane will be classified as 1 or 0. In the example above with shortwave and longwave radiation as predictors, the classification boundary would look like the black line in the plot below.</p>
<p>Furthermore, to fit a logistic regression model means to calculate the maximum likelihood estimator of <span class="math inline">\(\beta\)</span> with an iterative algorithm.</p>
<p><strong>Implementation in R</strong></p>
<p>To fit a logistic regression in R we can use the <code>glm()</code> function, which fits a generalized linear model, indicating that our target variable is binary and the link function is a logit function. Let’s see the model output:</p>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="regression_classification.html#cb334-1" aria-hidden="true" tabindex="-1"></a>logmod <span class="ot">&lt;-</span> <span class="fu">glm</span>(NIGHT <span class="sc">~</span> SW_IN_F <span class="sc">+</span> LW_IN_F,</span>
<span id="cb334-2"><a href="regression_classification.html#cb334-2" aria-hidden="true" tabindex="-1"></a>              <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> logit),</span>
<span id="cb334-3"><a href="regression_classification.html#cb334-3" aria-hidden="true" tabindex="-1"></a>              <span class="at">data =</span> hhdf)</span></code></pre></div>
<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="regression_classification.html#cb336-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logmod)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = NIGHT ~ SW_IN_F + LW_IN_F, family = binomial(link = logit), 
##     data = hhdf)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.3046   0.0000   0.0000   0.1853   8.4904  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  7.6170522  0.2326894   32.73   &lt;2e-16 ***
## SW_IN_F     -1.1869830  0.0222507  -53.35   &lt;2e-16 ***
## LW_IN_F     -0.0110463  0.0007195  -15.35   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 72825.2  on 52607  degrees of freedom
## Residual deviance:  9728.7  on 52605  degrees of freedom
## AIC: 9734.7
## 
## Number of Fisher Scoring iterations: 15</code></pre>
<p>This fitted model results in a linear classification boundary that splits the predictor variables space in two. Where that line falls depends on the choice of threshold <span class="math inline">\(\tau=0.5\)</span>. You can see it plotted below:</p>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb338-1"><a href="regression_classification.html#cb338-1" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">coef</span>(logmod)</span>
<span id="cb338-2"><a href="regression_classification.html#cb338-2" aria-hidden="true" tabindex="-1"></a><span class="co"># reuse previous plot with classification line</span></span>
<span id="cb338-3"><a href="regression_classification.html#cb338-3" aria-hidden="true" tabindex="-1"></a>gg2 <span class="sc">+</span></span>
<span id="cb338-4"><a href="regression_classification.html#cb338-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">slope =</span> <span class="sc">-</span>beta[<span class="dv">2</span>]<span class="sc">/</span>beta[<span class="dv">3</span>], <span class="at">intercept =</span> (<span class="fl">0.5</span><span class="sc">-</span>beta[<span class="dv">1</span>])<span class="sc">/</span>beta[<span class="dv">3</span>])</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-172-1.png" width="672" /></p>
<p>Most blue points fall to one side of the dashed classification line and most red points to the other side; this is what we wanted. The points that are in the wrong side of the line are misclassified by the logistic regression model, we’re trying to minimize that.</p>
<p>Note that, just like for linear regression, a logistic regression model allows to use categorical explanatory variables and polynomial transformations of the predictors to achieve better-fitting classification models.</p>
<p><strong>Model advantages and concerns</strong></p>
<p>One advantage of logistic regression is simplicity. It’s part of the <em>generalized linear regression</em> family of models and the concept of a link function used to build such a model can also be used for various types of response variables (not only binary, but also count data…). You can find more details in this <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">Wikipedia article</a>.</p>
<p>Furthermore, logistic regression allows for an interesting interpretation of its model parameters: <em>log-odds</em> and <em>log-odds ratios</em>. Logg-odds ratios represent how much likely it is to find one class versus the other (e.g. class 1 is twice as likely than class 0 whenever we have probabilities <span class="math inline">\(66\%\)</span> vs <span class="math inline">\(33\%\)</span>). Increases in the values of the predictors affect the log-odds multiplicatively.</p>
<p>It is easy to extend a logistic regression model to more than two classes by fitting models iteratively. For example, first you classify class 1 against classes 2 and 3; then another logistic regression classifies class 2 against 3.</p>
<p>Nevertheless, logistic regression relies on statistical assumptions to fit the parameters and interpret the fitted parameters. So whenever these assumptions are not met, one must be careful with the conclusions drawn. Other machine learning methods, that will be covered in Chapters <a href="supervised_ml_i.html#supervised_ml_i">9</a> and <a href="supervised_ml_ii.html#supervised_ml_ii">10</a>, can also be used for classification tasks. These offer more flexibility than logistic regression (are not necessarily linear) and don’t need to satisfy strict statistical assumptions.</p>
</div>
<div id="metrics-for-classification" class="section level4 hasAnchor" number="8.2.3.2">
<h4><span class="header-section-number">8.2.3.2</span> Metrics for classification<a href="regression_classification.html#metrics-for-classification" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Measuring the quality of a classification model is based on counting how many observations were correctly classified, rather than the distance between the values predicted by a regression and the true observed values. These can be represented in a <em>confusion matrix</em>:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center"><span class="math inline">\(Y = 1\)</span></th>
<th align="center"><span class="math inline">\(Y = 0\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\hat{Y} = 1\)</span></td>
<td align="center">True positives (TP)</td>
<td align="center">False positives (FP)</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat{Y} = 0\)</span></td>
<td align="center">False negatives (FN)</td>
<td align="center">True negatives (TN)</td>
</tr>
</tbody>
</table>
<p>In a confusion matrix, correctly classified observations are on the diagonal and off-diagonal values correspond to different types of errors. Some of these error types are more relevant for certain applications.</p>
<p>Imagine that you want to classify whether the water of a river is safe to drink based on measurements of certain particles or chemicals in the water (Y=1 means safe, Y=0 means unsafe). It’s much worse to tag as “safe” a polluted river than to tag as “unsafe” a potable water source, one must be conservative. In this case, we would prioritize avoiding false positives and wouldn’t care so much about false negatives.</p>
<p>The following metrics are widely used and highlight different aspects of our modeling goals.</p>
<ul>
<li><strong>Accuracy</strong> is simply the proportion of outputs that were correctly classified:
<span class="math display">\[ \text{Accuracy}=\frac{\text{TP} + \text{TN}}{N},\]</span>
where <span class="math inline">\(N\)</span> is the number of observations. This is a very common metric for training ML models and treats both classes as equally important. It’s naturally extended to multi-class classification and usually compared to the value <span class="math inline">\(\frac{1}{C}\)</span> where <span class="math inline">\(C\)</span> is the number of classes.</li>
</ul>
<p>Classification models are usually compared to randomness: how much better is our model compared to throwing a coin for classification? At random, we would assign each class <span class="math inline">\(50\%\)</span> of the time. So if we assume that both classes are as likely to appear, that is, they are <em>balanced</em>, the accuracy of a random guess would be around <span class="math inline">\(0.5\)</span>. Hence, we want the accuracy to be “better than random”. If there are <span class="math inline">\(C\)</span> different classes and the observations are balanced, we want the accuracy to be above <span class="math inline">\(\frac{1}{C}\times 100 \%\)</span>. Keep in mind that for a dataset where <span class="math inline">\(90\%\)</span> of the observations are from class 1 and <span class="math inline">\(10\%\)</span> from class 0, always predicting 1 would lead to a <span class="math inline">\(0.9\)</span> accuracy. This value may sound good, but that model is rubbish because it doesn’t use any information from predictors. Be careful when working with imbalanced classes and interpreting your results.</p>
<ul>
<li><p><strong>Precision</strong> measures how often our “positive” predictions are correct:
<span class="math display">\[\text{precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}.\]</span></p></li>
<li><p>The <strong>true positive rate</strong> (TPR), also called <strong>Recall</strong> or <strong>sensitivity</strong> measures the proportion of real “positives” (<span class="math inline">\(Y = 1\)</span>) we are able to capture:
<span class="math display">\[ \text{TPR} = \frac{\text{TP}}{\text{TP}+\text{FN}}.\]</span></p></li>
<li><p>The <strong>false positive rate</strong> (FPR) is defined by
<span class="math display">\[\text{FPR} = \frac{\text{FP}}{\text{FP}+\text{TN}}.\]</span>
and is related to another metric called <strong>specificity</strong> by <span class="math inline">\(\text{FPR} = 1 - \text{specificity}\)</span>.</p></li>
<li><p><strong>ROC curve</strong>: To evaluate the performance of a binary classification model, it’s common to plot the <em>ROC curve</em>. The TPR is plotted against the FPR, for varying values of the threshold used in the classification rule. When we decrease the threshold, we get more positive values (more observations are classified as 1), increasing both the true positive and false positive rate. The following image describes clearly how to interpret a ROC curve plot:</p></li>
</ul>
<div class="figure">
<img src="figures/Roc_curve.png" alt="" />
<p class="caption">ROC curves and how they compare, from <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">Wikipedia</a>.</p>
</div>
<ul>
<li><p><strong>AUC</strong>: The “area under the curve” is defined as the area left below the ROC curve. For a random classifier we would have AUC=0.5 and for the perfect classifier, AUC=1. It’s good to try to increase the AUC and it’s used often as a reporting metric. Nevertheless, a visual inspection of the ROC curve can say even more.</p></li>
<li><p><strong>F1</strong>: The F1 score is a more sophisticated metric, defined as the harmonic mean of precision and sensitivity, or in terms of the confusion matrix values:
<span class="math display">\[F1= 2 \times \frac{\text{prec} \times \text{recall}}{\text{prec} + \text{recall}} = \frac{2 \text{TP}}{2 \text{TP} + \text{FP} + \text{FN}}.\]</span>
This metric provides good results for both balanced and imbalanced datasets and takes into account both the model’s ability to capture positive cases (recall) and be correct with the cases it does capture (precision). It takes values between 0 and 1, with 1 being the best and values of 0.5 and below being bad.</p></li>
</ul>
<p>These metrics can be used to compare the quality of different classifiers but also to understand the behaviour of a single classifier from different perspectives.</p>
<p>This was an introduction of the most basic classification metrics. For a more information on the topic, check out <a href="https://bookdown.org/max/FES/measuring-performance.html#class-metrics">this book chapter</a>.</p>
<p><strong>Implementation in R</strong></p>
<p>Let’s take a look at the previous metrics for the logistic regression model we fitted before. The <code>confusionMatrix()</code> function from the {caret} library provides most of the statistics introduced above.</p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb339-1"><a href="regression_classification.html#cb339-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make classification predictions</span></span>
<span id="cb339-2"><a href="regression_classification.html#cb339-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(logmod<span class="sc">$</span>data<span class="sc">$</span>NIGHT)</span>
<span id="cb339-3"><a href="regression_classification.html#cb339-3" aria-hidden="true" tabindex="-1"></a>Y_pred <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">round</span>(logmod<span class="sc">$</span>fitted.values)) <span class="co"># Use 0.5 as threshold</span></span>
<span id="cb339-4"><a href="regression_classification.html#cb339-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb339-5"><a href="regression_classification.html#cb339-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Change class names</span></span>
<span id="cb339-6"><a href="regression_classification.html#cb339-6" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(Y) <span class="ot">&lt;-</span> <span class="fu">levels</span>(Y_pred) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;DAY&quot;</span>, <span class="st">&quot;NIGHT&quot;</span>)</span>
<span id="cb339-7"><a href="regression_classification.html#cb339-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb339-8"><a href="regression_classification.html#cb339-8" aria-hidden="true" tabindex="-1"></a><span class="co"># plot confusion matrix</span></span>
<span id="cb339-9"><a href="regression_classification.html#cb339-9" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(<span class="at">data =</span> Y_pred, <span class="at">reference =</span> Y)</span>
<span id="cb339-10"><a href="regression_classification.html#cb339-10" aria-hidden="true" tabindex="-1"></a>conf_matrix</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   DAY NIGHT
##      DAY   26218    19
##      NIGHT  1261 25110
##                                          
##                Accuracy : 0.9757         
##                  95% CI : (0.9743, 0.977)
##     No Information Rate : 0.5223         
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      
##                                          
##                   Kappa : 0.9513         
##                                          
##  Mcnemar&#39;s Test P-Value : &lt; 2.2e-16      
##                                          
##             Sensitivity : 0.9541         
##             Specificity : 0.9992         
##          Pos Pred Value : 0.9993         
##          Neg Pred Value : 0.9522         
##              Prevalence : 0.5223         
##          Detection Rate : 0.4984         
##    Detection Prevalence : 0.4987         
##       Balanced Accuracy : 0.9767         
##                                          
##        &#39;Positive&#39; Class : DAY            
## </code></pre>
<p>Now we can visualize the confusion matrix as a mosaic plot. This is quite helpful when we work with many classes.</p>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb341-1"><a href="regression_classification.html#cb341-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mosaicplot</span>(conf_matrix<span class="sc">$</span>table,</span>
<span id="cb341-2"><a href="regression_classification.html#cb341-2" aria-hidden="true" tabindex="-1"></a>           <span class="at">main =</span> <span class="st">&quot;Confusion matrix&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-174-1.png" width="672" /></p>
</div>
</div>
<div id="model-evaluation" class="section level3 hasAnchor" number="8.2.4">
<h3><span class="header-section-number">8.2.4</span> Model evaluation<a href="regression_classification.html#model-evaluation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Model evaluation refers to several techniques that help you understand how the model performs, whether this behavior is what you expect and how you can improve it. You can use metrics and plots to get an overview of the weaknesses of your model. This section covers model comparison, variable selection and outlier detection, and more concepts related to model evaluation (overfitting, data pre-processing, cross-validation…) are explained in the remaining chapters. Concepts will be explained using regression as an example, but are directly translated to classification problems.</p>
<div id="model-comparison" class="section level4 hasAnchor" number="8.2.4.1">
<h4><span class="header-section-number">8.2.4.1</span> Model comparison<a href="regression_classification.html#model-comparison" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Be systematic with your model comparisons. Three key ideas in model selection are:</p>
<ul>
<li>Comparisons should be hierarchical: compare a model to another that “contains it”, i.e. compare <code>GPP_NT_VUT_REF ~ SW_IN_F</code> to <code>GPP_NT_VUT_REF ~ SW_IN_F + LW_IN_F</code>, not <code>GPP_NT_VUT_REF ~ SW_IN_F</code> to <code>GPP_NT_VUT_REF ~ NIGHT + TA_F</code>.</li>
<li>Complexity must be increased slowly: add one variable at a time, not three variables all at once. This helps avoid collinearity in the predictors.</li>
<li>Choose the most appropriate metric: if possible, a metric that accounts for model complexity and represents the goal of your analysis (e.g. recall for a classification where you don’t want to miss any positives).</li>
</ul>
<p>If you’re considering different model approaches for the same task, you should first fit the best possible model for each approach, and then compare those optimized models to each other. For example, fit the best linear regression with your available data, the best kNN non-parametric regression model and a random forest; then compare those three final models and choose the one that answers your research question the best.</p>
<p>One must be careful not to keep training or improving models until they fit the data perfectly, but maintain the models’ ability to generalize to newly available data. Chapter <a href="#suspervised_ml_i">9</a> introduces the concept of overfitting, which is central to data science. Think of model interpretation and generalization when comparing them, not only of performance. Simple models can be more valuable than very complex ones because they tell a better story about the data (e.g. by having few very good predictors rather than thousands of mediocre ones, from which we cannot learn the underlying relationships).</p>
</div>
<div id="variable-selection-1" class="section level4 hasAnchor" number="8.2.4.2">
<h4><span class="header-section-number">8.2.4.2</span> Variable selection<a href="regression_classification.html#variable-selection-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let’s think of variable selection in the context of linear regression. A brute force approach to variable selection would be: Fit a linear regression for each combination of available predictors, calculate a metric (e.g. AIC) and choose the best one (lowest AIC). The problem is, if you have 8 predictors, you would fit 40320 different regression models. This can be very computationally expensive.</p>
<p>Instead, take a hierarchical greedy approach, starting with an empty model (just an intercept) and adding one variable at a time. This is called <strong>stepwise (forward) regression</strong>. The algorithm goes as follows:</p>
<ul>
<li>First, you fit all regression models with just one variable and compute the <span class="math inline">\(R^2\)</span>.</li>
<li>Then, select the one leading to the greatest <span class="math inline">\(R^2\)</span> (best fitting model) and compute the AIC (or BIC).</li>
<li>In the next step, you add a second variable, one at a time, fitting the regression models and calculating their <span class="math inline">\(R^2\)</span>.</li>
<li>Choose as second variable the one leading to the best <span class="math inline">\(R^2\)</span>. Then, compute the AIC. If the AIC (which accounts for model fit and complexity) is worse, that is, bigger, stop and keep the univariate linear model. If the AIC is better, that is, smaller, add the second variable and repeat the previous steps to include a third variable. The method finishes once you cannot reduce the AIC anymore, or when you run out of variables. In the end, you’ll have more or less the best possible linear regression model. The function <code>step()</code> implements the stepwise algorithm in R.</li>
</ul>
<p>This stepwise approach can be done backwards, starting with a full model (all available variables) and removing one at a time. Or even with a back-and-forth approach, where you look at both including a new or removing an existing variable at each step (optimizing AIC). Furthermore, this algorithm can be applied to fitting a polynomial regression. We want to increase the degree of the polynomials unit by unit. For a model with categorical variables, interaction terms should only be considered after having the involved variables as “intercept only”.</p>
<p><strong>Multicollinearity</strong> exists when there is a correlation between multiple explanatory variables in a multivariate regression model. This is problematic because it makes the estimated coefficients corresponding to the variables that are highly correlated very unstable. Since two highly correlated variables explain almost the same, it doesn’t matter whether we include one or the other in the model (the performance metrics will be similar) or even if we include both of them. Hence, it becomes difficult to say which variables actually influence the target.</p>
<p>The <strong>variance inflation factor (VIF)</strong> is a score from economics that measures the amount of multicollinearity in regression, based on how the estimated variance of a coefficient is inflated due to its correlation with another predictor. It’s calculated as
<span class="math display">\[\text{VIF}_i = \frac{1}{1 - R^2_i},\]</span>
where <span class="math inline">\(R^2_i\)</span> is the coefficient of determination for regressing the i<span class="math inline">\(^{th}\)</span> predictor on the remaining ones. A VIF<span class="math inline">\(_i\)</span> is computed for each predictor in the multivariate regression model we are evaluating, meaning: if <span class="math inline">\(\text{VIF}_i = 1\)</span> variables are not correlated; if <span class="math inline">\(1 &lt; \text{VIF}_i &lt; 5\)</span> there is moderate collinearity; and if <span class="math inline">\(\text{VIF}_i \geq 5\)</span> they are highly correlated. Because they can be almost fully explained by all the other predictors (high <span class="math inline">\(R^2_i\)</span>), these variables are redundant in our final model.</p>
<p>When we work with high-dimensional data (that is, we have more variables than observations) there are better techniques to do variable selection than stepwise regression. Since the predictors space is so large, we could fit a line that passes through all the observations (a perfect fit), but does the model generalize? We don’t know. For example, Lasso and Ridge regression incorporate variable selection in the fitting process (you can check <a href="https://www.r-bloggers.com/2020/06/understanding-lasso-and-ridge-regression/">this post</a> if you’re curious).</p>
</div>
<div id="outlier-detection" class="section level4 hasAnchor" number="8.2.4.3">
<h4><span class="header-section-number">8.2.4.3</span> Outlier detection<a href="regression_classification.html#outlier-detection" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Detecting outliers is very important, because they can affect the fit of a model greatly. Take a look at the two linear regressions below and how one single weird observation can throw off the fit. Whenever an observation is very distant from the center of the predictor’s distribution, it becomes very influential (it has a bit <em>leverage</em>). If the observed response for that data point is in harmony with the rest of points, nothing happens, but if it’s also off, the regression model will be affected greatly.</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="regression_classification.html#cb342-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2023</span>)</span>
<span id="cb342-2"><a href="regression_classification.html#cb342-2" aria-hidden="true" tabindex="-1"></a>hhdf_small <span class="ot">&lt;-</span> hhdf <span class="sc">|&gt;</span></span>
<span id="cb342-3"><a href="regression_classification.html#cb342-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sample_n</span>(<span class="dv">100</span>) <span class="sc">|&gt;</span> <span class="co"># reduce dataset</span></span>
<span id="cb342-4"><a href="regression_classification.html#cb342-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(SW_IN_F, GPP_NT_VUT_REF)</span>
<span id="cb342-5"><a href="regression_classification.html#cb342-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb342-6"><a href="regression_classification.html#cb342-6" aria-hidden="true" tabindex="-1"></a>gg3 <span class="ot">&lt;-</span> hhdf_small <span class="sc">|&gt;</span></span>
<span id="cb342-7"><a href="regression_classification.html#cb342-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> SW_IN_F, <span class="at">y =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb342-8"><a href="regression_classification.html#cb342-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.75</span>) <span class="sc">+</span></span>
<span id="cb342-9"><a href="regression_classification.html#cb342-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">fullrange =</span> <span class="cn">TRUE</span>) <span class="sc">+</span></span>
<span id="cb342-10"><a href="regression_classification.html#cb342-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Shortwave radiation (W m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;)&quot;</span>)), </span>
<span id="cb342-11"><a href="regression_classification.html#cb342-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;GPP (gC m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;s&quot;</span><span class="sc">^-</span><span class="dv">1</span>, <span class="st">&quot;)&quot;</span>))) <span class="sc">+</span></span>
<span id="cb342-12"><a href="regression_classification.html#cb342-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb342-13"><a href="regression_classification.html#cb342-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="sc">-</span><span class="dv">20</span>, <span class="dv">40</span>) <span class="sc">+</span> </span>
<span id="cb342-14"><a href="regression_classification.html#cb342-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>, <span class="dv">2000</span>)</span>
<span id="cb342-15"><a href="regression_classification.html#cb342-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb342-16"><a href="regression_classification.html#cb342-16" aria-hidden="true" tabindex="-1"></a>gg4 <span class="ot">&lt;-</span> hhdf_small <span class="sc">|&gt;</span></span>
<span id="cb342-17"><a href="regression_classification.html#cb342-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_row</span>(<span class="at">SW_IN_F =</span> <span class="dv">2000</span>, <span class="at">GPP_NT_VUT_REF =</span> <span class="sc">-</span><span class="dv">20</span>) <span class="sc">|&gt;</span> <span class="co"># add outlier</span></span>
<span id="cb342-18"><a href="regression_classification.html#cb342-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> SW_IN_F, <span class="at">y =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb342-19"><a href="regression_classification.html#cb342-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.75</span>) <span class="sc">+</span></span>
<span id="cb342-20"><a href="regression_classification.html#cb342-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">fullrange =</span> <span class="cn">TRUE</span>) <span class="sc">+</span></span>
<span id="cb342-21"><a href="regression_classification.html#cb342-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Shortwave radiation (W m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;)&quot;</span>)), </span>
<span id="cb342-22"><a href="regression_classification.html#cb342-22" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;GPP (gC m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;s&quot;</span><span class="sc">^-</span><span class="dv">1</span>, <span class="st">&quot;)&quot;</span>))) <span class="sc">+</span></span>
<span id="cb342-23"><a href="regression_classification.html#cb342-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb342-24"><a href="regression_classification.html#cb342-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">2000</span>, <span class="at">y =</span> <span class="sc">-</span><span class="dv">20</span>), <span class="at">colour=</span><span class="st">&#39;blue&#39;</span>) <span class="sc">+</span></span>
<span id="cb342-25"><a href="regression_classification.html#cb342-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="sc">-</span><span class="dv">20</span>, <span class="dv">40</span>) <span class="sc">+</span> </span>
<span id="cb342-26"><a href="regression_classification.html#cb342-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>, <span class="dv">2000</span>)</span>
<span id="cb342-27"><a href="regression_classification.html#cb342-27" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb342-28"><a href="regression_classification.html#cb342-28" aria-hidden="true" tabindex="-1"></a>cowplot<span class="sc">::</span><span class="fu">plot_grid</span>(gg3, gg4)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;
## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-175-1.png" width="672" /></p>
<p>The first step to identifying outliers is to look at your data, one variable at a time. Plot a histogram to see the rough distribution of a variable, this will help identify what kind of values to expect. In Chapters <a href="data_wrangling.html#data_wrangling">3</a> and <a href="data_vis.html#data_vis">4</a> it was introduced how to identify values that fell out of this distribution using histograms and boxplots. Checking in the histogram if the distribution has fat tails helps to discern whether the values that pop out of a boxplot should be considered outliers or not.</p>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb344-1"><a href="regression_classification.html#cb344-1" aria-hidden="true" tabindex="-1"></a>gg5 <span class="ot">&lt;-</span> hhdf_small <span class="sc">|&gt;</span></span>
<span id="cb344-2"><a href="regression_classification.html#cb344-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_row</span>(<span class="at">SW_IN_F =</span> <span class="dv">2000</span>, <span class="at">GPP_NT_VUT_REF =</span> <span class="sc">-</span><span class="dv">20</span>) <span class="sc">|&gt;</span> <span class="co"># add outlier</span></span>
<span id="cb344-3"><a href="regression_classification.html#cb344-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> GPP_NT_VUT_REF, <span class="at">y =</span> <span class="fu">after_stat</span>(density))) <span class="sc">+</span></span>
<span id="cb344-4"><a href="regression_classification.html#cb344-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">fill =</span> <span class="st">&quot;grey70&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb344-5"><a href="regression_classification.html#cb344-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">color =</span> <span class="st">&#39;red&#39;</span>)<span class="sc">+</span></span>
<span id="cb344-6"><a href="regression_classification.html#cb344-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&#39;Histogram, density and boxplot&#39;</span>, </span>
<span id="cb344-7"><a href="regression_classification.html#cb344-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;GPP (gC m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;s&quot;</span><span class="sc">^-</span><span class="dv">1</span>, <span class="st">&quot;)&quot;</span>))) <span class="sc">+</span></span>
<span id="cb344-8"><a href="regression_classification.html#cb344-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span>
<span id="cb344-9"><a href="regression_classification.html#cb344-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb344-10"><a href="regression_classification.html#cb344-10" aria-hidden="true" tabindex="-1"></a>gg6 <span class="ot">&lt;-</span> hhdf_small <span class="sc">|&gt;</span></span>
<span id="cb344-11"><a href="regression_classification.html#cb344-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_row</span>(<span class="at">SW_IN_F =</span> <span class="dv">2000</span>, <span class="at">GPP_NT_VUT_REF =</span> <span class="sc">-</span><span class="dv">20</span>) <span class="sc">|&gt;</span> <span class="co"># add outlier</span></span>
<span id="cb344-12"><a href="regression_classification.html#cb344-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="st">&quot;&quot;</span>, <span class="at">y =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb344-13"><a href="regression_classification.html#cb344-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="at">fill =</span> <span class="st">&quot;grey70&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb344-14"><a href="regression_classification.html#cb344-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb344-15"><a href="regression_classification.html#cb344-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb344-16"><a href="regression_classification.html#cb344-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.y=</span><span class="fu">element_blank</span>(),</span>
<span id="cb344-17"><a href="regression_classification.html#cb344-17" aria-hidden="true" tabindex="-1"></a>        <span class="at">axis.ticks.y=</span><span class="fu">element_blank</span>()) <span class="sc">+</span></span>
<span id="cb344-18"><a href="regression_classification.html#cb344-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;GPP (gC m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;s&quot;</span><span class="sc">^-</span><span class="dv">1</span>, <span class="st">&quot;)&quot;</span>)))</span>
<span id="cb344-19"><a href="regression_classification.html#cb344-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb344-20"><a href="regression_classification.html#cb344-20" aria-hidden="true" tabindex="-1"></a>gg7 <span class="ot">&lt;-</span> hhdf_small <span class="sc">|&gt;</span></span>
<span id="cb344-21"><a href="regression_classification.html#cb344-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_row</span>(<span class="at">SW_IN_F =</span> <span class="dv">2000</span>, <span class="at">GPP_NT_VUT_REF =</span> <span class="sc">-</span><span class="dv">20</span>) <span class="sc">|&gt;</span> <span class="co"># add outlier</span></span>
<span id="cb344-22"><a href="regression_classification.html#cb344-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> SW_IN_F, <span class="at">y =</span> <span class="fu">after_stat</span>(density))) <span class="sc">+</span></span>
<span id="cb344-23"><a href="regression_classification.html#cb344-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">fill =</span> <span class="st">&quot;grey70&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb344-24"><a href="regression_classification.html#cb344-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">color =</span> <span class="st">&#39;red&#39;</span>)<span class="sc">+</span></span>
<span id="cb344-25"><a href="regression_classification.html#cb344-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&#39;Histogram, density and boxplot&#39;</span>, </span>
<span id="cb344-26"><a href="regression_classification.html#cb344-26" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Shortwave radiation (W m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;)&quot;</span>))) <span class="sc">+</span></span>
<span id="cb344-27"><a href="regression_classification.html#cb344-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span>
<span id="cb344-28"><a href="regression_classification.html#cb344-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb344-29"><a href="regression_classification.html#cb344-29" aria-hidden="true" tabindex="-1"></a>gg8 <span class="ot">&lt;-</span> hhdf_small <span class="sc">|&gt;</span></span>
<span id="cb344-30"><a href="regression_classification.html#cb344-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_row</span>(<span class="at">SW_IN_F =</span> <span class="dv">2000</span>, <span class="at">GPP_NT_VUT_REF =</span> <span class="sc">-</span><span class="dv">20</span>) <span class="sc">|&gt;</span> <span class="co"># add outlier</span></span>
<span id="cb344-31"><a href="regression_classification.html#cb344-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="st">&quot;&quot;</span>, <span class="at">y =</span> SW_IN_F)) <span class="sc">+</span></span>
<span id="cb344-32"><a href="regression_classification.html#cb344-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="at">fill =</span> <span class="st">&quot;grey70&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb344-33"><a href="regression_classification.html#cb344-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb344-34"><a href="regression_classification.html#cb344-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb344-35"><a href="regression_classification.html#cb344-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.y=</span><span class="fu">element_blank</span>(),</span>
<span id="cb344-36"><a href="regression_classification.html#cb344-36" aria-hidden="true" tabindex="-1"></a>        <span class="at">axis.ticks.y=</span><span class="fu">element_blank</span>()) <span class="sc">+</span></span>
<span id="cb344-37"><a href="regression_classification.html#cb344-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Shortwave radiation (W m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;)&quot;</span>)))</span>
<span id="cb344-38"><a href="regression_classification.html#cb344-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb344-39"><a href="regression_classification.html#cb344-39" aria-hidden="true" tabindex="-1"></a>cowplot<span class="sc">::</span><span class="fu">plot_grid</span>(gg5, gg7, gg6, gg8,</span>
<span id="cb344-40"><a href="regression_classification.html#cb344-40" aria-hidden="true" tabindex="-1"></a>                   <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">rel_heights =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>),</span>
<span id="cb344-41"><a href="regression_classification.html#cb344-41" aria-hidden="true" tabindex="-1"></a>                   <span class="at">align =</span> <span class="st">&#39;v&#39;</span>, <span class="at">axis =</span> <span class="st">&#39;lr&#39;</span>)</span></code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-176-1.png" width="672" />
A <strong>Q-Q Plot</strong> depicts the sample quantiles of a variable against the theoretical quantiles of a distribution of our choice, usually a normal distribution. In the histograms above, GPP looks somewhat Gaussian but with fatter tails and slightly skewed to the right, while shorwave radiation is skewed to the right, resembling an exponential distribution. This is also visible in the Q-Q plots below, because outliers deviate greatly from the straight line (which represents a match between the observed values and the theoretical distribution):</p>
<div class="sourceCode" id="cb346"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb346-1"><a href="regression_classification.html#cb346-1" aria-hidden="true" tabindex="-1"></a>gg9 <span class="ot">&lt;-</span> hhdf_small <span class="sc">|&gt;</span></span>
<span id="cb346-2"><a href="regression_classification.html#cb346-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_row</span>(<span class="at">SW_IN_F =</span> <span class="dv">2000</span>, <span class="at">GPP_NT_VUT_REF =</span> <span class="sc">-</span><span class="dv">20</span>) <span class="sc">|&gt;</span> <span class="co"># add outlier |&gt;</span></span>
<span id="cb346-3"><a href="regression_classification.html#cb346-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">sample =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb346-4"><a href="regression_classification.html#cb346-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_qq</span>() <span class="sc">+</span></span>
<span id="cb346-5"><a href="regression_classification.html#cb346-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_qq_line</span>() <span class="sc">+</span></span>
<span id="cb346-6"><a href="regression_classification.html#cb346-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;GPP (gC m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;s&quot;</span><span class="sc">^-</span><span class="dv">1</span>, <span class="st">&quot;)&quot;</span>)),</span>
<span id="cb346-7"><a href="regression_classification.html#cb346-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Theoretical normal quantiles&quot;</span>) <span class="sc">+</span></span>
<span id="cb346-8"><a href="regression_classification.html#cb346-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span>
<span id="cb346-9"><a href="regression_classification.html#cb346-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb346-10"><a href="regression_classification.html#cb346-10" aria-hidden="true" tabindex="-1"></a>gg10 <span class="ot">&lt;-</span> hhdf_small <span class="sc">|&gt;</span></span>
<span id="cb346-11"><a href="regression_classification.html#cb346-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_row</span>(<span class="at">SW_IN_F =</span> <span class="dv">2000</span>, <span class="at">GPP_NT_VUT_REF =</span> <span class="sc">-</span><span class="dv">20</span>) <span class="sc">|&gt;</span> <span class="co"># add outlier |&gt;</span></span>
<span id="cb346-12"><a href="regression_classification.html#cb346-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">sample =</span> SW_IN_F)) <span class="sc">+</span></span>
<span id="cb346-13"><a href="regression_classification.html#cb346-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_qq</span>() <span class="sc">+</span></span>
<span id="cb346-14"><a href="regression_classification.html#cb346-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_qq_line</span>() <span class="sc">+</span></span>
<span id="cb346-15"><a href="regression_classification.html#cb346-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Shortwave radiation (W m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;)&quot;</span>)),</span>
<span id="cb346-16"><a href="regression_classification.html#cb346-16" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Theoretical normal quantiles&quot;</span>) <span class="sc">+</span></span>
<span id="cb346-17"><a href="regression_classification.html#cb346-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span>
<span id="cb346-18"><a href="regression_classification.html#cb346-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb346-19"><a href="regression_classification.html#cb346-19" aria-hidden="true" tabindex="-1"></a>cowplot<span class="sc">::</span><span class="fu">plot_grid</span>(gg9, gg10, <span class="at">ncol=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-177-1.png" width="672" /></p>
<p>For linear (and logistic) regression, we would like variables to look as normal as possible. You’ve probably learned some of the reasons for this in quantitative methods courses, but are beyond the scope of this class. It’s common to study the distribution of the regression residuals with QQ-plots to assess if model assumptions are met.</p>
<p>Above, you can see the distributions of our target and predictor (with outliers). And it’s very easy to see the weird value for the shortwave radiation but for GPP it doesn’t stick out so much. This already points to how important it is to check their multivariate distribution. R provides some useful plots from the fitted regression objects, in particular the “Residuals vs Leverage” plot:</p>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb347-1"><a href="regression_classification.html#cb347-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit regression with outlier</span></span>
<span id="cb347-2"><a href="regression_classification.html#cb347-2" aria-hidden="true" tabindex="-1"></a>linmod_outlier <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> SW_IN_F, </span>
<span id="cb347-3"><a href="regression_classification.html#cb347-3" aria-hidden="true" tabindex="-1"></a>                     <span class="at">data =</span> <span class="fu">add_row</span>(hhdf_small, <span class="at">SW_IN_F =</span> <span class="dv">2000</span>, <span class="at">GPP_NT_VUT_REF =</span> <span class="sc">-</span><span class="dv">20</span>))</span>
<span id="cb347-4"><a href="regression_classification.html#cb347-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb347-5"><a href="regression_classification.html#cb347-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(linmod_outlier, <span class="dv">5</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-178-1.png" width="672" />
This plot shows the leverage (see the mathematical definition <a href="https://en.wikipedia.org/wiki/Leverage_(statistics)">here</a>) of each observation against the corresponding residual from the fitted linear regression. Points with high leverage, i.e. far from the center of the predictor distribution, and big residuals, i.e. far from the fitted regression line, are very influential. <strong>Cook’s distance</strong> (definition <a href="https://en.wikipedia.org/wiki/Cook%27s_distance">here</a>) is an estimate of the influence of a data point in a linear regression and observations with Cook’s distance &gt; 1 are candidates for being outliers. See in the plot above how the point with index 101 (our added outlier) has a very large Cook’s distance. Boundary regions for Cook’s distance equal to 0.5 (suspicious) and 1 (certainly influential) are painted with a dotted line.</p>
<p>Finally, it’s very important that, before you remove a value because it may be an outlier, you understand where the data came from and if such an abnormal observation is possible. If it depicts an extraordinary but possible situation, this information can be very valuable and it’s wiser to keep it in the model. Interesting research questions arise when data doesn’t align with our preconceptions, so keep looking into it and potentially collect more data.</p>
</div>
</div>
</div>
<div id="exercises-7" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Exercises<a href="regression_classification.html#exercises-7" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Performance assessment: Exercise for stepwise regression <a href="https://stineb.github.io/esds_book/ch-08.html">link</a>, <a href="https://github.com/stineb/esds_book-gitlab/blob/master/08_variable_selection.Rmd">link</a></li>
</ul>
</div>
<div id="solutions-6" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span> Solutions<a href="regression_classification.html#solutions-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="open_science.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="supervised_ml_i.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/08-regression_classification.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
