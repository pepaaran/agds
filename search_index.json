[["index.html", "Applied Geodata Science About this book 0.1 Target readers of this book", " Applied Geodata Science Benjamin Stocker (lead), Koen Hufkens (contributing), Pepa Aran (contributing), Pascal Schneider (contributing) 2023-02-01 About this book This book accompanies the course(s) Applied Geodata Science, taught at the Institute of Geography, University of Bern. 0.1 Target readers of this book The target of this book are people interested in applying data science methods for research. Methods introduced, example data sets, and prediction challenges are chosen to make the book relate most to scientists in Geography and Environmental Sciences. No prior knowledge in working with code, or R in particular, are required. Brief introductions into the fundamentals are given in Chapters @ref(getting_started) and @ref(programming_primers). "],["introduction.html", "Introduction 0.2 What is Applied Geodata Science? 0.3 The (data) science workflow 0.4 Why now?", " Introduction The sheer volume of data that is becoming available now bears a huge potential for answering long-standing questions in all fields of environmental and geo-sciences. This gives rise to a new set of tools that can be used and a new set of challenges when applying them. This course teaches you the basic concepts, methods, and tools for your future data science projects in environmental sciences - in academia and in the wild. The course thus prepares you to benefit from the general data richness in environmental and geo-sciences. The course introduces the typical data science workflow using various examples of geographical and environmental data. With a strong hands-on component and a series of input lectures, the course introduces the basic concepts of data science and teaches how to conduct each step of the data science workflow. This includes the handling of various data formats, the formulation and fitting of robust statistical models, including basic machine learning algorithms, the effective visualisation and communication of results, and the implementation of reproducible workflows, founded in Open Science principles. The overall course goal is to teach students to tell a story with data. All tutorials use the R programming language. 0.2 What is Applied Geodata Science? What is Data Science? Why “Applied Geo-” Data Science? 0.3 The (data) science workflow The red thread of this course is the data science workflow. Applied (geodata-) data science projects typically start with research questions and hypotheses, and some data at hand, and (ideally) end with an answer to the research questions. What lies in between is not a linear process, but a cycle. One has to “understand” the data in order to identify appropriate analyses for answering the research questions. Such an understanding of the data is gradually established by repeated cycles of transforming the data, visualizing it, and modelling relationships. Before we’ve visualized the data, we don’t know how to transform it. And before we’ve modeled it, we don’t know the most appropriate visualization. In practice, we approach answers to our research questions gradually, through repeated cycles of exploratory data analysis [Fig. data-science-workflow]. [Fig. data-science-workflow] As we work our way through repeated cycles of exploratory data analysis (Chapter 0.3), we take decisions based on our data analysis, modelling, and visualizations. And we write code. The final conclusions we draw, the answers to research questions we find, and the results we communicate rest on the combination of all steps of our data processing, analysis, and visualization. Simply put, it rests on the reproducibility (and legibility) of our code. This course teaches you the tools and concepts to effectively learn from (large volumes of) data, and to put reproducibility of a geodata science project into practice. 0.4 Why now? Three general developments set the stage for this course. First, Geography and Environmental Sciences (as many other realms of today’s world) have entered a data-rich era (Chapters @ref(data_variety). xxx ref(data-rich-era)). Second, machine learning algorithms have revolutionized the way we can extract information from large volumes of data (Chapters @ref(supervised_ml) - @ref(interpretable_ml). Third, Open Science principles (Chapter @ref(open_science)) - essential for inclusive research, boundless progress, and for diffusing science to society - are becoming a prerequisite for getting research funded and published (Chapter @ref(open_science))). This course teaches you the skills to benefit from these developments. What is data? How do we learn from data? What is a model? Our practice at GECO "],["getting_started.html", "Chapter 1 Getting started 1.1 Learning objectives 1.2 Tutorial 1.3 Exercises 1.4 Solutions", " Chapter 1 Getting started Chapter lead author: Pepa Aran TBC Contents: Lecture (Beni): Data revolution, opportunities, challenges; explain relevance and why new methods are required installing environment workspace management R, RStudio R libraries, other libraries and applications 1.1 Learning objectives After you’ve gone over the lecture and solved the exercises, you should be able to: Work with R and RStudio on your computer Know some R objects and basic classes Follow basic good coding practices Organize your workspace using R projects Save your code and progress in an organized way 1.2 Tutorial 1.2.1 Working with R and RStudio R is a free, open-source programming language and software environment for statistical computing and graphics. It is widely used among statisticians and data miners for developing statistical software and data analysis. RStudio is an integrated development environment (IDE) for R that makes it easy to use R for data analysis and visualization. 1.2.1.1 Installing R and RStudio To use R and RStudio, you will first need to download and install them on your computer. To install R, go to the CRAN website and download the latest version of R for your operating system. Once the download is complete, follow the on-screen installation instructions for your operating system to install R. To install RStudio, go to the RStudio website and download the latest version of RStudio for your operating system. Once the download is complete, follow the installation instructions for your operating system to install RStudio. 1.2.1.2 The RStudio interface RStudio provides a user-friendly interface for writing, running, and debugging R code. When you open RStudio, you will see the following: RStudio interface. The interface is divided into four main panels: The source editor is where you can write, edit, and save your R code. The console is where you can enter R commands and see the output. The environment panel shows you the objects (variables, data frames, etc.) that are currently in your R session, as well as their values. The files, plots, help, etc. panel shows you the files, plots, and other items that are currently in your R workspace, as well as help and documentation for R functions and packages. We will cover this in more detail later in this course. 1.2.1.3 Running R code Once you have both programs installed, you can open RStudio and begin a new R session. To run R code using R Studio, follow these steps: In the source editor panel, type your R code. To run the code, you can either press the Run button or use the keyboard shortcut Ctrl + Enter (Windows) or Command + Enter (Mac). The code will be executed in the console panel, and any output will be displayed there. Alternatively, you can directly type single-statement R commands in the console and run them by pressing Enter. For example, let’s say you want to calculate the sum of the numbers 1, 2, and 3. You can write the following code in the source editor: # Calculate the sum of 1, 2, and 3 1 + 2 + 3 ## [1] 6 Then, you can press the Run button or use the keyboard shortcut to run the code. The output will be displayed in the console: &gt; 1 + 2 + 3 [1] 6 1.2.1.4 Base R operations The R base package contains the basic functions which let R function as a programming language: arithmetic, input/output, basic programming support, etc. Its contents are always available when you start an R session. Here we introduce the main binary operators, which work on vectors, matrices and scalars. Arithmetic operators: + addition - subtraction * multiplication / division ^ or ** exponentiation Logical operators: &gt; greater than &gt;= greater than or equal to == exactly equal to &lt; less than &lt;= less than or equal to != not equal 1.2.1.5 Working directory The working directory is the default location to which R writes to and reads files from, and you can specify it by going to Session &gt; Set Working Directory… and navigating to your desired folder. Alternatively, you can check what is your current working directory or change it by entering the following in the console: getwd() # get working directory setwd(&quot;~/agds&quot;) # set working directory Ideally, the working directory is a folder containing only the files relevant to your data analysis project. 1.2.2 R objects In addition to running single statements in the R console, the output of a statement can be saved as a new object. There are many kinds of R objects, some of which are covered here and in future chapters. 1.2.2.1 Types of data First, we will introduce the different types of data that one can encounter. We can classify statistical variables according to what values they take. Numerical: These statistical variables can be measured quantitatively and their value is a number. Continuous: We say that a variable is continuous when it can take an infinite number of real values within an interval (for example, the altitude of a mountain). One could consider unbounded variables (temperature) or restricted variables, like positive variables (weight of a person) or an interval (a proportion between 0 and 1). Discrete: When the variable can only take a finite number of values in an interval, we say it is discrete. A common example is count data, like the population of a city. Categorical: The values are characteristics that cannot be quantified. Binary: These variables have two possible values: TRUE or FALSE (a variable indicating whether the person has siblings or not). Nominal: They describe a name, label or category without natural order (for example the name of a person). Ordinal: Like their name indicates, ordinal variables are categorical and follow a natural order. For example, “terrible”, “bad”, “neutral”, “good”, “great”. A numerical variable can sometimes be discretized and put into categories, like dividing people into age groups “toddler”, “child”, “teenager”, “adult”. Next, we will see how these different types of variables can be treated in R. 1.2.2.2 Variables and classes In R, a variable is a named location in memory that stores a value. To create a variable, you simply assign a value to a name using the &lt;- operator (or the = operator, which is equivalent, but &lt;- is preferred). For example: my_variable &lt;- 5 This code creates a variable called my_variable and assigns the value 5 to it. You can access the value of a variable or any other object by simply referring to its name, like this: my_variable ## [1] 5 When you run this code, the value of my_variable will be printed to the console. Running print(my_variable) is an alternative syntax, using the print() function. In R, every object and value has a class that determines how it is stored and how it behaves. For example, the 5 in our example above is a number, so its class is numeric. To find out the class of a value or a variable, you can use the class() function, like this: class(5) ## [1] &quot;numeric&quot; class(my_variable) ## [1] &quot;numeric&quot; The most basic classes are: numeric (num) - any real number, e.g. 2.375 integer (int) - integer numbers, e.g. 2 character (chr) - any string, e.g. “fluxes” logical (logi) - binary, i.e. TRUE/FALSE values factor (Factor) - categorical data, the variable can only be one of a defined amount of options, e.g. female/male/other. Factors can also be given an order. function - a set of statements organized to perform a specific task, for example mean() By default, any number is coerced as \"numeric\". So if you want an integer value to have class \"integer\", you need to specify it like this: my_variable &lt;- as.integer(5) class(my_variable) ## [1] &quot;integer&quot; Sometimes you need to convert the class of an object, for example turning a \"integer\" number into a \"character\". You can do so as follows: my_variable &lt;- as.character(my_variable) my_variable ## [1] &quot;5&quot; class(my_variable) ## [1] &quot;character&quot; Notice that now the values are in quotes \"5\". This way, R interprets it as a text and you will not be able to do any numeric calculations with it anymore. 1.2.2.3 Vectors A vector in R is a sequence of data elements of the same class. Vectors can be created with the c() function, which stands for concatenate, i.e., to link together in a series or chain. For example, the following code creates a numeric vector: x &lt;- c(1, 2, 3, 4, 5) To access the elements of a vector, you can use the square bracket notation. For example, the following code retrieves the second element of the vector x: x[2] ## [1] 2 You can also use the square bracket notation to extract a sub-vector from a larger vector. For example, you can extract the second to fourth elements of the vector x: x[2:4] ## [1] 2 3 4 Another useful property of vectors in R is that they can be easily combined using arithmetic operators. For example, adding the elements of two vectors x and y element-wise: x &lt;- c(1, 2, 3) y &lt;- c(4, 5, 6) x + y ## [1] 5 7 9 R also supports vectors of other classes, for example character vectors. Since all elements must be of the same class, the most general class will be adopted. The following code concatenates the vectors x and y, followed by new character elements: z &lt;- c(x, y, &quot;seven&quot;, &quot;eight&quot;) z ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;seven&quot; &quot;eight&quot; class(z) ## [1] &quot;character&quot; Operations on vectors are performed element-wise. For example, if we ask what numbers in x are greater than 2, we obtain a vector of logical values (and class \"logical\"): x &gt; 2 ## [1] FALSE FALSE TRUE 1.2.2.4 Lists Lists are another R object, of class \"list\". They are extremely flexible. They allow us to store different types of data, even if they are of different lengths or classes. They are created with the function list() and can be named or not. Here is an example where each element of the list is named. mylist &lt;- list( temperatures = c(2.234, 1.987, 4.345), my_favourite_function = mean, best_course = &quot;Applied Geodata Science&quot; ) Similar to vectors, we can extract elements from lists, either by index [[1]] or by the name using [[\"temperatures\"]] or $temperatures. Note the double [[]] here, indicating an element of a list as opposed to [] indicating an element of a vector. To get the entire vector of temperatures, do either of the three: mylist[[1]] ## [1] 2.234 1.987 4.345 mylist[[&quot;temperatures&quot;]] ## [1] 2.234 1.987 4.345 mylist$temperatures ## [1] 2.234 1.987 4.345 Notice below how, if we index the list like we would index a vector, a list with just one element would be returned, rather than the element itself. [ is used to select multiple elements and [[ or $ for selecting a single element. A thorough explanation of these differences is given here and here. mylist[1] ## $temperatures ## [1] 2.234 1.987 4.345 To get the first temperature value, which is an element of the vector (at the same time an element of the list), we can run: mylist[[&quot;temperatures&quot;]][1] ## [1] 2.234 You can also append elements to the list (either way is possible): mylist[[&quot;my_second_favourite_function&quot;]] &lt;- median mylist$my_second_favourite_function &lt;- median This was a very condensed introduction to vectors and lists. A more complete introduction is given here. 1.2.2.5 Data frames A data frame, of class \"data.frame\", is a tightly coupled collection of variables which share many of the properties of matrices and of lists, used as the fundamental data structure in R. You can think of a data frame as a table. The content of each data frame column is a vector. Columns need to be of the same length and all values in a column need to be of the same data type. A data frame can be created as follows: df &lt;- data.frame(name = c(&quot;Maria&quot;, &quot;Peter&quot;, &quot;Alex&quot;), age = c(13, 56, 30)) df ## name age ## 1 Maria 13 ## 2 Peter 56 ## 3 Alex 30 The elements of a data frame can be accessed the same way that we treated lists, with each column being one element of the list. To get the vector of ages: df$age ## [1] 13 56 30 Furthermore, they can be treated as a matrix. Notice that the first index refers to rows and the second to columns. For example: df[, 1] # first column ## [1] &quot;Maria&quot; &quot;Peter&quot; &quot;Alex&quot; df[2, ] # second row ## name age ## 2 Peter 56 df[2,2] # age of Peter ## [1] 56 There are many more things you can do with data frames. Since they are central to analyzing data with R, we go into more detail in Tutorial 2 and have dedicated all of Tutorial 3 to teach you how to work with data frames in a tidy way. 1.2.2.6 Functions R functions can be applied to an object (or several objects) and return another object. For example the mean() function can take a numeric vector as input and output the mean of its values. mean(df$age) ## [1] 33 Functions are also R objects and have class \"function\". They are covered in more detail in Tutorial 2. 1.2.2.7 Missing values R has two representations for missing values: NA and NULL. Similar objects also exist in other programming languages. NA is an identifyer to mark missing data and stands for not available. You will encounter this when reading data into a data frame, and some of its cells show NA because that value is missing. Also, if you ask for the 4th element of a vector of length 3, R returns NA. x[4] ## [1] NA Almost all operations on vectors where at least one value is NA also return NA. For example: mean(c(1, 2, NA)) ## [1] NA To remove all missing values in the function evaluation, the common argument to set in the respective function call is na.rm. By default, it is usually set to FALSE, but we can do: mean(c(1, 2, NA), na.rm = TRUE) ## [1] 1.5 Furthermore, NA counts as an element in vectors. A variable assigned just NA would have length 1 (of class \"logical\") and the vector above has length 3, as can be determined using the length() function, and has class \"numeric\". By contrast, NULL is the R null object or empty space. You can also assign NULL to a variable, which will have length zero because it is empty. Functions may return NULL when no output was defined, or if an error occurred. 1.2.2.8 R environment The set of objects (variables, data frames, etc.) defined during an R session are referred to as the environment. You can view the objects in RStudio in the environment panel in R Studio, grouped as Data, Values and Functions. After closing an existing R session (e.g., after quitting RStudio), the environment defined by the used during that session will not be saved automatically and will be lost. To save your environment, go to the Session menu and select Save Workspace As…. This will save all your objects in a .RData file in your working directory. However, this is not recommended. Next, we will go over some more sophisticated ways of writing code and saving your progress. 1.2.2.9 Read and save data The function save() allows to save multiple R objects of any form as a single .RData file. This is how the environment of your R session is saved. This is how we would save several R objects: save(df, df_small, file = &quot;data/data_frames.RData&quot;) .RData files are read into your environment using the load() function, rather than read(). This function loads the objects with the name that they were saved with. load(&quot;data/data_frames.RData&quot;) Alternatively, the function saveRDS() allows you save individual R objects of any form (not just a data frame). saveRDS() creates a binary file that is fast to write and read, but only intelligible to R. Such files are commonly identified by the suffix .rds. It is recommended to name the .rds files according to the single object they contain. For example: saveRDS(df_small, file = &quot;data/df_small.rds&quot;) This file can then be read into your R environment. We need to assign a name to the object read, otherwise it will be just printed to the console but not loaded into the R environment. Sometimes, it is useful to give it a new name, for example: df_small_2 &lt;- readRDS(&quot;data/df_small.rds&quot;) 1.2.3 R scripts Usually, multiple statements are needed to get, e.g., from reading data into R to final numbers and figures that make up a further analysis. Together, these multiple statements constitute a workflow. It is essential that all workflows that underlie results of publications are reproducible, that is, that another person can replicate your results using your code and certain data. To make a workflow reproducible, the sequence of statements that you needed to carry out your analysis and produce outputs can be saved as an R script. A script is a text file named with the suffix .R to indicate that it is executable by R. It contains a sequence of R commands, which you can run all at once or one at a time. These commands will always be run line-by-line, starting from the top. To create a new script in RStudio, go to the File menu and select New File &gt; R Script. This will open a new script file in the source editor. You can then type your R code in the script file and save it to your computer. To run a script, you can either use the Source button in the source editor or use the keyboard shortcut Ctrl + Shift + Enter (Windows) or Command + Shift + Enter (Mac). This will run all of the commands in the script file, in the order they are written, in the console. Alternatively, you can type into the console: &gt; source(&quot;my_r_script.R&quot;) Note that, to be able to run the code above, the file my_r_script.R must be in your curretn working directory. You can find more useful information about scripts and workflows in R for Data Science (Wickham2017R?). We should always strive to write nice scripts and good code. Good code is clean, readable, consistent, and extensible (easily modified or adapted). To achieve this, here are a few points to consider - inspired by best practices for coding and by the Tidyverse style guide (wickham_welcome_nodate?). 1.2.3.1 Structure your script At the beginning of each file add a header as a fully commented text section, describing what the code contains, and how it fits into the larger analysis framework. Note that Git stores all meta information about the file, including who created it, who modified it and when. This information should not be added to the header. Then, load all libraries needed within the script. Then, source any scripts and load data, and only then, start with the sequence of statements. To visually separate parts, break up your code using, commented lines. For example, a script could look like this: #//////////////////////////////////////// # Demonstrating script structure #--------------------------------------- #----Load packages, functions and data library(tidyverse) source(&quot;R/my_functions.R&quot;) my_df &lt;- readr::read_csv(&quot;data/my_df.csv&quot;) #--------------------------------------- # Main part #--------------------------------------- # convert units my_df$temp &lt;- my_df$temp + 273.15 # deg C -&gt; K #--------------------------------------- # Writing output #--------------------------------------- filn &lt;- &quot;data/my_df_kelvin.csv&quot; print(paste(&quot;Writing file&quot;, filn, &quot;...&quot;)) write_csv(my_df, filn) 1.2.3.2 Comments Adding comments in the code helps to explain exactly what the code is doing and why. This makes it easy to understand and modify the code, and can be key when debugging. In R source files, comments are prefixed with a #, which means that all what is right of the # is not interpreted by R. Avoid redundant comments like the one below. Instead, comments should explain the “why” of that chunk of code, not the “what” or “how”. # take the mean myvar_mean &lt;- mean(myvar) 1.2.3.3 Spaces and breaks Adding enough white spaces and line breaks in the right locations greatly helps the legibility of any code. Cramping it up too much leads to an unintelligible sequence of characters and it will not be clear what parts go together (operators, variable names, brackets). Therefore, consider the following points: Use spaces around operators (=, +, -, &lt;-, &gt;, etc.). Use &lt;-, not =, for allocating a value to a variable. An opening curly bracket ({) should be followed by a line break and never stand alone on a line. A closing curly bracket (}) should stand alone on a line unless followed by else. Code inside curly brackets should be indented (recommended: two white spaces at the beginning of each line for each indentation level - don’t use tabs). For example, well written code looks like this: if (temp &gt; 5.0){ growth_temp &lt;- growth_temp + temp } 1.2.3.4 Variable naming It is preferable to use concise and descriptive variable names. Different variable naming styles are being used. In this course, we use lowercase letters, and underscores (_) to separate words within a name (_). Avoid (.) as they are reserved for S3 objects (base R). Also, you should avoid naming your objects with names of common functions and variables since your re-definition will mask already defined object names. For example, df_daily is a data frame with data at a daily resolution. Or clean_daily is a function that cleans daily data. Note that a verb is used as a name for a function and an underscore (_) is used to separate words. It is also recommendable to avoid variable names consisting of only one character. This makes it practically impossible to search for that variable. # Good day_01 # Bad DayOne day.one first_day_of_the_month djm1 # Very bad mean &lt;- function(x) sum(x)/length(x) # mean() itself is already a function T &lt;- FALSE # T is an abbreviation of TRUE c &lt;- 10 # c() is used to create a vector (example &lt;- c(1, 2, 3)) 1.2.3.5 R Markdown R Markdown files are an enhanced version of scripts. They combine formatted text and executable code chunks. They can either be compiled (knitted) into an HTML or PDF output, where code chunks are executed upon compilation and visualization outputs are directly placed into the output, or they can be run like a script entirely or each code chunk separately. When run (not knitted), objects defined by the executed code are available in the environment. Text can be formatted using the Markdown syntax. For example, a top-level section title is specified by # and a title of a section one level lower by ##. R Markdown documents are also the basis of this book, with each chapter written in a separate R Markdown file. This lets you use the book in an interactive fashion. When opened in RStudio, you can knit an R Markdown document by clicking the Knit button at the top of the source panel. To run the chunks of code in an R Markdown file, you can click on the Run button also on the source panel and select an option from the drop-down menu. For example, Run All runs all the chunks in the document. Alternatively, individual chunks can be executed by clicking the green right-pointing triangle in the upper right corner of each chunk. R Markdown document opened in the source panel. 1.2.4 Workspace management Using R projects in combination with Git (covered in Tutorial 6) is the essence of efficient workspace management in R. All files that belong together are organised within one directory. This can be regarded as the project directory and is typically congruent with what belongs to the respective Git repository. By keeping an organized workspace, another person (or your future self) can find relevant files, run your code and reproduce your analysis workflow easily. 1.2.4.1 R projects RStudio also allows you to work with R projects. An R project is a collection of files and folders that you use for a specific analysis or data project. An R project makes it easier to organize and manage your files and keep track of your work. To create a new R project, go to the File menu and select New Project…. This will open the New Project dialog, where you can choose where to save your project and what type of project to create. The current project that you are working on is shown on the upper right corner of the RStudio interface. Here you can also switch between existing projects or create a new one. R Project menu. When starting a new project, a file &lt;project_name.Rproj&gt; is created. It sits in the project directory and stores information about your last session (settings, open files, etc.) and optionally (not recommended) the environment of that session. The use of R projects also automatically enables useful features in RStudio for easy package, website, or book building and lets you manage Git for the repository corresponding to the project. When you want to continue working on an existing R project, you can start a new session by clicking on your &lt;project_name.Rproj&gt; file. This restores settings from your last R session, including the variables in your environment, and sets your working directory to the project directory. Nevertheless, we recommend to start by emptying your environment and loading your data and variables using the code you previously wrote. That way, you ensure that your results are reproducible. 1.2.4.2 Folder structure Once you have created an R project, you can create new scripts and other files within the project. These files will be organized in a folder structure, which you can view and manage in the files, plots, help, etc. panel. For example, keep source files where R functions are defined in ./R (where . refers to the current project directory), data files in ./data and visualizations in ./fig. It’s advisable to write output files, created by the code of your project, to sub-directories within the project directory. To read and write from/to files you should use relative paths (relative to the project’s root directory), like any of the two equivalent following options: &gt; source(&quot;./R/my_r_script.R&quot;) &gt; source(&quot;R/my_r_script.R&quot;) A project directory should only contain code, data and outputs that belong to this one project. Stuff that may belong to multiple projects should be kept somewhere else. For example, keep original data (e.g., the raw data files that you created when collecting the data in the field, or data files you downloaded from the web) outside the project directory. Exceptions are small data files, which you can keep in ./data_raw. It is advisable to create a separate data directory outsidethe project (e.g., ~/data/, where ~ refers to your home directory) that holds all the original data you ever downloaded, or obtained from peers, or gathered yourself. Within such a data directory, you can put files from different sources into separate sub-directories and add a description file (e.g., ~/data/some_data_source/README) defining by whom, from where and when the data was obtained and defining data use policy. You can find an R project template in the GECO GitHub page. It shows an example of how you can organize your files into folders. Using such a template removes the overhead of designing a structure for each new project and can help you keep your work organized and make it easier to reuse and share your code. 1.3 Exercises 1.4 Solutions "],["programming_primers.html", "Chapter 2 Programming primers 2.1 Learning objectives 2.2 Tutorial 2.3 Exercises 2.4 Solutions", " Chapter 2 Programming primers Chapter lead author: Pepa Aran TBC Contents: Lecture (Beni): Models and data Base R variables, classes data frames loops conditional statements functions input and output intro to visualisation Performance assessment: link to my exercise, link to Dietze exercise 2.1 Learning objectives After you’ve gone over the lecture and solved the exercises, you should be able to: Install and load libraries and packages Read, inspect, visualise and write data frames Use loops, conditional statements and functions in your code Organize your R project for data analysis Look for help 2.2 Tutorial 2.2.1 Libraries Packages, sometimes called libraries, are collections of R functions, data, and complied code in a well-defined format. R comes with a standard set of packages (including base R, utils, stats…) and other packages targeted for specific applications are available for download and installation. Once installed, you need to load them each time you start a new R session to use them. For example, the {tidyverse} package is used for data wrangling and will be covered in this course. This is a special package which loads many other packages in the background (like {readr}, {ggplot2}, etc.). You can install a new package as follows: install.packages(&quot;tidyverse&quot;) Then, you can load it with the following code. Note that now the name of the package is not in quotation marks. library(tidyverse) You can now use the functions and features provided by the {tidyverse} package in your R scripts. At any time, you can see a list of your installed packages on the source panel with the following command: library() And a list of the packages currently loaded: search() ## [1] &quot;.GlobalEnv&quot; &quot;package:stats&quot; &quot;package:graphics&quot; ## [4] &quot;package:grDevices&quot; &quot;package:datasets&quot; &quot;renv:shims&quot; ## [7] &quot;package:utils&quot; &quot;package:methods&quot; &quot;Autoloads&quot; ## [10] &quot;package:base&quot; This information can also be found on the Packages panel in RStudio. The loaded packages are shown with a tick mark. Finally, let’s install all the missing packages and load all required packages for this course: use_pkgs &lt;- c(&quot;dplyr&quot;, &quot;tidyr&quot;, &quot;readr&quot;, &quot;lubridate&quot;, &quot;stringr&quot;, &quot;purrr&quot;, &quot;ggplot2&quot;, &quot;tidyverse&quot;, &quot;visdat&quot;, &quot;terra&quot;, &quot;hexbin&quot;, &quot;jsonlite&quot;, &quot;MODISTools&quot;, &quot;forcats&quot;, &quot;yardstick&quot;, &quot;recipes&quot;, &quot;caret&quot;, &quot;broom&quot;, &quot;skimr&quot;, &quot;cowplot&quot;, &quot;scico&quot;, &quot;hwsdr&quot;, &quot;usethis&quot;, &quot;renv&quot;, &quot;rsample&quot;, &quot;modelr&quot;) new_pkgs &lt;- use_pkgs[!(use_pkgs %in% installed.packages()[, &quot;Package&quot;])] if (length(new_pkgs) &gt; 0) install.packages(new_pkgs) ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/tidyverse_1.3.2.tgz&#39; ... ## OK [downloaded 411 Kb in 0.9 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/latest/src/contrib/dbplyr_2.3.0.tar.gz&#39; ... ## OK [downloaded 688.4 Kb in 0.6 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/assertthat_0.2.1.tgz&#39; ... ## OK [downloaded 51.5 Kb in 0.7 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/blob_1.2.3.tgz&#39; ... ## OK [downloaded 45 Kb in 0.7 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/DBI_1.1.3.tgz&#39; ... ## OK [downloaded 728.3 Kb in 1.2 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/dtplyr_1.2.2.tgz&#39; ... ## OK [downloaded 319.8 Kb in 0.9 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/data.table_1.14.6.tgz&#39; ... ## OK [downloaded 2.2 Mb in 1.2 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/googledrive_2.0.0.tgz&#39; ... ## OK [downloaded 1.8 Mb in 1.2 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/latest/src/contrib/gargle_1.3.0.tar.gz&#39; ... ## OK [downloaded 337 Kb in 0.9 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/uuid_1.1-0.tgz&#39; ... ## OK [downloaded 67.2 Kb in 0.8 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/googlesheets4_1.0.1.tgz&#39; ... ## OK [downloaded 477.3 Kb in 0.9 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/cellranger_1.1.0.tgz&#39; ... ## OK [downloaded 99.1 Kb in 0.8 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/rematch_1.0.1.tgz&#39; ... ## OK [downloaded 11.9 Kb in 0.6 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/ids_1.0.1.tgz&#39; ... ## OK [downloaded 117.3 Kb in 0.8 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/haven_2.5.1.tgz&#39; ... ## OK [downloaded 1021.6 Kb in 1 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/readxl_1.4.1.tgz&#39; ... ## OK [downloaded 1.5 Mb in 1.1 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/reprex_2.0.2.tgz&#39; ... ## OK [downloaded 481.6 Kb in 0.9 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/rvest_1.0.3.tgz&#39; ... ## OK [downloaded 205.6 Kb in 0.8 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/selectr_0.4-2.tgz&#39; ... ## OK [downloaded 477.6 Kb in 1 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/latest/src/contrib/MODISTools_1.1.4.tar.gz&#39; ... ## OK [downloaded 236.5 Kb in 1.1 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/sf_1.0-9.tgz&#39; ... ## OK [downloaded 85.4 Mb in 4.9 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/classInt_0.4-8.tgz&#39; ... ## OK [downloaded 484.9 Kb in 0.9 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/e1071_1.7-12.tgz&#39; ... ## OK [downloaded 656.4 Kb in 1 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/proxy_0.4-27.tgz&#39; ... ## OK [downloaded 186 Kb in 0.8 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/latest/src/contrib/s2_1.1.2.tar.gz&#39; ... ## OK [downloaded 2.3 Mb in 1 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/wk_0.7.1.tgz&#39; ... ## OK [downloaded 1.8 Mb in 1.2 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/units_0.8-1.tgz&#39; ... ## OK [downloaded 1.5 Mb in 1.2 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/latest/src/contrib/sp_1.6-0.tar.gz&#39; ... ## OK [downloaded 1010.5 Kb in 0.9 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/yardstick_1.1.0.tgz&#39; ... ## OK [downloaded 829.6 Kb in 0.9 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/hardhat_1.2.0.tgz&#39; ... ## OK [downloaded 776.7 Kb in 1 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/latest/src/contrib/recipes_1.0.4.tar.gz&#39; ... ## OK [downloaded 782.1 Kb in 0.8 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/clock_0.6.1.tgz&#39; ... ## OK [downloaded 8.1 Mb in 1.9 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/gower_1.0.1.tgz&#39; ... ## OK [downloaded 209.3 Kb in 0.8 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/ipred_0.9-13.tgz&#39; ... ## OK [downloaded 376 Kb in 0.8 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/prodlim_2019.11.13.tgz&#39; ... ## OK [downloaded 412.5 Kb in 0.9 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/latest/src/contrib/lava_1.7.1.tar.gz&#39; ... ## OK [downloaded 1.2 Mb in 0.8 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/future.apply_1.10.0.tgz&#39; ... ## OK [downloaded 149.1 Kb in 0.9 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/globals_0.16.2.tgz&#39; ... ## OK [downloaded 102.7 Kb in 0.9 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/future_1.30.0.tgz&#39; ... ## OK [downloaded 612.1 Kb in 1 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/listenv_0.9.0.tgz&#39; ... ## OK [downloaded 102.5 Kb in 0.7 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/latest/src/contrib/parallelly_1.34.0.tar.gz&#39; ... ## OK [downloaded 133.4 Kb in 1 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/numDeriv_2016.8-1.1.tgz&#39; ... ## OK [downloaded 110.6 Kb in 0.6 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/latest/src/contrib/progressr_0.13.0.tar.gz&#39; ... ## OK [downloaded 204.4 Kb in 0.8 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/SQUAREM_2021.1.tgz&#39; ... ## OK [downloaded 172.4 Kb in 0.9 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/latest/src/contrib/timeDate_4022.108.tar.gz&#39; ... ## OK [downloaded 284.5 Kb in 1 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/caret_6.0-93.tgz&#39; ... ## OK [downloaded 3.4 Mb in 1.5 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/foreach_1.5.2.tgz&#39; ... ## OK [downloaded 133.1 Kb in 0.8 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/iterators_1.0.14.tgz&#39; ... ## OK [downloaded 337.3 Kb in 0.9 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/ModelMetrics_1.2.2.2.tgz&#39; ... ## OK [downloaded 544.6 Kb in 1 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/plyr_1.8.8.tgz&#39; ... ## OK [downloaded 991.5 Kb in 1.2 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/pROC_1.18.0.tgz&#39; ... ## OK [downloaded 1.1 Mb in 1.4 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/reshape2_1.4.4.tgz&#39; ... ## OK [downloaded 325.6 Kb in 1 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/hwsdr_1.0.tgz&#39; ... ## OK [downloaded 34.2 Kb in 0.7 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/latest/src/contrib/raster_3.6-14.tar.gz&#39; ... ## OK [downloaded 567.6 Kb in 0.8 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/usethis_2.1.6.tgz&#39; ... ## OK [downloaded 771.6 Kb in 1 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/gert_1.9.2.tgz&#39; ... ## OK [downloaded 1.9 Mb in 1.3 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/credentials_1.3.2.tgz&#39; ... ## OK [downloaded 167.3 Kb in 1 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/zip_2.2.2.tgz&#39; ... ## OK [downloaded 196.3 Kb in 0.7 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/gh_1.3.1.tgz&#39; ... ## OK [downloaded 91.8 Kb in 0.7 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/gitcreds_0.1.2.tgz&#39; ... ## OK [downloaded 91.5 Kb in 0.7 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/ini_0.3.1.tgz&#39; ... ## OK [downloaded 13 Kb in 0.6 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/whisker_0.4.1.tgz&#39; ... ## OK [downloaded 64.1 Kb in 1.1 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/rsample_1.1.1.tgz&#39; ... ## OK [downloaded 485.6 Kb in 0.9 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/furrr_0.3.1.tgz&#39; ... ## OK [downloaded 994.3 Kb in 1.2 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/slider_0.3.0.tgz&#39; ... ## OK [downloaded 357.9 Kb in 0.9 secs] ## Retrieving &#39;https://mran.microsoft.com/snapshot/2023-01-08/bin/macosx/contrib/4.2/warp_0.2.0.tgz&#39; ... ## OK [downloaded 101.1 Kb in 0.7 secs] ## Installing assertthat [0.2.1] ... ## OK [installed binary] ## Moving assertthat [0.2.1] into the cache ... ## OK [moved to cache in 1.8 milliseconds] ## Installing blob [1.2.3] ... ## OK [installed binary] ## Moving blob [1.2.3] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing DBI [1.1.3] ... ## OK [installed binary] ## Moving DBI [1.1.3] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing dbplyr [2.3.0] ... ## OK [built from source] ## Moving dbplyr [2.3.0] into the cache ... ## OK [moved to cache in 1.6 milliseconds] ## Installing data.table [1.14.6] ... ## OK [installed binary] ## Moving data.table [1.14.6] into the cache ... ## OK [moved to cache in 1.6 milliseconds] ## Installing dtplyr [1.2.2] ... ## OK [installed binary] ## Moving dtplyr [1.2.2] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing gargle [1.3.0] ... ## OK [built from source] ## Moving gargle [1.3.0] into the cache ... ## OK [moved to cache in 1.4 milliseconds] ## Installing uuid [1.1-0] ... ## OK [installed binary] ## Moving uuid [1.1-0] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing googledrive [2.0.0] ... ## OK [installed binary] ## Moving googledrive [2.0.0] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing rematch [1.0.1] ... ## OK [installed binary] ## Moving rematch [1.0.1] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing cellranger [1.1.0] ... ## OK [installed binary] ## Moving cellranger [1.1.0] into the cache ... ## OK [moved to cache in 1.4 milliseconds] ## Installing ids [1.0.1] ... ## OK [installed binary] ## Moving ids [1.0.1] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing googlesheets4 [1.0.1] ... ## OK [installed binary] ## Moving googlesheets4 [1.0.1] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing haven [2.5.1] ... ## OK [installed binary] ## Moving haven [2.5.1] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing readxl [1.4.1] ... ## OK [installed binary] ## Moving readxl [1.4.1] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing reprex [2.0.2] ... ## OK [installed binary] ## Moving reprex [2.0.2] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing selectr [0.4-2] ... ## OK [installed binary] ## Moving selectr [0.4-2] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing rvest [1.0.3] ... ## OK [installed binary] ## Moving rvest [1.0.3] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing tidyverse [1.3.2] ... ## OK [installed binary] ## Moving tidyverse [1.3.2] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing proxy [0.4-27] ... ## OK [installed binary] ## Moving proxy [0.4-27] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing e1071 [1.7-12] ... ## OK [installed binary] ## Moving e1071 [1.7-12] into the cache ... ## OK [moved to cache in 1.6 milliseconds] ## Installing classInt [0.4-8] ... ## OK [installed binary] ## Moving classInt [0.4-8] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing wk [0.7.1] ... ## OK [installed binary] ## Moving wk [0.7.1] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing s2 [1.1.2] ... ## OK [built from source] ## Moving s2 [1.1.2] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing units [0.8-1] ... ## OK [installed binary] ## Moving units [0.8-1] into the cache ... ## OK [moved to cache in 2.1 milliseconds] ## Installing sf [1.0-9] ... ## OK [installed binary] ## Moving sf [1.0-9] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing sp [1.6-0] ... ## OK [built from source] ## Moving sp [1.6-0] into the cache ... ## OK [moved to cache in 1.4 milliseconds] ## Installing MODISTools [1.1.4] ... ## OK [built from source] ## Moving MODISTools [1.1.4] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing hardhat [1.2.0] ... ## OK [installed binary] ## Moving hardhat [1.2.0] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing yardstick [1.1.0] ... ## OK [installed binary] ## Moving yardstick [1.1.0] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing clock [0.6.1] ... ## OK [installed binary] ## Moving clock [0.6.1] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing gower [1.0.1] ... ## OK [installed binary] ## Moving gower [1.0.1] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing globals [0.16.2] ... ## OK [installed binary] ## Moving globals [0.16.2] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing listenv [0.9.0] ... ## OK [installed binary] ## Moving listenv [0.9.0] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing parallelly [1.34.0] ... ## OK [built from source] ## Moving parallelly [1.34.0] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing future [1.30.0] ... ## OK [installed binary] ## Moving future [1.30.0] into the cache ... ## OK [moved to cache in 1.6 milliseconds] ## Installing future.apply [1.10.0] ... ## OK [installed binary] ## Moving future.apply [1.10.0] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing numDeriv [2016.8-1.1] ... ## OK [installed binary] ## Moving numDeriv [2016.8-1.1] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing progressr [0.13.0] ... ## OK [built from source] ## Moving progressr [0.13.0] into the cache ... ## OK [moved to cache in 1.6 milliseconds] ## Installing SQUAREM [2021.1] ... ## OK [installed binary] ## Moving SQUAREM [2021.1] into the cache ... ## OK [moved to cache in 2.1 milliseconds] ## Installing lava [1.7.1] ... ## OK [built from source] ## Moving lava [1.7.1] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing prodlim [2019.11.13] ... ## OK [installed binary] ## Moving prodlim [2019.11.13] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing ipred [0.9-13] ... ## OK [installed binary] ## Moving ipred [0.9-13] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing timeDate [4022.108] ... ## OK [built from source] ## Moving timeDate [4022.108] into the cache ... ## OK [moved to cache in 1.7 milliseconds] ## Installing recipes [1.0.4] ... ## OK [built from source] ## Moving recipes [1.0.4] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing iterators [1.0.14] ... ## OK [installed binary] ## Moving iterators [1.0.14] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing foreach [1.5.2] ... ## OK [installed binary] ## Moving foreach [1.5.2] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing ModelMetrics [1.2.2.2] ... ## OK [installed binary] ## Moving ModelMetrics [1.2.2.2] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing plyr [1.8.8] ... ## OK [installed binary] ## Moving plyr [1.8.8] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing pROC [1.18.0] ... ## OK [installed binary] ## Moving pROC [1.18.0] into the cache ... ## OK [moved to cache in 1.7 milliseconds] ## Installing reshape2 [1.4.4] ... ## OK [installed binary] ## Moving reshape2 [1.4.4] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing caret [6.0-93] ... ## OK [installed binary] ## Moving caret [6.0-93] into the cache ... ## OK [moved to cache in 1.4 milliseconds] ## Installing raster [3.6-14] ... ## OK [built from source] ## Moving raster [3.6-14] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing hwsdr [1.0] ... ## OK [installed binary] ## Moving hwsdr [1.0] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing credentials [1.3.2] ... ## OK [installed binary] ## Moving credentials [1.3.2] into the cache ... ## OK [moved to cache in 1.6 milliseconds] ## Installing zip [2.2.2] ... ## OK [installed binary] ## Moving zip [2.2.2] into the cache ... ## OK [moved to cache in 1.6 milliseconds] ## Installing gert [1.9.2] ... ## OK [installed binary] ## Moving gert [1.9.2] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing gitcreds [0.1.2] ... ## OK [installed binary] ## Moving gitcreds [0.1.2] into the cache ... ## OK [moved to cache in 1.7 milliseconds] ## Installing ini [0.3.1] ... ## OK [installed binary] ## Moving ini [0.3.1] into the cache ... ## OK [moved to cache in 1.6 milliseconds] ## Installing gh [1.3.1] ... ## OK [installed binary] ## Moving gh [1.3.1] into the cache ... ## OK [moved to cache in 1.6 milliseconds] ## Installing whisker [0.4.1] ... ## OK [installed binary] ## Moving whisker [0.4.1] into the cache ... ## OK [moved to cache in 1.6 milliseconds] ## Installing usethis [2.1.6] ... ## OK [installed binary] ## Moving usethis [2.1.6] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing furrr [0.3.1] ... ## OK [installed binary] ## Moving furrr [0.3.1] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing warp [0.2.0] ... ## OK [installed binary] ## Moving warp [0.2.0] into the cache ... ## OK [moved to cache in 1.6 milliseconds] ## Installing slider [0.3.0] ... ## OK [installed binary] ## Moving slider [0.3.0] into the cache ... ## OK [moved to cache in 1.5 milliseconds] ## Installing rsample [1.1.1] ... ## OK [installed binary] ## Moving rsample [1.1.1] into the cache ... ## OK [moved to cache in 1.7 milliseconds] invisible(lapply(use_pkgs, require, character.only = TRUE)) 2.2.1.1 Other libraries and applications For this course, we will also need software that is not available as an R package. To work with other libraries and applications, you may need to install additional software on your computer. For example, to work with netcdf files in R, you would need to install the \"ncdf4\" library and the netCDF command-line tools: To install the \"ncdf4\" library, follow the same steps as above for installing an R library. To install the netCDF command-line tools, follow the instructions on the netCDF website. Once the \"ncdf4\" library and the netCDF command-line tools are installed, you can use them to work with .nc files in R. For example, you could use the nc_open() function from the \"ncdf4\" library to open a file. 2.2.2 Programming basics In this section, we will review the most basic programming elements (conditional statements, loops, functions…) for the R syntax. 2.2.2.1 Conditional statements In cases where we want certain statements to be executed or not, depending on a criterion, we can use conditional statements if, else if, and else. Conditionals are an essential feature of programming and available in all languages. The R syntax for conditional statements looks like this: if (temp &lt; 0.0){ is_frozen &lt;- TRUE } The evaluation of the criterion (here (temp &lt; 0.0)) has to return either TRUE or FALSE. Whenever the statement between parenthesis is true, the chunk of code between curly brackets is executed. Otherwise, nothing happens. if (temp &lt; 0.0){ is_frozen &lt;- TRUE } else { is_frozen &lt;- FALSE } You can also write a conditional that covers all possibilities, like the one above. When the temperature is below 0, the first chunk of code is executed. Whenever it is greater or equal that 0 (i.e. the condition returns FALSE) the second chunk of code is evaluated. You can also write more than two conditions, covering several cases. Conditionals are evaluated in order, so if the first condition is not true, it checks the second. If the second is false, it checks the third, and so on. The statements after else are evaluated when everything before was FALSE. 2.2.2.2 Loops Loops are another essential feature of programming. for and while loops exist in probably all programming languages. We introduce them here because they are a simple and powerful tool for solving many common tasks. for and while loops let us repeatedly execute the same set of commands, while changing an index or counter variable to take a sequence of different values. The following example calculates the sum of the first ten temperature values in df_small (from the previous chapter), by iteratively adding them together. temp_sum &lt;- 0 # initialize sum for (i in 1:10){ temp_sum &lt;- temp_sum + df_small$temp[i] } temp_sum Of course, this is equivalent to just using the sum() function. sum(df_small$temp[1:10]) Instead of directly telling R how many iterations it should do we can also define a condition. As long as the condition is TRUE, R will continue iterating. As soon as it is FALSE, R stops the loop. The following lines of code do the same operation as the for loop we just wrote. What is different? What is the same? i = 1 # initialize counter temp_sum &lt;- 0 # initialize sum while (i &lt;= 10){ temp_sum &lt;- temp_sum + df_small$temp[i] i = i+1 } temp_sum 2.2.2.3 Functions Often, analyses require many steps and your scripts may get excessively long. Over 2000 lines of code in one file are hard to digest. An important aspect of good programming is to avoid duplicating code. If the same sequence of multiple statements or functions are to be applied repeatedly to different objects, then it is usually advisable to bundle them into a new function and apply this single function to each object. This also has the advantage that if some requirement or variable name changes, it has to be edited only in one place. A further advantage of writing functions is that you can give the function an intuitively understandable name, so that your code reads like a sequence of orders given to a human. For example, the following code, converting temperature values provided in Fahrenheit to degrees Celsius, could be turned into a function. ## NOT ADVISABLE temp_soil &lt;- (temp_soil - 32) * 5 / 9 temp_air &lt;- (temp_air - 32) * 5 / 9 temp_leaf &lt;- (temp_leaf - 32) * 5 / 9 Functions are a set of instructions encapsulated within curly brackets ({}) that generate a desired outcome. Functions contain four main elements: They start with a name to describe their purpose, then they need arguments, which are a list of the objects being input, enclosed by curly brackets function(x){ ... } the code making up the body of the function, and lastly, within the body, a return statement indicting the output of the function. Below we define our own function convert_fahrenheit_to_celsius(): ## ADVISABLE convert_fahrenheit_to_celsius &lt;- function(temp_f){ # Convert values temp_c &lt;- (temp_f - 32) * 5 / 9 # Return statement temp_c } temp_soil &lt;- convert_fahrenheit_to_celsius(temp_soil) temp_air &lt;- convert_fahrenheit_to_celsius(temp_air) temp_leaf &lt;- convert_fahrenheit_to_celsius(temp_leaf) A good practice when writing a function is to document what the function does, the meaning and structure of every input (arguments) and the output (value) of the function. This helps you and others to reuse that function, without having to read and understand what the function does internally. For example, you can write it as a header in the body of the function, or as a header of the script where you define it. Furthermore, one should use the return() statement only for early returns (e.g. inside an if statement) and otherwise, R returns the result of the last evaluated expression. Note that, without the last line of the function’s body, our function wouldn’t return anything. convert_fahrenheit_to_celsius &lt;- function(temp_f){ # This function converts temperature values in Fahrenheit to Celsius # Arguments: # temp_f: numerical vector of temperature values in Fahrenheit # Value: # temp_c: numerical vector of temperatures in Celsius temp_c &lt;- (temp_f - 32) * 5 / 9 temp_c } Functions become increasingly important the more experienced one gets at coding. Using them minimises the amount of code being re-written, decreases accidental errors when retyping code and are key to keeping a clean workspace. Functions have their own environment, which means variables within the function are only ‘live’ or used when the function is running but are not saved to the global environment unless they are part of the output of the function. A good moment to think about using a function is when sections of code are being repeated again and again. Whenever possible, we should combine multiple processing steps that naturally belong together. Specifically, when the same sequence of steps must be applied to multiple datasets that have the same structure (variable names, etc.). Once such a function is created, we can apply it to the data in one go, instead of repeating the successive steps. Functions (particularly long ones) can be written to separate source files. These R scripts containing only function definitions can be saved in your ./R directory, to keep your workspace organized. Preferably, the file has the same name as the function. We can save the previous function in a script .R/convert_fahrenheit_to_celsius.R and load it later by running source(\".R/convert_fahrenheit_to_celsius\"). 2.2.3 Working with data frames In the first tutorial, we introduced data frames as an R object. Now, let’s get our hands on actual data for demonstrating how data is read, manipulated and written. As most of the code displayed in this book, the code chunks below are executable. You can try it out by opening the the book’s R project in RStudio. We are going to work with data from ecosystem flux measurements, taken by the eddy covariance technique, and provided as part of the FLUXNET2015 dataset (Pastorello2020?), which you can see here. The data we’re using below comes from a flux tower near Zürich (CH-Lae, located on the Laegern mountain between Regensberg and Baden and run by our colleagues at ETH). The data is stored as a Comma Separated Values file (.csv). This is a plain-text, and therefore a non-proprietary format. To follow the open science principles for data, distribute your data in a format that is non-proprietary and readable across platforms and applications. For example, avoid distributing your data as an Excel spreadsheat (.xlsx), or a Matlab data object (.mat), or an R data object (.RData, or .rds). 2.2.3.1 Reading data To import the data into the R environment, we use the function read_csv() from the {tidyverse} package. In other R code, you will also encounter the base R read.csv() function. However, read_csv() is much faster and reads data into a tidyverse-data frame (a tibble) which has some useful additional characteristics, on top of a common R data frame. To tell the function where the data is located, pass the data’s path as an argument. You can either use an absolute path, starting from C:/ on a Windows computer or ~/ on a Mac or Linux. Or, alternatively, you can provide a relative path, where ./ points to the present working directory and ../ is one level up, or ../../ is two levels up, etc. We recommend that you work with R projects and use relative paths, because the working directory is set to the root directory of the R project and relative paths will also work on another person’s computer, helping with reproducibility. df &lt;- readr::read_csv(&quot;./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv&quot;) print(df) ## # A tibble: 52,608 × 235 ## TIMEST…¹ TIMES…² TA_F_…³ TA_F_…⁴ TA_ERA TA_F TA_F_QC SW_IN…⁵ SW_IN…⁶ SW_IN…⁷ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.00e11 2.00e11 -9999 -9999 -2.22 -2.22 2 0 -9999 -9999 ## 2 2.00e11 2.00e11 -9999 -9999 -2.25 -2.25 2 0 -9999 -9999 ## 3 2.00e11 2.00e11 -9999 -9999 -2.28 -2.28 2 0 -9999 -9999 ## 4 2.00e11 2.00e11 -9999 -9999 -2.50 -2.50 2 0 -9999 -9999 ## 5 2.00e11 2.00e11 -9999 -9999 -2.72 -2.72 2 0 -9999 -9999 ## 6 2.00e11 2.00e11 -9999 -9999 -2.94 -2.94 2 0 -9999 -9999 ## 7 2.00e11 2.00e11 -9999 -9999 -3.17 -3.17 2 0 -9999 -9999 ## 8 2.00e11 2.00e11 -9999 -9999 -3.39 -3.39 2 0 -9999 -9999 ## 9 2.00e11 2.00e11 -9999 -9999 -3.61 -3.61 2 0 -9999 -9999 ## 10 2.00e11 2.00e11 -9999 -9999 -3.59 -3.59 2 0 -9999 -9999 ## # … with 52,598 more rows, 225 more variables: SW_IN_ERA &lt;dbl&gt;, SW_IN_F &lt;dbl&gt;, ## # SW_IN_F_QC &lt;dbl&gt;, LW_IN_F_MDS &lt;dbl&gt;, LW_IN_F_MDS_QC &lt;dbl&gt;, LW_IN_ERA &lt;dbl&gt;, ## # LW_IN_F &lt;dbl&gt;, LW_IN_F_QC &lt;dbl&gt;, LW_IN_JSB &lt;dbl&gt;, LW_IN_JSB_QC &lt;dbl&gt;, ## # LW_IN_JSB_ERA &lt;dbl&gt;, LW_IN_JSB_F &lt;dbl&gt;, LW_IN_JSB_F_QC &lt;dbl&gt;, ## # VPD_F_MDS &lt;dbl&gt;, VPD_F_MDS_QC &lt;dbl&gt;, VPD_ERA &lt;dbl&gt;, VPD_F &lt;dbl&gt;, ## # VPD_F_QC &lt;dbl&gt;, PA &lt;dbl&gt;, PA_ERA &lt;dbl&gt;, PA_F &lt;dbl&gt;, PA_F_QC &lt;dbl&gt;, P &lt;dbl&gt;, ## # P_ERA &lt;dbl&gt;, P_F &lt;dbl&gt;, P_F_QC &lt;dbl&gt;, WS &lt;dbl&gt;, WS_ERA &lt;dbl&gt;, WS_F &lt;dbl&gt;, … The file is automatically machine-readable because we have: Only one header row, containing the column (variable) names. Variables organised by columns, and observations by rows. Each column consists of a single data type (e.g., character, numeric, logical; see below for more info). Here, all columns are interpreted as numeric (`’). One value per cell. No merged cells. In short, the data frame is tidy. To understand the sort of object we work with, i.e. the class, we can do: class(df) ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Fundamentally, df is a data.frame. In addition, it is also of some other classes (spec_tbl_df\", \"tbl_df\", \"tbl\") which gives it additional features. Other types of data inputs and how to read them will be covered in Tutorial 5. 2.2.3.2 Understanding the data structure There are several base R functions to help you understand the structure of a data frame. Here is a non-exhaustive list of of them: Size dim() - Returns the dimensions of an object (here: number of rows and columns). nrow() - Returns the number of rows of an object. ncol() - Returns the number of columns of an object. Content head() - Returns the first 6 rows. tail() - Returns the last 6 rows. View() - Opens a window in the source panel in RStudio where you can look at the entire data set in the form of a table (It is not supported by the Jupyter environment). Names names() - Returns the column names (for data.frame objects it is synonymous to colnames()). rownames() - Returns the row names. Summary class() - Returns the classes of an object. str() - Returns the structure of an object and information about the class, length and content of each column. summary() - Returns generic statistics information, depending on the class of the object. For categorical variables it will show how common each class is, missing values, etc, and for numerical variables, the mean, quantiles, maximum and minimum values, etc. For example, the data frame df has 4018 rows and 334 columns: dim(df) ## [1] 52608 235 It is important to know the meaning of the column names and content. A description of standardized FLUXNET data variables is available here. A selection of available variables that we will use in subsequent chapters are: GPP (gC m\\(^{−2}\\) s\\(^{-1}\\)): Gross primary production WS (m s\\(^{-1}\\)): Horizontal wind speed USTAR (m s\\(^{-1}\\)): Friction velocity TA (\\(^{o}\\) C): Air temperature RH (%): Relative humidity (range 0–100%) PA (kPa): Atmospheric pressure G (W m\\(^{−2}\\)): Ground heat flux, not mandatory, but needed for the energy balance closure calculations NETRAD (W m\\(^{−2}\\)): Net radiation, not mandatory, but needed for the energy balance closure calculations SW_IN (W m\\(^{−2}\\)): Incoming shortwave radiation SW_IN_POT (W m\\(^−2\\)): Potential incoming shortwave radiation (top of atmosphere theoretical maximum radiation) PPFD_IN (\\(\\mu\\)mol photons m\\(^{−2}\\) s\\(^{-1}\\)): Incoming photosynthetic photon flux density P (mm): Precipitation total of each 30 or 60 minute period LW_IN (W m\\(^{−2}\\)): Incoming (down-welling) long-wave radiation SWC (%): Soil water content (volumetric), range 0–100% TS (\\(^{o}\\) C): Soil temperature CO2 (\\(\\mu\\)molCO2 mol\\(^{-1}\\)): Carbon dioxide (CO\\(_2\\)) mole fraction in moist air 2.2.3.3 Selecting data and entering the tidyverse df is a data frame. This is similar to a matrix and has two dimensions (rows and columns). If we want to extract specific data from it, we specify the indices, i.e. the “coordinates”, of the data. For two-dimensional objects (data frames, matrices), the first index refers to rows and the second to columns. For example, to refer to the element on the third row in the first column, we write: df[3,1] ## # A tibble: 1 × 1 ## TIMESTAMP_START ## &lt;dbl&gt; ## 1 200401010100 Reducing a data frame (tibble) to only the first columns can be done by: df[, 1] ## # A tibble: 52,608 × 1 ## TIMESTAMP_START ## &lt;dbl&gt; ## 1 200401010000 ## 2 200401010030 ## 3 200401010100 ## 4 200401010130 ## 5 200401010200 ## 6 200401010230 ## 7 200401010300 ## 8 200401010330 ## 9 200401010400 ## 10 200401010430 ## # … with 52,598 more rows The method of selecting parts of a data frame by index is quite flexible. For example, we may require the information in the third column for the first three rows. Putting a colon between two numbers, e.g. [1:3,], indicates we want to select the rows numbers starting at the first and ending with the second number. So here [1:3,] will give us rows one, two and three. df[1:3, 3] # reduces the data frame (tibble) to its first three rows and the 3rd column ## # A tibble: 3 × 1 ## TA_F_MDS ## &lt;dbl&gt; ## 1 -9999 ## 2 -9999 ## 3 -9999 To reduce the data frame (tibble) to several columns, the function c() is used. This outputs the data frame (tibble) reduced to the selected row or column numbers inside c(). df[, c(1,4,7)] ## # A tibble: 52,608 × 3 ## TIMESTAMP_START TA_F_MDS_QC TA_F_QC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 200401010000 -9999 2 ## 2 200401010030 -9999 2 ## 3 200401010100 -9999 2 ## 4 200401010130 -9999 2 ## 5 200401010200 -9999 2 ## 6 200401010230 -9999 2 ## 7 200401010300 -9999 2 ## 8 200401010330 -9999 2 ## 9 200401010400 -9999 2 ## 10 200401010430 -9999 2 ## # … with 52,598 more rows Another method is to select the columns by column names, i.e. giving as input a string vector with the name of each column we want to select (again, this is Base R notation). This is especially useful if the columns we want to select are not contiguous. For example: # Selecting data by name in base R df[, c(&quot;TIMESTAMP_START&quot;, &quot;TA_F_MDS&quot;, &quot;TA_F_MDS_QC&quot;)] ## # A tibble: 52,608 × 3 ## TIMESTAMP_START TA_F_MDS TA_F_MDS_QC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 200401010000 -9999 -9999 ## 2 200401010030 -9999 -9999 ## 3 200401010100 -9999 -9999 ## 4 200401010130 -9999 -9999 ## 5 200401010200 -9999 -9999 ## 6 200401010230 -9999 -9999 ## 7 200401010300 -9999 -9999 ## 8 200401010330 -9999 -9999 ## 9 200401010400 -9999 -9999 ## 10 200401010430 -9999 -9999 ## # … with 52,598 more rows In Tutorial 3, we will use the tidyverse, which is a set of R packages designed for working with tidy data and writing code in a way that makes your workflow more clear and understandable. A code chunk which does the same as above, but is written for the tidyverse can read as follows. select(df, 1) # reduces the data frame (tibble) to its first column ## # A tibble: 52,608 × 1 ## TIMESTAMP_START ## &lt;dbl&gt; ## 1 200401010000 ## 2 200401010030 ## 3 200401010100 ## 4 200401010130 ## 5 200401010200 ## 6 200401010230 ## 7 200401010300 ## 8 200401010330 ## 9 200401010400 ## 10 200401010430 ## # … with 52,598 more rows select(df, TIMESTAMP_START, TA_F_MDS, TA_F_MDS_QC) # reduces the data frame to columns specified by names ## # A tibble: 52,608 × 3 ## TIMESTAMP_START TA_F_MDS TA_F_MDS_QC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 200401010000 -9999 -9999 ## 2 200401010030 -9999 -9999 ## 3 200401010100 -9999 -9999 ## 4 200401010130 -9999 -9999 ## 5 200401010200 -9999 -9999 ## 6 200401010230 -9999 -9999 ## 7 200401010300 -9999 -9999 ## 8 200401010330 -9999 -9999 ## 9 200401010400 -9999 -9999 ## 10 200401010430 -9999 -9999 ## # … with 52,598 more rows As a further shortcut in tidyverse, we can use the pipe %&gt;% operator. The data frame is still reduced to its first column: df %&gt;% select(1) ## # A tibble: 52,608 × 1 ## TIMESTAMP_START ## &lt;dbl&gt; ## 1 200401010000 ## 2 200401010030 ## 3 200401010100 ## 4 200401010130 ## 5 200401010200 ## 6 200401010230 ## 7 200401010300 ## 8 200401010330 ## 9 200401010400 ## 10 200401010430 ## # … with 52,598 more rows We pipe the object df into the select() function with argument 1. Note that the pipe operator %&gt;% can be used on any function. It tells the function to interpret what’s coming from the left of %&gt;% as its first argument. For the remainder of the tutorial several variables will be required. The methods of data selection demonstrated above will be used below to get the desired variables. df_small &lt;- df %&gt;% select(TIMESTAMP_START, TA_F, PPFD_IN) Note: In the code above, an indentation was used to highlight which parts go together, which makes the code easy to understand. Indentations and line breaks have no meaning or function in R per se (unlike in other programming languages, e.g., Matlab, Python), but help to make the code easier to read. 2.2.3.4 Renaming TIMESTAMP_START, TA_F and PPFD_IN as variable names may be hard to remember and in this section you will have to type them a lot. Therefore we change their names to something more intelligible. df_small &lt;- df_small %&gt;% rename(time = TIMESTAMP_START, temp = TA_F, ppfd = PPFD_IN) 2.2.3.5 Writing data A data frame can be written to a CSV file by: write_csv(df_small, file = &quot;data/df_small.csv&quot;) Note that making a file publicly available as a .rds or .RData file violates the open science principles. It is not interoperable. Therefore, whenever possible, save your data in a format that is readable across platforms without requiring proprietary software. Hence use write_csv() whenever possible. We will encounter other non-proprietary formats that let you save and share more complex data structures in Tutorial 5. 2.2.3.6 Intro to visualisation Visualising data is an integral part of any data science workflow. In this section, we introduce just the very basics. In Tutorial 4, you will get introduced to additional methods for visualising data. Our data frame fluxes_subset contains three variables, one of which is time. In other words, we are dealing with a time series. Let’s look at the temporal course of temperature in the first 1440 time steps (corresponding to 30 days) as a line plot (type = \"l\"). plot(1:1440, df_small$temp[1:1440], type = &quot;l&quot;) Another useful way of looking, not at a temporal course, but rather at the distribution of your data, is to display a histogram. A histogram visualises the frequency or proportion of data that has a metric value that falls within a certain interval known as a ‘bin’. Below you will see the temperature on the x-axis split into these ‘bins’ ranging across 2°. The number of times a data point falls between say 2° to 4° is then tallied and displayed as the frequency on the y-axis. Here there are around 1500 temperature values between 2° and 4°. hist(df_small$temp, xlab = &quot;Temperature (°C)&quot;) Plots can be saved as files, as long as the file size does not get too large. It will write vector graphics as outputs, i.e. PDF. In base R, this can be done by: pdf(&quot;./figures/filename.pdf&quot;) hist(df_small$temp) 2.2.4 Where to find help The material covered in this course will give you a solid basis for your future projects. Even more so, it provides you with code examples that you can adapt to your own purposes. Naturally, you will face problems we did not cover in the course and you will need to learn more as you go. The good news is, you do not have to. Many people make their code available online and often others have faced similar problems. Modifying existing code might make it easier for you to get started. 2.2.4.1 Within R “I know the name of a function that might help solve the problem but I do not know how to use it.” Typing a ? in front of the function will open the documentation of the function, giving lots of information on the uses and options a function has. You have learned a few things about plots but you may not know how to make a boxplot: ?boxplot Running the above code will open the information on making boxplots in R. If you do know how a function works but need to be reminded of the arguments it takes, simply type: args(boxplot) “There must be a function that does task X but I do not know which one.” Typing ?? will call the function help.search(). Maybe you want to save a plot as a JPEG but you do not know how: ??jpeg Note that it only looks through your installed packages. 2.2.4.2 Online To search in the entire library of R go to the website rdocumentation.org or turn to a search engine of your choice. It will send you to the appropriate function documentation or a helpful forum where someone has already asked a similar question. Most of the time you will end up on stackoverflow.com, a forum where most questions have already been answered. 2.2.4.3 Error messages If you do not understand the error message, start by searching the web. Be aware, that this is not always useful as developers rely on the error catching provided by R. To be more specific add the name of the function and package you are using, to get a more detailed answer. 2.2.4.4 Asking for help If you cannot find a solution online, start by asking your friends and colleagues. Someone with more experience than you might be able and willing to help you. When asking for help it is important to think about how you state the problem. The key to receiving help is to make it as easy as possible to understand the issue your facing. Try to reduce what does not work to a simple example. Reproduce a problem with a simple data frame instead of one with thousands of rows. Generalize it in a way that people who do not do research in your field can understand the problem. If you are asking a question online in a forum include the output of sessionInfo() (it provides information about the R version, packages your using,…) and other information that can be helpful to understand the problem. stackoverflow.com has its own guidelines on how to ask a good question, which you should follow. If your question is well crafted and has not been answered before you can sometimes get an answer within 5 minutes. https://stackoverflow.com/help/how-to-ask Finally, many packages have a mailing list or allow you to open a query on the code repository, where you can ask specific questions. The same is true for R itself. The R-Help mailing list https://stat.ethz.ch/mailman/listinfo/r-help is read by many people. However, the tone of such mailing lists can be pretty dry and unwelcoming to new users. Be sure to use the right terminology or else you might get an answer pointing out your misuse of language instead of your problem. Also, be sure your question is valid. Or else you won’t get an answer. 2.3 Exercises 2.4 Solutions "],["data_wrangling.html", "Chapter 3 Data wrangling 3.1 Learning objectives 3.2 Tutorial 3.3 Extra material 3.4 Exercises 3.5 Solutions", " Chapter 3 Data wrangling Chapter lead author: Benjamin Stocker 3.1 Learning objectives TBC 3.2 Tutorial Exploratory data analysis - the transformation, visualization, and modelling of data - is the central part of any (geo-) data science workflow and typically takes up a majority of the time we spend on a research project. The transformation of data often turns out to be particularly (and often surprisingly) time-demanding. Therefore, it is key to master typical steps of data transformation, and to implement them in a transparent fashion and efficiently - both in terms of robustness against coding errors (“bugs”) and in terms of code execution speed. This chapter introduces typical transformation steps (the reduction, filtering, aggregation, and combination of data), applied to tabular data, and implemented using the R tidyverse “dialect” (see below). We refer to data wrangling here to encompass the steps for preparing the data set prior to modelling - including, the combination of variables from different data sources, the removal of bad data, and the aggregation of data to the desired resolution or granularity (e.g., averaging over all time steps in a day, or over all replicates in a sample). In contrast, pre-processing refers to the additional steps that are either required by the the specific machine learning algorithm used with the data (e.g., centering and scaling for K-Nearest Neighbors or Neural Networks), the gap-filling of variables, or the transformation of variables guided by the resulting improvement of the predictive power of the machine learning model. Pre-processing is part of the modelling workflow and includes all steps that apply transformations that use parameters derived from the data. We will introduce and discuss data pre-processing in Chapter @ref(supervised_ml). 3.2.1 Example data The example data used in this chapter are parallel time series of (gaseous) CO\\(_2\\) and water vapor exchange fluxes between the vegetation and the atmosphere, along with various meteorological variables measured in parallel. Quasi-continuous measurements of temporally changing gas exchange fluxes are measured with the eddy covariance technique which relies on the parallel quantification of vertical wind speeds and gas concentrations. The data is provided at half-hourly resolution for the site CH-Lae, located on the south slope of the Lägern mountain on the Swiss Plateau at 689 m a.s.l. in a mixed forest with a distinct seasonal course of active green leaves (a substantial portion of the trees are deciduous). The dataset is generated and formatted following standard protocols (FLUXNET2015). For more information of the variables in the dataset, see the FLUXNET2015 website and Pastorello et al., 2020 for a comprehensive documentation of variable definitions and methods. For our demonstrations, the following variables are the most relevant: TIMESTAMP_START: Hour and day of the start of the measurement period for which the respective row’s data is representative. Provided in a format of “YYYYMMDDhhmm”. TIMESTAMP_END: Hour and day of the end of the measurement period for which the respective row’s data is representative. Provided in a format of “YYYYMMDDhhmm”. TA_* (°C): Air temperature. SW_IN_* (W m\\(^{-2}\\)): Shortwave incoming radiation LW_IN_* (W m\\(^{-2}\\)): Longwave incoming radiation VPD_* (hPa): Vapor pressure deficit (the difference between actual and saturation water vapor pressure) PA_* (kPa): Atmospheric pressure P_* (mm): Precipitation WS_* (m \\(^{-1}\\)): Wind speed SWC_* (%): Volumetric soil water content GPP_* (\\(\\mu\\)mol CO\\(_2\\) m\\(^{-1}\\) s\\(^{-1}\\)): Gross primary production (the ecosystem-level gross CO\\(_2\\) uptake flux driven by photosynthesis) *_QC: Quality control information for the variable *. Important for us: NEE_*_QC is the quality control information for the net ecosystem CO\\(_2\\) exchange flux (NEE_*) and for GPP derived from the corresponding NEE estimate (GPP_*). 0 = measured, 1 = good quality gap-filled, 2 = medium, 3 = poor. Suffixes _* indicate that multiple estimates for respective variables are available and distinguished by different suffixes. For example, variables TA_* contain the same information, but are derived with slightly different assumptions and gap-filling techniques. The meanings of suffixes are described in Pastorello et al., 2020. 3.2.2 Required libraries Install missing packages and load all required packages for this tutorial. use_pkgs &lt;- c(&quot;dplyr&quot;, &quot;tidyr&quot;, &quot;readr&quot;, &quot;lubridate&quot;, &quot;stringr&quot;, &quot;purrr&quot;, &quot;visdat&quot;) new_pkgs &lt;- use_pkgs[!(use_pkgs %in% installed.packages()[, &quot;Package&quot;])] if (length(new_pkgs) &gt; 0) install.packages(new_pkgs) invisible(lapply(use_pkgs, require, character.only = TRUE)) 3.2.3 Tidyverse The tidyverse is a collection of R packages and functions that share a common design philosophy, enabling a particularly efficient implementation of transformation steps on tabular data. The most important data and function design principle of the tidyverse is that each function takes a data frame as its first argument and returns a data frame as its output. From this design principles, even the most convoluted code and implementation of data transformation steps fall into place and fast and error-free progression through exploratory data analysis is facilitated. Therefore, you will be introduced to the R tidyverse here and we heavily rely on this dialect of the R language throughout the remainder of this course. 3.2.4 Tabular data Tabular data is organised in rows and columns. Each column can be regarded as a vector of a certain type. Each row contains the same number of columns and each column contains the same type of values (for example numeric, or characters). Each row can be regarded as a separate instance of the same type, for example a record of simultaneously taken measurements, along with some attributes. For example, a data frame in R is tabular data. In Chapter @ref(data_variety), you will be introduced to other types of data. The most common format for tabular data is CSV (comma-separated-values), typically indicated by the file name suffix .csv. CSV is a text-based file format, readable across platforms and does not rely on proprietary software (as opposed to, for example, .xlsx). The first row in a CSV file typically specifies the name of the variable provided in the respective column. Let’s get started with working with our example data set and read it into R, as the variable hhdf. hhdf &lt;- read_csv(&quot;./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv&quot;) ## Rows: 52608 Columns: 235 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (235): TIMESTAMP_START, TIMESTAMP_END, TA_F_MDS, TA_F_MDS_QC, TA_ERA, TA... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. hhdf ## # A tibble: 52,608 × 235 ## TIMEST…¹ TIMES…² TA_F_…³ TA_F_…⁴ TA_ERA TA_F TA_F_QC SW_IN…⁵ SW_IN…⁶ SW_IN…⁷ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.00e11 2.00e11 -9999 -9999 -2.22 -2.22 2 0 -9999 -9999 ## 2 2.00e11 2.00e11 -9999 -9999 -2.25 -2.25 2 0 -9999 -9999 ## 3 2.00e11 2.00e11 -9999 -9999 -2.28 -2.28 2 0 -9999 -9999 ## 4 2.00e11 2.00e11 -9999 -9999 -2.50 -2.50 2 0 -9999 -9999 ## 5 2.00e11 2.00e11 -9999 -9999 -2.72 -2.72 2 0 -9999 -9999 ## 6 2.00e11 2.00e11 -9999 -9999 -2.94 -2.94 2 0 -9999 -9999 ## 7 2.00e11 2.00e11 -9999 -9999 -3.17 -3.17 2 0 -9999 -9999 ## 8 2.00e11 2.00e11 -9999 -9999 -3.39 -3.39 2 0 -9999 -9999 ## 9 2.00e11 2.00e11 -9999 -9999 -3.61 -3.61 2 0 -9999 -9999 ## 10 2.00e11 2.00e11 -9999 -9999 -3.59 -3.59 2 0 -9999 -9999 ## # … with 52,598 more rows, 225 more variables: SW_IN_ERA &lt;dbl&gt;, SW_IN_F &lt;dbl&gt;, ## # SW_IN_F_QC &lt;dbl&gt;, LW_IN_F_MDS &lt;dbl&gt;, LW_IN_F_MDS_QC &lt;dbl&gt;, LW_IN_ERA &lt;dbl&gt;, ## # LW_IN_F &lt;dbl&gt;, LW_IN_F_QC &lt;dbl&gt;, LW_IN_JSB &lt;dbl&gt;, LW_IN_JSB_QC &lt;dbl&gt;, ## # LW_IN_JSB_ERA &lt;dbl&gt;, LW_IN_JSB_F &lt;dbl&gt;, LW_IN_JSB_F_QC &lt;dbl&gt;, ## # VPD_F_MDS &lt;dbl&gt;, VPD_F_MDS_QC &lt;dbl&gt;, VPD_ERA &lt;dbl&gt;, VPD_F &lt;dbl&gt;, ## # VPD_F_QC &lt;dbl&gt;, PA &lt;dbl&gt;, PA_ERA &lt;dbl&gt;, PA_F &lt;dbl&gt;, PA_F_QC &lt;dbl&gt;, P &lt;dbl&gt;, ## # P_ERA &lt;dbl&gt;, P_F &lt;dbl&gt;, P_F_QC &lt;dbl&gt;, WS &lt;dbl&gt;, WS_ERA &lt;dbl&gt;, WS_F &lt;dbl&gt;, … Since the file is properly formatted, with variable names given in the first line of the file, the function read_csv() identifies them correctly as column names and interprets values in each column as values of a consistent type. We used the function read_csv() from the readr package (part of tidyverse) here for reading the CSV since it is faster than the base-R read.csv() and generates a nicely readable output when printing the object as is done above. 3.2.5 Variable selection For our further data exploration, we will reduce the data frame we are working with and select a reduced set of variables. Reducing the dataset can have the advantage of speeding up further processing steps, especially when the data is large. For the further steps in this chapter we will now subset our original data. We select the following variants of variables described above, plus some additional variables (further information in Pastorello et al., 2020): All variables with names starting with TIMESTAMP) All meteorological variables derived following the “final gap-filled method”, as indicated with names ending with _F. GPP estimates that are based on the nighttime decomposition method, using the “most representative” of different gap-filling versions, after having applied the variable u-star filtering method (GPP_NT_VUT_REF) and the corresponding quality control information (NEE_VUT_REF_QC) Soil water measured at different depths (variables starting with SWC_F_MDS_) Do not use any radiation variables derived with the “JSBACH” algorithm (not with a name that contains the string JSB) Flag indicating whether a time step is at night (NIGHT) This is implemented by: hhdf &lt;- select( hhdf, starts_with(&quot;TIMESTAMP&quot;), ends_with(&quot;_F&quot;), GPP_NT_VUT_REF, NEE_VUT_REF_QC, starts_with(&quot;SWC_F_MDS_&quot;), -contains(&quot;JSB&quot;), NIGHT ) This reduces our dataset from 235 available variables to 59 variables. Our data set now only contains the columns we will need in our further analysis. As you can see, select() is a powerful tool to apply multiple selection criteria on your data frame in one step. It takes many functions that make filtering the columns easier. For example, criteria can be formulated based on the variable names with starts_with(), ends_with, contains(), matches(), etc. Using these functions within select() can help if several column names start with the same characters or contain the same pattern and all need to be selected. If a minus (-) is added in front of a column name or one of the mentioned functions within select(), then R will not include the stated column(s). Note that the selection criteria are evaluated in the order we write them in the select() function call. You can find the complete reference for selecting variables here. 3.2.6 Time objects The automatic interpretation of the variables TIMESTAMP_START and TIMESTAMP_END by the function read_csv() is not optimal: class(hhdf$TIMESTAMP_START[[1]]) ## [1] &quot;numeric&quot; as.character(hhdf$TIMESTAMP_START[[1]]) ## [1] &quot;200401010000&quot; As we can see, it is considered by R as a numeric variable with 12 digits (“double-precision”, occupying 64 bits in computer memory). After printing the variable as a string, we can guess that the format is: YYYYMMDDhhmm. The lubridate (R-lubridate?) package is designed to facilitate processing date and time objects. Knowing the format of the timestamp variables in our dataset, we can use ymd_hm() to convert them to actual date-time objects. dates &lt;- ymd_hm(hhdf$TIMESTAMP_START) dates[1] ## [1] &quot;2004-01-01 UTC&quot; Working with such date-time objects facilitates typical operations on time series. For example, adding one day can be done by: nextday &lt;- dates + days(1) nextday[1] ## [1] &quot;2004-01-02 UTC&quot; The following returns the month of each date object: month(dates[1]) ## [1] 1 The number 1 stands for the month of the year, i.e., January. You can find more information on formatting dates and time within the tidyverse here, and a complete reference of the lubridate package is available here. 3.2.7 Variable (re-) definition Since read_csv() did not interpret the TIMESTAMP_* variables as desired, we may convert the entire column in the data frame into a date-time object. In base-R, we would do this by: hhdf$TIMESTAMP_START &lt;- ymd_hm(hhdf$TIMESTAMP_START) Modifying existing or creating new variables (columns) in a data frame is done in the tidyverse using the function mutate(). The equivalent statement is: hhdf &lt;- mutate(hhdf, TIMESTAMP_START = ymd_hm(TIMESTAMP_START)) Note that in the code chunk above, the function mutate() is from the tidyverse package dplyr. It takes a dataframe as its first argument (here hhdf) and returns a dataframe as its output. You will encounter an alternative, but equivalent, syntax in the following form: hhdf &lt;- hhdf |&gt; mutate(TIMESTAMP_START = ymd_hm(TIMESTAMP_START)) Here, the pipe operator |&gt; is used. It “pipes” the object evaluated on its left side into the function on its right side, where the object takes the place of (but is not spelled out as) the first argument of that function. Using the pipe operator can have the advantage of facilitating the separation, removal, inserting, or re-arranging of individual transformation steps. Arguably, it facilitates reading code, especially for complex data transformation workflows. Therefore, you will encounter the pipe operator frequently throughout the remainder of this course. Mutating both our timestamp variables could be written as mutate(TIMESTAMP_START = ymd_hm(TIMESTAMP_START), TIMESTAMP_END = ymd_hm(TIMESTAMP_END)). Sometimes, such multiple-variable mutate statements can get quite long. A handy short version of this can be implemented using across(): hhdf &lt;- hhdf |&gt; mutate(across(starts_with(&quot;TIMESTAMP_&quot;), ymd_hm)) We will encounter more ways to use mutate later in this tutorial. A complete reference to mutate() is available here. 3.2.8 Axes of variation Tabular data is two-dimensional (rows \\(\\times\\) columns), but not all two-dimensional data is tabular. In Chapter @ref(data_variety), you will encounter raster data - a two-dimensional array of data (a matrix) representing variables on an evenly spaced grid, for example pixels in remotely sensed imagery. For example the volcano data (provided as an example dataset in R) is a 2-dimensional array, each column contains the same variable, and no variable names are provided. dim(volcano) ## [1] 87 61 volcano[1:5, 1:5] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 100 100 101 101 101 ## [2,] 101 101 102 102 102 ## [3,] 102 102 103 103 103 ## [4,] 103 103 104 104 104 ## [5,] 104 104 105 105 105 In the volcano dataset, rows and columns represent different geographic positions in latitude and longitude, respectively. The volcano data is not tabular data. Another typical example for non-tabular data are climate model outputs. They are typically given for each pixel along a longitudinal, latitudinal, and vertical axis, and for multiple time steps. Such data is four-dimensional and, as such, definitely not tabular. However, tabular data, although formatted in two dimensions by rows and columns, may represent data that varies along multiple axes. Our example data contains values recorded at each half-hourly time interval over the course of eleven years (check by nrow(hhdf)/(2*24*365)). The data is recorded at a site, located in the temperate climate zone, where solar radiation and therefore also other meteorological variables and ecosystem fluxes vary substantially over the course of a day and over the course of a year. Although not explicitly separated, the date-time object thus encodes information along multiple axes of variation in the data. For example, over the course of one day (2*24 rows in our data), the shortwave incoming radiation SW_IN_F varies over a typical diurnal cycle: plot(hhdf[1:(2*24),]$TIMESTAMP_START, hhdf[1:(2*24),]$SW_IN_F, type = &quot;l&quot;) Over the course of an entire year, shortwave incoming radiation varies with the seasons, peaking in summer: plot(hhdf[1:(365*2*24),]$TIMESTAMP_START, hhdf[1:(365*2*24),]$SW_IN_F, type = &quot;l&quot;) All data frames have two dimensions, rows and columns. Our data frame is organised along half-hourly time steps in rows. As described above, these time steps belong to different days, months, and years, although these “axes of variation” are not reflected by the structure of the data frame and we do not have columns that indicate the day, month or year of each half-hourly time step. This would be redundant information since the date-time objects of columns TIMESTAMP_* contain this information. However, for certain applications, it may be useful to separate information regarding these axes of variation more explicitly. For example by: hhdf |&gt; mutate(year = year(TIMESTAMP_START), month = month(TIMESTAMP_START)) |&gt; select(TIMESTAMP_START, TIMESTAMP_END, year, month) # for displaying ## # A tibble: 52,608 × 4 ## TIMESTAMP_START TIMESTAMP_END year month ## &lt;dttm&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2004-01-01 00:00:00 2004-01-01 00:30:00 2004 1 ## 2 2004-01-01 00:30:00 2004-01-01 01:00:00 2004 1 ## 3 2004-01-01 01:00:00 2004-01-01 01:30:00 2004 1 ## 4 2004-01-01 01:30:00 2004-01-01 02:00:00 2004 1 ## 5 2004-01-01 02:00:00 2004-01-01 02:30:00 2004 1 ## 6 2004-01-01 02:30:00 2004-01-01 03:00:00 2004 1 ## 7 2004-01-01 03:00:00 2004-01-01 03:30:00 2004 1 ## 8 2004-01-01 03:30:00 2004-01-01 04:00:00 2004 1 ## 9 2004-01-01 04:00:00 2004-01-01 04:30:00 2004 1 ## 10 2004-01-01 04:30:00 2004-01-01 05:00:00 2004 1 ## # … with 52,598 more rows Note that we used mutate() here to create a new variable (column) in the data frame, as opposed to above where we overwrote an existing variable with the same function. Note also that these temporal axes are hierarchical (several half-hours within a day, several days within a year). Other examples of such a hierarchical structure of axes of variation are cantons (or provinces or counties) within countries, or (typical for ecological data) individuals within species within genera within families, or (typical for data collected in an experiment) sample within treatment. 3.2.9 Tidy data Data comes in many forms and shapes. For example, Excel provides a playground for even the wildest layouts of information in tabular form and merged cells as we will see in the Exercises. A data frame imposes a relatively strict formatting in named columns of equal length. But even data frames can come in various shapes - even if the information they contain is the same. df1 ## # A tibble: 36 × 3 ## year month co2_concentration ## &lt;dbl&gt; &lt;ord&gt; &lt;dbl&gt; ## 1 1959 Jan 315. ## 2 1959 Feb 316. ## 3 1959 Mar 316. ## 4 1959 Apr 318. ## 5 1959 May 318. ## 6 1959 Jun 318 ## 7 1959 Jul 316. ## 8 1959 Aug 315. ## 9 1959 Sep 314. ## 10 1959 Oct 313. ## # … with 26 more rows df2 ## # A tibble: 3 × 13 ## year Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1959 315. 316. 316. 318. 318. 318 316. 315. 314. 313. 315. 315. ## 2 1960 316. 317. 317. 319. 320. 319. 318. 316. 314 314. 315. 316. ## 3 1961 317. 318. 318. 319. 320. 320. 318. 317. 315. 315. 316. 317. df3 ## # A tibble: 12 × 4 ## month `1959` `1960` `1961` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jan 315. 316. 317. ## 2 Feb 316. 317. 318. ## 3 Mar 316. 317. 318. ## 4 Apr 318. 319. 319. ## 5 May 318. 320. 320. ## 6 Jun 318 319. 320. ## 7 Jul 316. 318. 318. ## 8 Aug 315. 316. 317. ## 9 Sep 314. 314 315. ## 10 Oct 313. 314. 315. ## 11 Nov 315. 315. 316. ## 12 Dec 315. 316. 317. There are advantages for interoperability and ease of use when data frames come with consistent layouts, adhering to certain design principles. We have learned that in tabular data, each row contains the same number of columns and each column contains the same type of values (for example numeric, or characters). And that each row can be regarded as a separate instance of the same type, for example a record of simultaneously taken measurements, along with some attributes. Following these principles strictly leads to tidy data. In essence, quoting Wickham, data is tidy if: Each variable has its own column. Each observation has its own row. Each value has its own cell. The concept of tidy data can even be taken further by understanding a “value” as any object type, e.g. a list or a data frame. This leads to a list or data frame “nested” within a data frame. You will learn more about this below. The contents of this tutorial are inspired by the (freely available online) book *R for Data Science* by Grolemund &amp; Wickham.](https://r4ds.had.co.nz/).) 3.2.10 Aggregating data Aggregating data refers to collapsing a larger set of values into a smaller set of values that are derived from the larger set. For example, we can aggregate over all \\(N\\) rows in a data frame (\\(N\\times M\\)), calculating the sum for each of the \\(M\\) columns. This returns a data frame (\\(1 \\times M\\)) with the same number of columns as the initial data frame, but only one row. Often, aggregations are done not across all rows but for rows within \\(G\\) groups of rows. This yiels a data frame (\\(G \\times M\\)) with the number of rows corresponding to the number of groups. Let’s say we want to calculate the mean of half-hourly shortwave radiation within each day. That is, to aggregate our half-hourly data to daily data by taking a mean. There are two pieces of information needed for an aggregation step: The factor (or “axis of variation”), here days, that groups a vector of values for collapsing it into a single value, and the function used for collapsing values, here, the mean() function. This function should take a vector as an argument and return a single value as an output. These two steps are implemented by the dplyr functions group_by() and summarise(). This aggregation workflow is implemented by the following code: ddf &lt;- hhdf |&gt; mutate(date = as_date(TIMESTAMP_START)) |&gt; # converts the ymd_hm-formatted date-time object to a date-only object (ymd) group_by(date) |&gt; summarise(SW_IN_F = mean(SW_IN_F)) The seasonal course can now be more clearly be visualized with the data aggregated to daily values. plot(ddf[1:365,]$date, ddf[1:365,]$SW_IN_F, type = &quot;l&quot;) We can also apply multiple aggregation functions to different variables simultaneously. In the example below, we aggregate half-hourly data to daily data by… taking the daily mean GPP counting the number of half-hourly data points by day counting the number of measured (not gap-filled) data points taking the mean shortwave radiation Finally, we calculate the fraction of measured underlying half-hourly data from which the aggregation is calculated and we save the daily data frame as a CSV file for later use. ddf &lt;- hhdf %&gt;% mutate(date = as_date(TIMESTAMP_START)) %&gt;% # converts time object to a date object group_by(date) %&gt;% summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE), n_datapoints = n(), # counts the number of observations per day n_measured = sum(NEE_VUT_REF_QC == 0), # counts the number of actually measured data (excluding gap-filled and poor quality data) SW_IN_F = mean(SW_IN_F, na.rm = TRUE), # we will use this later .groups = &#39;drop&#39; # to un-group the resulting data frame ) %&gt;% mutate(f_measured = n_measured / n_datapoints) # calculate the fraction of measured values over total observations write_csv(ddf, file = &quot;data/ddf.csv&quot;) ddf ## # A tibble: 1,096 × 6 ## date GPP_NT_VUT_REF n_datapoints n_measured SW_IN_F f_measured ## &lt;date&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2004-01-01 -0.0138 48 0 38.1 0 ## 2 2004-01-02 0.768 48 0 23.9 0 ## 3 2004-01-03 0.673 48 0 54.1 0 ## 4 2004-01-04 -0.322 48 0 41.7 0 ## 5 2004-01-05 0.841 48 0 17.4 0 ## 6 2004-01-06 1.22 48 0 40.5 0 ## 7 2004-01-07 0.215 48 0 31.6 0 ## 8 2004-01-08 1.11 48 0 58.4 0 ## 9 2004-01-09 1.44 48 0 11.9 0 ## 10 2004-01-10 0.364 48 0 27.6 0 ## # … with 1,086 more rows More info on how to group values using summarise functions here, or a summary on the inputs the function group_by() and summarise() take. Let’s Aggregating is related to nesting performed by the tidyr function nest(): hhdf |&gt; mutate(date = as_date(TIMESTAMP_START)) |&gt; group_by(date) |&gt; nest() ## # A tibble: 1,096 × 2 ## # Groups: date [1,096] ## date data ## &lt;date&gt; &lt;list&gt; ## 1 2004-01-01 &lt;tibble [48 × 20]&gt; ## 2 2004-01-02 &lt;tibble [48 × 20]&gt; ## 3 2004-01-03 &lt;tibble [48 × 20]&gt; ## 4 2004-01-04 &lt;tibble [48 × 20]&gt; ## 5 2004-01-05 &lt;tibble [48 × 20]&gt; ## 6 2004-01-06 &lt;tibble [48 × 20]&gt; ## 7 2004-01-07 &lt;tibble [48 × 20]&gt; ## 8 2004-01-08 &lt;tibble [48 × 20]&gt; ## 9 2004-01-09 &lt;tibble [48 × 20]&gt; ## 10 2004-01-10 &lt;tibble [48 × 20]&gt; ## # … with 1,086 more rows Here, the data frame has one row per date and therefore the same number of rows as the data frame ddf, but the data itself is not reduced by a summarising function. Instead, the data is kept at the half-hourly level, but it’s nested inside the new column data, which now contains a list of half-hourly data frames for each day. This is just a brief perspective of what nesting is about. More is explained in the Section Extra material below. More comprehensive tutorials on nesting and functional programming are available in Altman, Behrman and Wickham (2021) or in Wickham &amp; Grolemund (2017), Chapter 21. 3.2.11 Data cleaning Data cleaning is often a time-consuming task and decisions taken during data cleaning may be critical for analyses and modelling. In the following, we distinguish between cleaning formats, the identification (and removal) of “bad” data, and the gap-filling of missing or removed data. An excellent source for further reading is the Quartz Guide to Bad Data which provides an overview of how to deal with different types of bad data. 3.2.11.1 Cleaning formats As a general principle, we want to have machine readable data. Key for achieving machine-readability is that a cell should only contain one value of one type. Hence, for example, character strings should be kept in separate columns (as separate variables) from numeric data. Character strings can impose particular challenges for achieving machine-readability. Typically, they encode categorical or ordinal information, but are prone to spelling inconsistencies or errors that undermine the ordering or categorization. Here are typical examples for challenges working with character strings and lessons for avoiding problems: Often, character strings encode the units of a measurement, and entries are c(\"kg m-2\", \"kg/m2\", \"Kg / m2\", \"1000 g m-2\") . They are all equivalent, but “the machine” treats them as non-identical. To clean such data, one may compile a lookup-table to identify equivalent (but not identical) strings. Much better is to specify a consistent treatment of units before data collection. Even if the data is clean and contains a consistently spelled categorical variable in the form of a character string, R doesn’t necessarily treat it as categorical. For certain downstream steps of the workflow, it may be necessary to transform such a variable to one of type factor. For example, as entries of an unordered categorical variable, we have unique(df$gender) = c(\"female\", \"male\", \"non-binary\"). To treat them as categorical and not just mere character strings, we would have to do: df &lt;- df |&gt; mutate(gender = as.factor(gender)) Character strings may encode ordinal information. For example, entries specify quality control information and are one of c(\"good quality\", \"fair quality, \"poor quality\"). A challenge could be that the spelling is inconsistent (c(\"Good quality\", \"good quality\", …)). Using integers (positive natural numbers) instead of character strings avoids such challenges and enforces an order. The quality control variable NEE_VUT_REF_QC in our example dataset hhdf follows this approach: unique(hhdf$NEE_VUT_REF_QC) ## [1] 3 2 1 0 An entry like &gt;10 m is not a friend of a data scientist. Here, we have three pieces of information: &gt; as in “greater than”, 10, and m indicating the units. A machine-readable format would be obtained by creating separate columns for each piece of information. The &gt; should be avoided already at the stage of recording the data. Here, we may have to find a solution for encoding it in a machine readable manner (see Exercises). Can you think of more such examples? (-&gt; Exercises) String manipulations are usually required for cleaning data. The section Strings below demonstrates some simple examples. Note that a majority of machine learning algorithms and other statistical model types require all data to be numeric. Methods exist to convert categorical data into numeric data, as we will learn later. We re-visit data cleaning in the form of data preprocessing as part of the modelling workflow in Chapter @ref(supervised_ml). 3.2.11.2 Bad data Data may be “bad” for different reasons, including sensor error, human error, a data point representing a different population, or unsuitable measurement conditions. In this sense, data is “bad” if it doesn’t represent what it is assumed to represent. Its presence in analyses and modelling may undermine the model skill or even lead to spurious results. A goal of data cleaning typically is to remove bad data. But how to detect them? And how safe is it to remove them? A diversity of processes may generate bad data and it is often not possible to formulate rules and criteria for their identification a priori. Therefore, an understanding of the data and the data generation processes is important for the identification and treatment of bad data. Often, such an understanding is gained by repeated exploratory data analysis cycles, involving the visualization, transformation, and analysis of the data. Ideally, information about the quality of the data is provided as part of the dataset. Also other meta-information (e.g., sensor type, human recording the data, environmental conditions during the data collection) may be valuable for data cleaning purposes. In our example dataset, the column with suffices _QC provide such information (see ‘Example data’ section above) and an example for their use in data cleaning is given further below. Bad data may come in the form of outliers, which are commonly defined based on their value with respect to the distribution of all values of the same variable in a dataset. Hence, their identification most commonly relies on quantifying their distance from the center of the variable’s empirical distribution. The default boxplot() plotting function in R (which we will learn about more in Chapter @ref(data_vis)) shows the median (bold line in the center), the upper and lower quartiles (corresponding to the 25% and the 75% quantiles, often referred to as \\(Q_1\\) and \\(Q_3\\) , given by the upper and lower edge of the box plot) and the range of \\(( Q_1 - 1.5 (Q_3 - Q_1), Q_3 + 1.5 (Q_3 - Q_1))\\). Any point outside this range is plotted by a circle and labeled an “outlier”. However, this definition is very restrictive and may lead to a false labeling of outliers, in particular if they are drawn from a distribution with a fat tail or from asymmetrical distributions. Outliers may also be identified via multivariate distributions. We will re-visit such methods later, in Chapter @ref(regression_classification). For certain applications, outliers or anomalies may be the target of the investigation, not the noise in the data. This has spurred the field of anomaly detection which relies on machine learning algorithms for determining whether a value is anomalous, given a set of covariates. Sensor error may generate spurious values, identified, for example when a continuous variable attains the numerically identical value with a spuriously high frequency. XXX spurious? hhdf$GPP_NT_VUT_REF |&gt; table() |&gt; sort(decreasing = TRUE) |&gt; head() ## ## 5.18422 3.54996 1.3107 -5.57199 0.984756 2.49444 ## 32 22 19 18 17 17 Other processes may lead to spurious trends or drift in the data, for example caused by sensor degradation. Spurious step changes or change points in time series or in (multivariate) regressions may be related to the replacement or deplacement of the measuring device. Different methods and R libraries help identifying such cases (see for example this tutorial). Solutions have to be found for the remediation of such spurious patterns in the data on a case-by-case basis. 3.2.11.3 Handling missing data The question about when data is “bad” and whether to remove it is often critical. Such decisions are important to keep track of and should be reported as transparently as possible in publications. In reality, where the data generation process may start in the field with actual human beings writing notes in a lab book, and where the human collecting the data is often not the same as the human analyzing the data or writing the paper, it’s often more difficult to keep track of such decisions. As a general principle, it is advisable to design data records such that decisions made during the data collection process remain transparent throughout all stages of the workflow and that sufficient information be collected to enable later revisions of particularly critical decisions. In practice, this means that the removal of data and entire rows should be avoided and implemented only at the very last step if necessary (e.g., when passing the data into a model fitting function). Instead, information about whether data is bad or not should be kept in a separate, categorical, variable (a quality control variable, like *_QC variables in our example data hhdf). Data may be missing for several reasons. Some yield random patterns of missing data, others not. In the latter case, we can speak of informative missingness (Kuhn &amp; Johnson, 2003) and its information can be used for modelling. For categorical data, we may replace such data with \"none\" (instead of NA), while randomly missing data may be dropped altogether. Some machine learning algorithms (mainly tree-based methods, e.g., Random Forest) can handle missing values. However, when comparing the performance of alternative ML algorithms, they should be tested with the same data and removing missing data should be done beforehand. Most machine learning algorithms require missing values to be removed. That is, if any of the cells in one row has a missing value, the entire cell gets removed. This generally leads to a loss of information contained in the remaining variables that are not missing. Methods exist to impute missing values in order to avoid this information loss. However, the gain of data imputation has to be traded off against effects of associating the available variables with the imputed (knowingly wrong) values, and effects of data leakage have to be considered. Data imputation as part of the modelling process will be dealt with in Chapter @ref(supervised_ml). In our example dataset, some values of SWC_F_MDS_* are given as -9999. hhdf |&gt; select(TIMESTAMP_START, starts_with(&quot;SWC_F_MDS_&quot;)) |&gt; head() ## # A tibble: 6 × 9 ## TIMESTAMP_START SWC_F_MD…¹ SWC_F…² SWC_F…³ SWC_F…⁴ SWC_F…⁵ SWC_F…⁶ SWC_F…⁷ ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2004-01-01 00:00:00 -9999 -9999 -9999 -9999 -9999 -9999 -9999 ## 2 2004-01-01 00:30:00 -9999 -9999 -9999 -9999 -9999 -9999 -9999 ## 3 2004-01-01 01:00:00 -9999 -9999 -9999 -9999 -9999 -9999 -9999 ## 4 2004-01-01 01:30:00 -9999 -9999 -9999 -9999 -9999 -9999 -9999 ## 5 2004-01-01 02:00:00 -9999 -9999 -9999 -9999 -9999 -9999 -9999 ## 6 2004-01-01 02:30:00 -9999 -9999 -9999 -9999 -9999 -9999 -9999 ## # … with 1 more variable: SWC_F_MDS_4_QC &lt;dbl&gt;, and abbreviated variable names ## # ¹​SWC_F_MDS_1, ²​SWC_F_MDS_2, ³​SWC_F_MDS_3, ⁴​SWC_F_MDS_4, ⁵​SWC_F_MDS_1_QC, ## # ⁶​SWC_F_MDS_2_QC, ⁷​SWC_F_MDS_3_QC When reading the documentation of this specific dataset, we learn that -9999 is the code for missing data. We can replace such values in any column with NA using the dplyr function na_if(). hhdf &lt;- hhdf |&gt; na_if(-9999) This lets us visualise the data and its gaps with vis_miss() from the visdat package. Visualising missing data can be informative for making decisions about dropping rows with missing data versus removing predictors from the analysis (which would imply too much data removal). vis_miss( hhdf, cluster = FALSE, warn_large_data = FALSE ) For many applications, we want to filter the data so that the values of particular variables satisfy certain conditions. The dplyr function used for such tasks is filter(). As argument, it takes the expressions that specify the criterion for filtering using logical operators (&gt;, &gt;=, &lt;, ==, !-, ..., see Chapter @ref(getting_started)). Multiple filtering criteria can be combined with logical (boolean) operators: &amp;: logical AND |: logical OR ! logical NOT For example, if we wanted only those rows in our data where NEE is based on measured or good quality gap-filled NEE data, we write: hhdf |&gt; filter(NEE_VUT_REF_QC == 0 | NEE_VUT_REF_QC == 1) For evaluating multiple OR operations simultaneously, we can write alternatively and equivalently: hhdf |&gt; filter(NEE_VUT_REF_QC %in% c(0,1)) Note that filter() removes entire rows. In some cases this is undesired and it is preferred to replace bad quality values with NA. It is important to note that specifying a value as missing is information itself. Dropping an entire row leads to the loss of this information. For cases where we do not want to drop entire rows when applying filter(), we can just replace certain values with NA. In our case, where we want to retain only data where NEE is based on actual measurements or good quality gap-filling, we can do this by: hhdf |&gt; mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC %in% c(0,1), GPP_NT_VUT_REF, NA)) If we decide to drop a row containing NA in any of the variables later during the workflow, we can do this, for example using the useful tidyr function drop_na(). hhdf |&gt; drop_na() An excellent source for a more comprehensive introduction to missing data handling is given in Kuhn &amp; Johnson. After having applied some data reduction and cleaning steps above, let’s save the data frame in the form of a CSV file for use in later chapters. write_csv(hhdf, file = &quot;data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN.csv&quot;) 3.2.12 Combining relational data Often, data is spread across multiple files and tables and needs to be combined for the planned analysis. In the simplest case, data frames have a corresponding set of columns and we can “stack” them along rows: df4 ## # A tibble: 6 × 4 ## month `1959` `1960` `1961` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jan 315. 316. 317. ## 2 Feb 316. 317. 318. ## 3 Mar 316. 317. 318. ## 4 Apr 318. 319. 319. ## 5 May 318. 320. 320. ## 6 Jun 318 319. 320. df5 ## # A tibble: 6 × 4 ## month `1959` `1960` `1961` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jul 316. 318. 318. ## 2 Aug 315. 316. 317. ## 3 Sep 314. 314 315. ## 4 Oct 313. 314. 315. ## 5 Nov 315. 315. 316. ## 6 Dec 315. 316. 317. bind_rows(df4, df5) ## # A tibble: 12 × 4 ## month `1959` `1960` `1961` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jan 315. 316. 317. ## 2 Feb 316. 317. 318. ## 3 Mar 316. 317. 318. ## 4 Apr 318. 319. 319. ## 5 May 318. 320. 320. ## 6 Jun 318 319. 320. ## 7 Jul 316. 318. 318. ## 8 Aug 315. 316. 317. ## 9 Sep 314. 314 315. ## 10 Oct 313. 314. 315. ## 11 Nov 315. 315. 316. ## 12 Dec 315. 316. 317. …, or data frames have a corresponding set of rows (and in the same order) and we can “stack” them along columns df6 ## # A tibble: 12 × 3 ## month `1959` `1960` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jan 315. 316. ## 2 Feb 316. 317. ## 3 Mar 316. 317. ## 4 Apr 318. 319. ## 5 May 318. 320. ## 6 Jun 318 319. ## 7 Jul 316. 318. ## 8 Aug 315. 316. ## 9 Sep 314. 314 ## 10 Oct 313. 314. ## 11 Nov 315. 315. ## 12 Dec 315. 316. df7 ## # A tibble: 12 × 2 ## month `1961` ## &lt;ord&gt; &lt;dbl&gt; ## 1 Jan 317. ## 2 Feb 318. ## 3 Mar 318. ## 4 Apr 319. ## 5 May 320. ## 6 Jun 320. ## 7 Jul 318. ## 8 Aug 317. ## 9 Sep 315. ## 10 Oct 315. ## 11 Nov 316. ## 12 Dec 317. bind_cols(df6, df7) ## New names: ## • `month` -&gt; `month...1` ## • `month` -&gt; `month...4` ## # A tibble: 12 × 5 ## month...1 `1959` `1960` month...4 `1961` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;dbl&gt; ## 1 Jan 315. 316. Jan 317. ## 2 Feb 316. 317. Feb 318. ## 3 Mar 316. 317. Mar 318. ## 4 Apr 318. 319. Apr 319. ## 5 May 318. 320. May 320. ## 6 Jun 318 319. Jun 320. ## 7 Jul 316. 318. Jul 318. ## 8 Aug 315. 316. Aug 317. ## 9 Sep 314. 314 Sep 315. ## 10 Oct 313. 314. Oct 315. ## 11 Nov 315. 315. Nov 316. ## 12 Dec 315. 316. Dec 317. But beware! In particular the stacking along columns (bind_cols()) is very error-prone. Since a tidy data frame regards each row as an instance of associated measurements, the rows of the two data frames and their order must match exactly. Otherwise, an error is raised or (even worse) rows get associated when they shouldn’t be. In such cases, where information about a common set of observations is distributed across multiple data objects, we are dealing with relational data. The key for their combination (or “merging”) is a unique identification key - the column that is present in both data frames and which contains values along which the merging of the two data frames is performed. In our example from above, this is month, and we can use the dplyr function left_join(). df6 |&gt; slice(sample(1:n(), replace = FALSE)) |&gt; # re-shuffling rows left_join(df7, by = &quot;month&quot;) |&gt; arrange(month) # sort in ascending order ## # A tibble: 12 × 4 ## month `1959` `1960` `1961` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jan 315. 316. 317. ## 2 Feb 316. 317. 318. ## 3 Mar 316. 317. 318. ## 4 Apr 318. 319. 319. ## 5 May 318. 320. 320. ## 6 Jun 318 319. 320. ## 7 Jul 316. 318. 318. ## 8 Aug 315. 316. 317. ## 9 Sep 314. 314 315. ## 10 Oct 313. 314. 315. ## 11 Nov 315. 315. 316. ## 12 Dec 315. 316. 317. Note that here, we first re-shuffled (permuted) the rows of df6 for demonstration purposes, and arranged the output data frame again by month - an ordinal variable. left_join() is not compromised by the order of the rows, but instead relies on the unique identification key, specified by the argument by = \"month\", for associating (merging, joining) the two data frames. In some cases, multiple columns may act as the unique identification key in their combination (for example by = c(\"year\", \"month\")). Other variants of *_join() are available as described here. 3.3 Extra material 3.3.1 Functional programming I Above, we read a CSV table into R and applied several data transformation steps. In practice, we often have to apply the same data transformation steps repeatedly over a set of similar objects. This extra material section outlines an example workflow for demonstrating how to efficiently work with lists of similar objects - in particular, lists of data frames. Our aim is to read a set of files into R data frames and apply transformation steps to each data frame separately. Here, we will work with daily data, not half-hourly data. The daily data contains largely identical variables with consistent naming and units as in the half-hourly data (description above). Let’s start by creating a list of paths that point to the files with daily data. They are all located in the directory \"./data\" and share a certain string of characters in their file names \"_FLUXNET2015_FULLSET_DD_\". vec_files &lt;- list.files(&quot;./data&quot;, pattern = &quot;_FLUXNET2015_FULLSET_DD_&quot;, full.names = TRUE) print(vec_files) ## [1] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; ## [2] &quot;./data/FLX_FI-Hyy_FLUXNET2015_FULLSET_DD_1996-2014_1-3.csv&quot; ## [3] &quot;./data/FLX_FR-Pue_FLUXNET2015_FULLSET_DD_2000-2014_2-3.csv&quot; vec_files is now a vector of three files paths as character strings. To read in the three files and combine the three data frames (list_df below) into a list of data frames, we could use a for loop: list_df &lt;- list() for (ifil in vec_files){ list_df[[ifil]] &lt;- read_csv(ifil) } Repeatedly applying a function (here read_csv()) over a list similar objects is facilitated by the map*() family of functions from the purrr package. An (almost) equivalent statement is: list_df &lt;- map(as.list(vec_files), ~read_csv(.)) Here, map() applies the function read_csv() to elements of a list. Hence, we first have to convert the vector vec_files to a list. A list is always the first argument within the map() function. Note two new symbols (~ and .). The ~ always goes before the function that is repeatedly applied (or “mapped”) to elements of the list. The . indicates where the elements of the list would go if spelled out (e.g., here, read_csv(.) would be read_csv(\"./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv\") for the first iteration). The output of map() is again a list. There are many variants of the function map() that each have a specific use. A complete reference for all purrr functions is available here. A useful and more extensive tutorial on purrr is available here. The above map() call does not return a named list as our for loop created. But we can give each element of the returned list of data frames different names by: names(list_df) &lt;- vec_files # this makes it a named list Next, we will apply a similar data cleaning procedure to this data set as we did above for half-hourly data. To do so, we “package” the individual cleaning steps into a function … # function definition clean_data_dd &lt;- function(df){ df &lt;- df |&gt; # select only the variables we are interested in select( TIMESTAMP, ends_with(&quot;_F&quot;), GPP_NT_VUT_REF, NEE_VUT_REF_QC, starts_with(&quot;SWC_F_MDS_&quot;), -contains(&quot;JSB&quot;)) |&gt; # convert to a nice date object mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) |&gt; # set all -9999 to NA na_if(-9999) return(df) } … and apply this function to each data frame within our list of data frames: list_df &lt;- map(list_df, ~clean_data_dd(.)) Having different data frames as elements of a list may be impractical. Since we read in similarly formatted files and selected always the same variables in each data frame, all elements of the list of data frames list_df share the same columns. This suggests that we can collapse our list of data frames and “stack” data frames along rows. As described above, this can be done using bind_rows() and we can automatically create a new column \"siteid\" in the stacked data frame that takes the name of the corresponding list element. ddf_allsites &lt;- bind_rows(list_df, .id = &quot;siteid&quot;) ddf_allsites ## # A tibble: 18,993 × 21 ## siteid TIMESTAMP TA_F SW_IN_F LW_IN_F VPD_F PA_F P_F WS_F GPP_N…¹ ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ./data/FLX… 1997-01-01 -4.57 77.4 223. 0.565 82.6 0.4 0.559 0.697 ## 2 ./data/FLX… 1997-01-02 -3.34 45.6 235. 0.978 82.9 0 1.11 1.04 ## 3 ./data/FLX… 1997-01-03 0.278 74.1 239. 2.24 82.4 0 2.03 -0.242 ## 4 ./data/FLX… 1997-01-04 -1.88 58.1 250. 1.38 81.7 1.8 1.92 0.247 ## 5 ./data/FLX… 1997-01-05 -4.96 80.8 248. 1.16 82.3 0 0.407 0.520 ## 6 ./data/FLX… 1997-01-06 -4.48 59.6 237. 0.838 82.7 0 0.466 0.0182 ## 7 ./data/FLX… 1997-01-07 -3.15 45.5 234. 1.33 82.9 0 1.03 0.0777 ## 8 ./data/FLX… 1997-01-08 -2.45 76.7 222. 1.87 82.7 0 1.95 -0.484 ## 9 ./data/FLX… 1997-01-09 -2.43 47.6 251. 1.44 82.2 0 0.785 -0.379 ## 10 ./data/FLX… 1997-01-10 -3.09 39.6 242. 0.776 82.8 0 1.25 -0.552 ## # … with 18,983 more rows, 11 more variables: NEE_VUT_REF_QC &lt;dbl&gt;, ## # SWC_F_MDS_1 &lt;dbl&gt;, SWC_F_MDS_2 &lt;dbl&gt;, SWC_F_MDS_3 &lt;dbl&gt;, ## # SWC_F_MDS_1_QC &lt;dbl&gt;, SWC_F_MDS_2_QC &lt;dbl&gt;, SWC_F_MDS_3_QC &lt;dbl&gt;, ## # SWC_F_MDS_4 &lt;dbl&gt;, SWC_F_MDS_5 &lt;dbl&gt;, SWC_F_MDS_4_QC &lt;dbl&gt;, ## # SWC_F_MDS_5_QC &lt;dbl&gt;, and abbreviated variable name ¹​GPP_NT_VUT_REF A visualisation of missing data indicates that soil water content data (SWC_F_MDS_*) are often missing. vis_miss( ddf_allsites, cluster = FALSE, warn_large_data = FALSE ) 3.3.2 Strings The column siteid currently contains strings specifying the full paths of the files that were read in earlier. The next task is to extract the site name from these strings. The file names follow a clear pattern (this also highlights why naming files wisely can often make life a lot simpler). ddf_allsites$siteid |&gt; head() ## [1] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; ## [2] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; ## [3] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; ## [4] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; ## [5] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; ## [6] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; The paths each start with the subdirectory where they are located (\"./data/\"), then \"FLX_\", followed by the site name (the first three entries of the table containing data from all sites are for the site \"CH-Dav\"), and then some more specifications, including the years that respective files’ data cover. The stringr package (R-stringr?) (part of tidyverse) offers a set of functions for working with strings. Wikham (XXX) provide a more comprehensive introduction to working with strings. Here, we would like to extract the six characters, starting at position 12. The function str_sub() does that job. vec_sites &lt;- str_sub(vec_files, start = 12, end = 17) head(vec_sites) ## [1] &quot;CH-Dav&quot; &quot;FI-Hyy&quot; &quot;FR-Pue&quot; We can use this function to mutate all values of column \"siteid\", overwriting it with just these six characters. ddf_allsites &lt;- ddf_allsites |&gt; mutate(siteid = str_sub(siteid, start = 12, end = 17)) ddf_allsites ## # A tibble: 18,993 × 21 ## siteid TIMESTAMP TA_F SW_IN_F LW_IN_F VPD_F PA_F P_F WS_F GPP_NT_VUT…¹ ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 CH-Dav 1997-01-01 -4.57 77.4 223. 0.565 82.6 0.4 0.559 0.697 ## 2 CH-Dav 1997-01-02 -3.34 45.6 235. 0.978 82.9 0 1.11 1.04 ## 3 CH-Dav 1997-01-03 0.278 74.1 239. 2.24 82.4 0 2.03 -0.242 ## 4 CH-Dav 1997-01-04 -1.88 58.1 250. 1.38 81.7 1.8 1.92 0.247 ## 5 CH-Dav 1997-01-05 -4.96 80.8 248. 1.16 82.3 0 0.407 0.520 ## 6 CH-Dav 1997-01-06 -4.48 59.6 237. 0.838 82.7 0 0.466 0.0182 ## 7 CH-Dav 1997-01-07 -3.15 45.5 234. 1.33 82.9 0 1.03 0.0777 ## 8 CH-Dav 1997-01-08 -2.45 76.7 222. 1.87 82.7 0 1.95 -0.484 ## 9 CH-Dav 1997-01-09 -2.43 47.6 251. 1.44 82.2 0 0.785 -0.379 ## 10 CH-Dav 1997-01-10 -3.09 39.6 242. 0.776 82.8 0 1.25 -0.552 ## # … with 18,983 more rows, 11 more variables: NEE_VUT_REF_QC &lt;dbl&gt;, ## # SWC_F_MDS_1 &lt;dbl&gt;, SWC_F_MDS_2 &lt;dbl&gt;, SWC_F_MDS_3 &lt;dbl&gt;, ## # SWC_F_MDS_1_QC &lt;dbl&gt;, SWC_F_MDS_2_QC &lt;dbl&gt;, SWC_F_MDS_3_QC &lt;dbl&gt;, ## # SWC_F_MDS_4 &lt;dbl&gt;, SWC_F_MDS_5 &lt;dbl&gt;, SWC_F_MDS_4_QC &lt;dbl&gt;, ## # SWC_F_MDS_5_QC &lt;dbl&gt;, and abbreviated variable name ¹​GPP_NT_VUT_REF 3.3.3 Functional programming II Functions can be applied to a list of objects of any type. Therefore, map() is a powerful approach to “iterating” over multiple instances of the same object type and can be used for all sorts of tasks. In the following, list elements are data frames of daily data and the function lm() fits a linear regression model of GPP versus shortwave radiation to each sites’ data. We’ll learn more about fitting statistical models in R in Chapter @ref(regression_classification). list_linmod &lt;- map(list_df, ~lm(GPP_NT_VUT_REF ~ SW_IN_F, data = .)) Note how the . indicates where the elements of list_df go when evaluating the lm() function. This returns a list of linear model objects (the type of objects returned by the lm() function call). We can spin the functional programming concept further and apply (or map) the summary() function to the lm-model objects to get a list of useful statistics and metrics, and then further extract the element \"r.squared\" from that list as: list_linmod |&gt; map(summary) |&gt; # applyting a function map_dbl(&quot;r.squared&quot;) # extracting from a named list ## ./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv ## 0.4201802 ## ./data/FLX_FI-Hyy_FLUXNET2015_FULLSET_DD_1996-2014_1-3.csv ## 0.6415685 ## ./data/FLX_FR-Pue_FLUXNET2015_FULLSET_DD_2000-2014_2-3.csv ## 0.3772839 map_dbl() is a variant of the map() function that returns not a list, but a vector of numeric values of class “double” (hence, the name _dbl). Note further, that providing a character (\"r.squared\") as an argument instead of an (unquoted) function name, map() extracts the correspondingly named list element, instead of applying a function to a list element. When writing code for an analysis, it’s useful, if not essential, to understand the objects we’re working with, understand its type and shape, and make sense of the results of simple print &lt;object&gt; statements. Data frames are particularly handy as they provide an organisation of data that is particularly intuitive (variables along columns, observations along rows, values in cells). Here, we’re dealing with a list of linear model objects. Can such a list fit into the paradigm of tidy data frames? Yes, they can. Think of the linear model objects as ‘values’. Values don’t necessarily have to be scalars, but they can be of any type (class). tibble( siteid = vec_sites, linmod = list_linmod ) ## # A tibble: 3 × 2 ## siteid linmod ## &lt;chr&gt; &lt;named list&gt; ## 1 CH-Dav &lt;lm&gt; ## 2 FI-Hyy &lt;lm&gt; ## 3 FR-Pue &lt;lm&gt; The fact that cells can contain any type of object offers a powerful concept. Instead of a linear model object as in the example above, each cell may even contain another data frame. In such a case, we say that the data frame is no longer flat, but nested. The following creates a nested data frame, where the column data is defined by the list of data frames read from files above (list_df). tibble( siteid = vec_sites, data = list_df ) ## # A tibble: 3 × 2 ## siteid data ## &lt;chr&gt; &lt;named list&gt; ## 1 CH-Dav &lt;tibble [6,574 × 16]&gt; ## 2 FI-Hyy &lt;tibble [6,940 × 20]&gt; ## 3 FR-Pue &lt;tibble [5,479 × 10]&gt; We can achieve the same result by directly nesting the flat data frame holding all sites’ data (ddf_allsites). This is done by combining the group_by(), which we have encountered above when aggregating using summarise(), with the function nest() from the tidyr package. ddf_allsites |&gt; group_by(siteid) |&gt; nest() ## # A tibble: 3 × 2 ## # Groups: siteid [3] ## siteid data ## &lt;chr&gt; &lt;list&gt; ## 1 CH-Dav &lt;tibble [6,574 × 20]&gt; ## 2 FI-Hyy &lt;tibble [6,940 × 20]&gt; ## 3 FR-Pue &lt;tibble [5,479 × 20]&gt; The function nest() names the nested data column automatically \"data\". This structure is very useful. For example, for applying functions over sites’ data frames separately (and not over the entire data frame). By combining map() and mutate(), we can fit linear models on each site’s data frame individually in one go. ddf_allsites |&gt; group_by(siteid) |&gt; nest() |&gt; mutate(linmod = map(data, ~lm(GPP_NT_VUT_REF ~ SW_IN_F, data = .))) This approach is extremely powerful and lets you stick to working with tidy data frames and use the rows-dimension flexibly. Here, rows are sites and no longer time steps, while the nested data frames in column \"data\" have time steps along their rows. The power of nesting is also to facilitate complex aggregation steps over a specified dimension (or axis of variation, here given by siteid), where the aggregating function is not limited to taking a vector as input and returning a scalar, as is the case for applications of summarise() (see above). Combining the steps described above into a single workflow, we have: ddf_allsites_nested &lt;- ddf_allsites |&gt; group_by(siteid) |&gt; nest() |&gt; mutate(linmod = map(data, ~lm(GPP_NT_VUT_REF ~ SW_IN_F, data = .))) |&gt; mutate(summ = map(linmod, ~summary(.))) |&gt; mutate(rsq = map_dbl(summ, &quot;r.squared&quot;)) |&gt; arrange(desc(rsq)) # to arrange output, with highest r-squared on top ddf_allsites_nested ## # A tibble: 3 × 5 ## # Groups: siteid [3] ## siteid data linmod summ rsq ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;dbl&gt; ## 1 FI-Hyy &lt;tibble [6,940 × 20]&gt; &lt;lm&gt; &lt;smmry.lm&gt; 0.642 ## 2 CH-Dav &lt;tibble [6,574 × 20]&gt; &lt;lm&gt; &lt;smmry.lm&gt; 0.420 ## 3 FR-Pue &lt;tibble [5,479 × 20]&gt; &lt;lm&gt; &lt;smmry.lm&gt; 0.377 This code is a demonstration of the power of tidy and nested data frames and for the clarity of the tidyverse syntax. Nesting is useful also for avoiding value duplication when joining relational data objects. Above, we nested time series data objects (where time steps and sites are both organised along rows) by sites and got a data frame where only sites are organised along rows, while time steps are nested inside the column \"data\". This now fits the structure of a relational data object (siteinfo_fluxnet2015) containing site-specific meta information (also with only sites along rows). base::load(&quot;data/siteinfo_fluxnet2015.rda&quot;) # loads siteinfo_fluxnet2015 Joining the nested data frame with site meta information results in a substantially smaller and much handier data frame compared to an alternative, where the site meta information is joined into the un-nested (daily) data frame, and therefore duplicated for each day within sites. ddf_allsites_nested_joined &lt;- siteinfo_fluxnet2015 |&gt; rename(siteid = sitename) |&gt; right_join(select(ddf_allsites_nested, -linmod, -summ, -rsq), by = &quot;siteid&quot;) ddf_allsites_joined &lt;- siteinfo_fluxnet2015 |&gt; rename(siteid = sitename) |&gt; right_join(ddf_allsites, by = &quot;siteid&quot;) print(paste(&quot;Flat and joined:&quot;, format(object.size(ddf_allsites_joined), units = &quot;auto&quot;, standard = &quot;SI&quot;))) ## [1] &quot;Flat and joined: 4.8 MB&quot; print(paste(&quot;Nested and joined:&quot;, format(object.size(ddf_allsites_nested_joined), units = &quot;auto&quot;, standard = &quot;SI&quot;))) ## [1] &quot;Nested and joined: 3.1 MB&quot; # save for later use write_rds(ddf_allsites_nested_joined, file = &quot;data/ddf_allsites_nested_joined.rds&quot;) 3.4 Exercises find a way to encode &gt; in &gt;10 m in machine-readable way. Exercise: hhdf cleaning based on QC info, non-random missingness (more at night), removing data implies shift of the mean. aggregate the CO2 data, calculating the mean by Treatment. CO2 |&gt; group_by(Treatment) |&gt; summarise(uptake = mean(uptake)) ## # A tibble: 2 × 2 ## Treatment uptake ## &lt;fct&gt; &lt;dbl&gt; ## 1 nonchilled 30.6 ## 2 chilled 23.8 make the following tables tidy: df_sunspots df_sunspots &lt;- tibble(count = as.vector(sunspots), year_dec = time(sunspots) ) |&gt; mutate(month_dec = year_dec - as.integer(year_dec)) |&gt; mutate(year = year_dec - month_dec) |&gt; mutate(month = as.integer(month_dec * 12)) |&gt; mutate(date = ymd(paste0(as.character(year), as.character(month + 1), &quot;-15&quot;))) |&gt; mutate(month = month(date, label = TRUE)) |&gt; select(year, month, count) link Exercises: identify outliers in VPD and temperature in our example dataset. hhdf |&gt; select(TA_F, VPD_F) |&gt; boxplot() # the function used to calculate boxplot statistics # returning &quot;outliers&quot; in TA_F hhdf$TA_F[hhdf$TA_F %in% boxplot.stats(hhdf$TA_F)$out] |&gt; head() ## numeric(0) # returning &quot;outliers&quot; in VPD_F hhdf$VPD_F[hhdf$VPD_F %in% boxplot.stats(hhdf$VPD_F)$out] |&gt; head() ## [1] 10.791 11.889 11.897 11.904 11.912 11.919 combine df_sunspots and df_co2 along years and months. df_co2 &lt;- tibble(co2_concentration = as.vector(co2), year_dec = time(co2) ) |&gt; mutate(month_dec = year_dec - as.integer(year_dec)) |&gt; mutate(year = year_dec - month_dec) |&gt; mutate(month = as.integer(month_dec * 12)) |&gt; mutate(date = ymd(paste0(as.character(year), as.character(month + 1), &quot;-15&quot;))) |&gt; mutate(month = month(date, label = TRUE)) |&gt; select(year, month, co2_concentration) 3.5 Solutions "],["data_vis.html", "Chapter 4 Data visualisation 4.1 Learning objectives 4.2 Required packages 4.3 Tutorial 4.4 Exercises 4.5 Solutions", " Chapter 4 Data visualisation Chapter lead author: Benjamin Stocker 4.1 Learning objectives TBC 4.2 Required packages use_pkgs &lt;- c(&quot;dplyr&quot;, &quot;tidyr&quot;, &quot;readr&quot;, &quot;ggplot2&quot;, &quot;cowplot&quot;, &quot;lubridate&quot;, &quot;scico&quot;, &quot;hexbin&quot;) new_pkgs &lt;- use_pkgs[!(use_pkgs %in% installed.packages()[, &quot;Package&quot;])] if (length(new_pkgs) &gt; 0) install.packages(new_pkgs) invisible(lapply(use_pkgs, require, character.only = TRUE)) 4.3 Tutorial Visualizations often take the center stage of publications and are often the main vehicles for transporting information in scientific publications and (ever more often) in the media. Visualizations communicate data and its patterns in visual form. Visualizing data is also an integral part of the exploratory data analysis cycle. Visually understanding the data guides its transformation and the identification of suitable models and analysis methods. The quality of a data visualization can be measured by its effectiveness of conveying information about the data and thus of answering a question with the data and telling a story. Different aspects determine this effectiveness, including the appropriateness of visualization elements, the intuitiveness of how information can be decoded from the visualization by the reader, the visual clarity and legibility (taking into account the vision and potential vision deficiencies of the reader), the visual appeal, etc. This tutorial introduces data visualization under the premise that not all aspects of data visualization are a matter of taste. There are appropriate and less appropriate ways of encoding data in visual form. This tutorial is inspired by the comprehensive and online available textbook Fundamentals of Data Visualization by Claus O. Wilke. Another excellent resource is the [Chapter Data Visualisation in R for Data Science by Hadley Wickham](https://r4ds.had.co.nz/data-visualisation.html. 4.3.1 The grammar of graphics In Chapter @ref(data_wrangling), we learned about axes of variation in the data. For example, time is an axis of variation in our example data hhdf, or site identity and the date are axes of variation in our example data ddf. We have also learned that we can aggregate over axes of variation, and that we can often separate an axis of variation into a hierarchy of subordinate axes of variation (e.g., years, months, days, and a half-hourly time axis). In this chapter, we will be working mainly with the same half-hourly time series data of ecosystem-atmosphere fluxes and parallel measurements of meteorological variables - as in Chapters @ref(getting_started) and @ref(data_wrangling). For time series data, the entry point of the exploratory data analysis cycle may be a visualization of some variable of interest (here GPP_NT_VUT_REF) against time: hhdf &lt;- read_csv(&quot;data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN.csv&quot;) ## Rows: 52608 Columns: 20 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (18): TA_F, SW_IN_F, LW_IN_F, VPD_F, PA_F, P_F, WS_F, GPP_NT_VUT_REF, N... ## dttm (2): TIMESTAMP_START, TIMESTAMP_END ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. plot(hhdf$TIMESTAMP_START, hhdf$GPP_NT_VUT_REF, type = &quot;l&quot;) You may notice the spurious-looking values on the left, in the first third of year 2004. We’ll revisit this point later in this Chapter. From (Wilke): “All data visualizations map data values into quantifiable features of the resulting graphic. We refer to these features as aesthetics.” Applied to our example, the aesthetics are the x-axis and the y-axis of a cartesian coordinate system. TIMESTAMP_START is mapped onto the x-axis, GPP_NT_VUT_REF is mapped onto the y-axis, and their respective values specify the position of points in the cartesian coordinate system that are then connected with lines - making up the geometrical object that represents the data. Often, the aesthetic that is used to plot the target variable against corresponds to a known axis of variation in the data. The notion of mapping data onto aesthetics and using objects whose geometry is defined by the aesthetics gives rise to the grammar of graphics and to the ggplot2 R package for data visualisation (which we will use throughout the remainder of this course). The equivalent ggplot2 code that follows the philosophy of the grammar of graphics is: ggplot(data = hhdf, aes(x = TIMESTAMP_START, y = GPP_NT_VUT_REF)) + geom_line() The argument provided by the aes() statement specifies the aesthetics (x, and y) and which variables in data are mapped onto them. Once this is specified, we can use any suitable geometrical object that is defined by these aesthetics. Here, we used a line plot specified by + geom_line(). The data visualisation above is a dense plot and we cannot distinguish patterns because variations in GPP happen at time scales that are too narrow for displaying three years of half-hourly data in one plot. GPP varies throughout a day just as much as it varies throughout a season. To see this, we can focus on a narrower time span (selecting rows by index using slice() in the code below). Visual clarity is also facilitated by an appropriate labeling (title and axes labels using labs()) and by a reduction of displayed elements to a minimum (therefore, the changing of the formatting theme by theme_classic()): hhdf |&gt; slice(24000:25000) |&gt; ggplot(aes(x = TIMESTAMP_START, y = GPP_NT_VUT_REF)) + geom_line() + labs(title = &quot;Gross primary productivity&quot;, subtitle = &quot;Site: CH-Lae&quot;, x = &quot;Time&quot;, y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() Find a complete reference to ggplot2 here (R-ggplot2?). The grammar of graphics has found its way also into Python and you can use ggplot using the plotnine Python package (see here). 4.3.2 Every data has its representation In the above example, we mapped two continuous variables (TIMESTAMP_START and GPP_NT_VUT_REF) onto the aesthetics x, and y to visualize time series data. A line plot is an appropriate choice for such data as points are ordered along the time axis and can be connected by a line. Different “geometries” are suitable for visualizing different aspects of the data, and different variable types are suited to mapping onto different aesthetics. Common, available aesthetics are shown in Fig. XXX and can be allocated to variable types: Continuous variables: position, size, color (a color gradient), line width, etc. Categorical variables: shape, color (a discrete set of colors), line type, etc. Not only the different aesthetics, but also the each type of geometry (the layers of the visualization added to a plot by + geom_*()) goes with certain types of variables and aspects of the data (but not with others). The sub-sections below provide a brief categorization of data visualization types. A more comprehensive overview is given by GGPlot2 Essentials for Great Data Visualization in R by Alboukadel Kassambara. 4.3.2.1 One value per category Probably the simplest case of data vizualisation is where a single value is shown across a categorical variable. This calls for a bar plot (geom_bar()). In the example below, we plot the mean GPP for within each month. The “custom plot” shown below is a demonstration for what you can do by combining different elements with ggplot2. Try to understand the command for creating the object gg2. Both examples are based on the data frame ddf which we created in Chapter @ref(data_wrangling). ddf &lt;- read_csv(&quot;data/ddf.csv&quot;) ## Rows: 1096 Columns: 6 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (5): GPP_NT_VUT_REF, n_datapoints, n_measured, SW_IN_F, f_measured ## date (1): date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Bar plot gg1 &lt;- ddf |&gt; mutate(month = month(date, label = TRUE)) |&gt; group_by(month) |&gt; summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF)) |&gt; ggplot(aes(x = month, y = GPP_NT_VUT_REF)) + geom_bar(stat = &quot;identity&quot;) + theme_classic() + labs(title = &quot;Bar plot&quot;, x = &quot;Month&quot;, y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) # Custom plot gg2 &lt;- ddf |&gt; mutate(month = month(date, label = TRUE)) |&gt; group_by(month) |&gt; summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF)) |&gt; ggplot(aes(x = month, y = GPP_NT_VUT_REF)) + geom_segment(aes(x = month, xend = month, y = 0, yend = GPP_NT_VUT_REF), size = 3, color = &quot;grey40&quot;) + geom_point(aes(x = month, y = GPP_NT_VUT_REF), size = 8, color = &quot;grey40&quot;) + geom_text(aes(x = month, y = GPP_NT_VUT_REF, label = format(GPP_NT_VUT_REF, digits = 2)), size = 3, color = &quot;white&quot;) + theme_classic() + labs(title = &quot;Custom plot&quot;, x = &quot;Month&quot;, y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + scale_y_continuous(limits = c(0, 8.75), expand = c(0, 0)) + coord_flip() ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. cowplot::plot_grid(gg1, gg2) Above, we created two objects, gg1 and gg2, that contain the instructions for creating the plots. To combine multiple sub-plots within panels of a single plot, we used cowplot::plot_grid() from the cowplot library. Note also the stat = \"identity\" specification within the geom_bar() function call. This is required when the bar height is specified by a single value within each category (month in the example above). To visualize not a value per se but the count of values within categories, use stat = \"count\" to get the equivalent result as when aggregating by taking the number of observations within categories explicitly using the dplyr function summarise(). This equivalency is demonstrated below. # separate aggregation gg1 &lt;- hhdf |&gt; filter(NEE_VUT_REF_QC == 0) |&gt; group_by(NIGHT) |&gt; summarise(count = n()) |&gt; ggplot(aes(x = NIGHT, y = count)) + geom_bar(stat = &quot;identity&quot;) + theme_classic() # implicit aggregation by &#39;stat&#39; gg2 &lt;- hhdf |&gt; filter(NEE_VUT_REF_QC == 0) |&gt; ggplot(aes(x = NIGHT)) + geom_bar(stat = &quot;count&quot;) + theme_classic() cowplot::plot_grid(gg1, gg2) 4.3.2.2 Distribution of one variable Examining the distribution of a variable is often the first step of exploratory data analysis. A histogram displays the distribution of numerical data by mapping the frequency (or count) of values within discrete bins (equally spaced ranges along the full range values of a given variable) onto the “height” of a bar, and the range of values within bins onto the position of the bar. In other words, it shows the count of how many points of a certain variable (below GPP_NT_VUT_REF) fall into a discrete set of bins. When normalizing (scaling) the “bars” of the histogram to unity, we get a density histogram. To specify the y-axis position of the upper end of the histogram bar as the density, use y = ..density.. in the aes() call. To show counts, use y = ..count... hhdf |&gt; ggplot(aes(x = GPP_NT_VUT_REF, y = ..density..)) + geom_histogram(fill = &quot;grey70&quot;, color = &quot;black&quot;) + geom_density(color = &quot;red&quot;) + # we can overlay multiple plot layers! labs(title = &quot;Histogram and density&quot;, x = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() ## Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0. ## ℹ Please use `after_stat(density)` instead. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Note that the red line plotted by geom_density() on top of the density histogram visualises the density distribution in continuous (not discrete, or binned) form. Note also that both “geoms” share the same aesthetics with aes() specified in the ggplot() function call. 4.3.2.3 Distributions within categories To visualize distributions of a single continuous variable within categories, perhaps the most common visualization type is the box plot. As described in Chapter @ref(data_wrangling), it shows the median (bold line in the center), the upper and lower quartiles, corresponding to the 25% and the 75% quantiles, often referred to as \\(Q_1\\) and \\(Q_3\\) , and given by the upper and lower edge of the box plot. The lines extending from the box edges visualize the range of \\(( Q_1 - 1.5 (Q_3 - Q_1)\\) to \\(Q_3 + 1.5 (Q_3 - Q_1)\\). Any point outside this range is plotted by a point. The box plot is rather reductionist in showing the data (the vector of all values is reduced to the median, \\(Q_1\\) , \\(Q_3\\), and outlying points) and may yield a distorted picture of the data distribution and does not reflect information about the data volume. For this reason, several journals are now requiring individual data points or at least the number of data points to be shown in addition to each box. Below, points are added by geom_jitter() , where points are “jittered”, that is, randomly spread out along the x-axis. Violin plots are a hybrid of a density plot and a box plot. The form of their edge is given by the density distribution of the points they represent. # Box plot set.seed(1985) # for random number reproducibility in sample_n() and jitter gg1 &lt;- hhdf |&gt; sample_n(300) |&gt; mutate(Night = ifelse(NIGHT == 1, TRUE, FALSE)) |&gt; ggplot(aes(x = Night, y = VPD_F)) + geom_boxplot(fill = &quot;grey70&quot;) + labs(title = &quot;Box plot&quot;) + labs(y = &quot;VPD (hPa)&quot;) + theme_classic() # Box plot + jittered points set.seed(1985) gg2 &lt;- hhdf |&gt; sample_n(300) |&gt; mutate(Night = ifelse(NIGHT == 1, TRUE, FALSE)) |&gt; ggplot(aes(x = Night, y = VPD_F)) + geom_boxplot(fill = &quot;grey70&quot;, outlier.shape = NA) + geom_jitter(width = 0.2, alpha = 0.3) + labs(title = &quot;Boxplot + jittered points&quot;) + labs(y = &quot;VPD (hPa)&quot;) + theme_classic() # Violin plot set.seed(1985) gg3 &lt;- hhdf |&gt; sample_n(300) |&gt; mutate(Night = ifelse(NIGHT == 1, TRUE, FALSE)) |&gt; ggplot(aes(x = Night, y = VPD_F)) + geom_violin(fill = &quot;grey70&quot;) + labs(title = &quot;Violin plot&quot;) + labs(y = &quot;VPD (hPa)&quot;) + theme_classic() cowplot::plot_grid(gg1, gg2, gg3, ncol = 3) 4.3.2.4 Regression of two continuous variables Scatter plots visualize how two variables co-vary. The position of each point in a scatter plot is given by the simultaneously recorded value of two variables, provided in two columns along the same row in a data frame, and mapped onto two dimensions in a cartesian coordinate system. We can also say that two variables are regressed against each other. In the figure below, we start with a simple scatter plot (a), regressing GPP against shortwave radiation. A visualization is supposed to tell a story with data. The positive and largely linear relationship between shortwave radiation and GPP is expected from theory (Monteith, 1972) and our process understanding of the dominant controls on photosynthesis - it’s mainly solar (shortwave) radiation. The linear regression line, added by geom_smooth(method = \"lm\") in (a), indicates that relationship. # a gg1 &lt;- hhdf |&gt; sample_n(1000) |&gt; # to reduce the dataset ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(size = 0.75) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() # b gg2 &lt;- hhdf |&gt; sample_n(1000) |&gt; ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = NIGHT)) + geom_point(size = 0.75) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() # c gg3 &lt;- hhdf |&gt; sample_n(1000) |&gt; ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = as.factor(NIGHT))) + geom_point(size = 0.75) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() # d gg4 &lt;- hhdf |&gt; sample_n(1000) |&gt; ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = TA_F)) + geom_point(size = 0.75) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() + scale_color_viridis_c() cowplot::plot_grid(gg1, gg2, gg3, gg4, ncol = 2, labels = &quot;auto&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Are there additional variables that modify the relationship between solar radiation and GPP? To visually investigate this, we can map additional variables in our data set onto additional aesthetics. For example, at night, photosynthesis ceases (b). Here, the variable NIGHT was mapped onto the aesthetic color of same geometry (geom_point()). By default, ggplot() used a continuous color scale, as indicated by the color key on the right. It did so although NIGHT is a categorical (a binary) variable because in the data frame, NIGHT is stored as a numeric value (as can be checked by class(hhdf$NIGHT)). To avoid this, and automatically trigger the use of a color scheme that is suitable for categorical variables, we specify aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = as.factor(NIGHT)) in (c). 4.3.2.5 Use of colors The above example demonstrates that color schemes have to be chosen depending on the nature of the data. Mapping a continuous variable onto the aesthetics color requires a continuous color scheme to be applied, categorical data requires discrete color schemes. More distinctions should be considered: Continuous variables should be distinguished further if they span a range that includes zero or not. If so, diverging color schemes should be used, where zero appears neutral (e.g., white). If zero is not contained within the range of values in the data, diverging color schemes should be avoided. Continuous or ordinal variables may be cyclic in nature. For example, hours in a day are cyclic, although there are twelve discrete numbers. The time 00:00 is nearer to 23:59 than it is from, for example, 01:00. The cyclical, or periodical nature of the data should be reflected in the choice of a color scheme where the edges of the range are more similar to one another than they are to the center of the range (see example below XXX) Multisequential color schemes reflect that there is a natural distinction between two parts of the range of continuous values (see example below XXX). Choices of colors and their combination is far from trivial. Colors in color schemes (or “scales”) should be: Distinguishable for people with color vision deficiency Distinguishable when printed in black and white Evenly spaced in the color space Intuitively encoding the information in the data (for example, blue-red for cold-hot) Visually appealing In (d), we mapped temperature, a continuous variable, onto the color aesthetic of the points and chose the continuous viridis color scale by specifying + scale_color_viridis_c(). The viridis scales have become popular for their respect of the points listed above. For further reading, several excellent resources exist that theoretize and guide the use of color in data visualization. Excellent sources are: Fabio Crameri’s Scientific colour maps, Crameri (2018) and its R package scico (on CRAN). Paul Tol’s Notes, available for example in the khroma R package (on CRAN). 4.3.2.6 Regression within categories In the sub-plot (d) above, we may observe a pattern: GPP recorded at low temperatures (dark colored points) tend to be located in the lower range of the cloud of points. We may formulate a hypothesis from this observation, guiding further data analysis and modelling. This illustrates how data visualization is an integral part of any (geo-) data science workflow. Since the relationship between incoming solar radiation and ecosystem photosynthesis is strongly affected by how much of this light is actually absorbed by leaves, and because the amount of green foliage varies strongly throughout a year (the site CH-Lae from which the data is recorded is located in a mixed forest), the slope of the regression between solar radiation and GPP should change between months. Hence, let’s consider months as the categories to be used for separating the data and analyzing the bi-variate relationships separately within. Below, two alternatives are presented. Either the data is separated into a grid of sub-plots, or the data is separated by colors within the same plot panel. Separation by color ddf |&gt; mutate(month = month(date, label = TRUE)) |&gt; ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = month)) + geom_point(alpha = 0.5) + geom_smooth(formula = y ~ x + 0, method = &quot;lm&quot;, se = FALSE) + labs(x = expression(paste(&quot;PPFD (&quot;, mu, &quot;mol m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)) ) + theme_classic() + scico::scale_color_scico_d(palette = &quot;romaO&quot;) Note three aspecte here. First, the color-mapping is specified within aes() in the ggplot() function call and then adopted for all subsequent additions of geoms. Hence, also the geom_smooth() thus takes the color information, and not by a “hard-coded” specification of color = inside the geom_smooth() call as done in Fig. XXX. Second, we specified a formula for the linear regression “smooting curve” to force the lines through the origin (y ~ x + 0). This is motivated by our a priori understanding of the process generating the data: when solar radiation is zero, photosynthesis (and hence GPP) should be zero. Third, we chose a color palette that reflects the cyclic (or periodic) nature of the categories (months). January is closer to December than it is to April. Therefore, their respective colors should also be closer in color space. An appropriate palette for this is \"romaO\" from the scico package. Separation into sub-plots Yet another “mapping” is available with facet_wrap(). It separates the visualisation into different sub-plots, each showing only the part of the data that falls into the respective category, separated by facet_wrap(). Note, this mapping is not dealt with the same way as other aesthetics - not with specifying it with aes()), but with adding the facet_wrap() with a + to the ggplot() object. The variable by which facet_wrap() separates the plot has to be specified as an argument with a preceeding ~. Here, this is ~month. ddf |&gt; mutate(month = month(date, label = TRUE)) |&gt; ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(alpha = 0.4) + geom_smooth(formula = y ~ x + 0, method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + labs(x = expression(paste(&quot;PPFD (&quot;, mu, &quot;mol m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)) ) + facet_wrap(~month) You may object here that a linear regression is not a good model for our data. Instead, the relationship looks saturating, as indicated for example by the data in August. But we’ll get to modelling in later chapters. Nevertheless, the two visualizations above confirm our suspicion that the light-GPP relationship varies between months - a demonstration for why data visualization is an integral part of the scientific process. 4.3.2.7 Time series A time series plot can be regarded as a special case of a regression of two variables. In this case, one variable is regressed against time. A defining aspect of time is that there is a natural order in time steps. Therefore, it makes sense to visualize temporal data using lines that connect the points using geom_line(). The example below shows the time series of daily GPP in three years. ddf |&gt; ggplot(aes(x = date, y = GPP_NT_VUT_REF)) + geom_line() + labs(title = &quot;Line plot&quot;, x = &quot;Time&quot;, y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() In the line plot above, we see a spurious-looking part of the time series in the first third of year 2004. Is this bad data that should be removed? Also, in winter of 2005/2005, some daily GPP values appear as high as a typical summer level of GPP. Is this bad data? Remember, that in Chapter @ref(data_wrangling), we aggregated the half-hourly hhdf data frame to a daily data frame ddf from which data is visualized above. The aggregation kept a record of the fraction fraction of actually measured (not gap-filled) half-hourly data points per day (f_measured). This yields a “data quality axis”. Is there a pattern between f_measured and the presumably bad data? Discerning such patterns is often only possible with a suitable visualization. What is suitable here? A solution is to “map” f_measured to the color axis. When adding such an additional mapping to visualisation dimensions (“aesthetics”), we have to specify it using aes(). This only affects the points and the color of points, while the lines and points and their position in x-y space is shared. Hence, we write aes(x = date, y = GPP_NT_VUT_REF) in the ggplot() function call (indicating that all subsequent additions of geom_ layers share this x-y mapping); while aes(color = f_measured) is specified only in the geom_point() layer. ddf |&gt; ggplot(aes(x = date, y = GPP_NT_VUT_REF)) + geom_line() + geom_point(aes(color = f_measured), size = 0.9) + labs(x = &quot;Time&quot;, y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + scale_color_viridis_c(direction = -1) + # inverse color scale is more intuitive here theme_classic() We observe the presumably bad data appear in yellow, and are therefore indeed characterised with a particularly low fraction of actually measured data from which their values are derived. This is an insight we would never have reached by just looking at the naked values in our data frames. Data visualizations are essential for guiding analyses and data processing throughout all steps. Having learned this, we now have a justification for applying further data filtering criteria. 4.3.2.8 Periodic data The seasons are an important axis of variation in our data. Hence our data are periodic - with a periodicity of 365 days in the ddf dataset and with both 12 hours and 365 days in the hhdf dataset. A polar coordinate system, instead of a cartesian system, lends itself to displaying periodic data. A polar coordinate system reflects the fact that, for example, January 1st is closer to December 31st, although they are located on the extreme end of a linear spectrum of days in a year. In a polar coordinate system, the x-axis spans the angle (360\\(^\\circ\\), like the clock hands), while the y-axis spans the radius (distance from the center). This is specified by changing the coordinate system of the ggplot object by + coord_polar(). Below, we first aggregate the data to get a mean seasonal cycle from ddf (a, b), and to get a mean diurnal (daily) cycle from June data in hhdf (c, d). To get the mean seasonal cycle, we first determine the day-of-year (counting from 1 for January first to 365 for December 31st) using the lubridate function yday(). # seasonal cycle, cartesian gg1 &lt;- ddf |&gt; mutate(doy = yday(date)) |&gt; group_by(doy) |&gt; summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF)) |&gt; ggplot(aes(doy, GPP_NT_VUT_REF)) + geom_line() # seasonal cycle, polar gg2 &lt;- ddf |&gt; mutate(doy = yday(date)) |&gt; group_by(doy) |&gt; summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF)) |&gt; ggplot(aes(doy, GPP_NT_VUT_REF)) + geom_line() + coord_polar() # diurnal cycle, cartesian gg3 &lt;- hhdf |&gt; mutate(month = month(TIMESTAMP_START)) |&gt; filter(month == 6) |&gt; # taking only June data mutate(hour = hour(TIMESTAMP_START)) |&gt; group_by(hour) |&gt; summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF)) |&gt; ggplot(aes(hour, GPP_NT_VUT_REF)) + geom_line() # diurnal cycle, polar gg4 &lt;- hhdf |&gt; mutate(month = month(TIMESTAMP_START)) |&gt; filter(month == 6) |&gt; # taking only June data mutate(hour = hour(TIMESTAMP_START)) |&gt; group_by(hour) |&gt; summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF)) |&gt; ggplot(aes(hour, GPP_NT_VUT_REF)) + geom_line() + coord_polar() cowplot::plot_grid(gg1, gg2, gg3, gg4, ncol = 2, labels = &quot;auto&quot;) 4.3.2.9 Density along two continuous variables Scatter plots can appear “overcrowded” when points are plotted on top of each other and potentially important information is lost in the visualization. hhdf |&gt; ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point() + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() To avoid obscuring important details about the data, we may want to visualise the density of points. We want to plot how many points fall within bins of a certain range values in GPP and shortwave radiation, or, in other words, within grid cells in the GPP-radiation space. We can visualize the data, for example, with a raster plot that measures the density using stat_density_2d() or with a binning into hexagonal cells using the simple geom_hex() layer. # density raster ddf |&gt; ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + stat_density_2d( geom = &quot;raster&quot;, # the geometric object to display the data aes(fill = after_stat(density)), # using `density`, a variable calculated by the stat contour = FALSE ) + scale_fill_viridis_c() + labs(x = expression(paste(&quot;PPFD (&quot;, mu, &quot;mol m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)) ) + theme_classic() + scale_x_continuous(expand = c(0, 0)) + # avoid gap between plotting ara and axis scale_y_continuous(expand = c(0, 0)) # density hexagonal bins ddf |&gt; ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_hex() + scale_fill_viridis_c() + labs(x = expression(paste(&quot;PPFD (&quot;, mu, &quot;mol m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)) ) + theme_classic() + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) An alternative solution to “overplotting” points is described in this blog post. 4.3.2.10 Raster data In the figure above, the density of points in a grid of equally spaced bins along two axes, one for each variable, was shown. Often, data is organised along a grid of equally spaced bins by nature - think a matrix or raster data. Examples of such data are climate model outputs (which often span more than two dimensions), remote sensing images (again, just one “layer” of an image or one “band”), or images in general. In these cases, two (often spatial) axes span the space of a cartesian coordinate system and the value within each pixel is mapped onto the color aesthetic. The base-R function image() can be used to visualize such spatial data as images. image(volcano) ggplot2 forces us to the data frame paradigm and therefore doesn’t lend itself naturally to raster data. We can, however, convert raster data into a data frame in a separate step. # example from https://github.com/thomasp85/scico df_volcano &lt;- tibble( x = rep(seq(ncol(volcano)), each = nrow(volcano)), y = rep(seq(nrow(volcano)), ncol(volcano)), height = c(volcano) - 140 # subtract 140 for example below ) ggplot(df_volcano, aes(x = x, y = y, fill = height)) + geom_raster() + scico::scale_fill_scico(palette = &#39;bukavu&#39;, midpoint = 0) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) In this example, we used the multisequential \"bukavu\" color scale from the scico package to reflect values above 0 (e.g., elevation above the sea level) and below 0 (ocean depth). 4.3.2.11 Geospatial data Raster data often reflects values in geographic space. Also points and polygons may be located in geographic space. This opens the door to geospatial data visualisation (a.k.a. creating maps) and will be dealt with in more detail in Chapter @ref(data_variety). 4.4 Exercises Using the data frame ddf_allsites_nested_joined from Chapter @ref(data_wrangling), visualise the mean seasonal cycle of GPP for each site. Remove data in ddf based on f_measured. Visualize data removal, fill missing data with the respective day’s value of the mean seasonal cycle, visualize good data, bad removed data, and filled data in one plot using different colors. ***Tell a story about the airquality data. 4.5 Solutions "],["data_variety.html", "Chapter 5 Data variety 5.1 Learning objectives 5.2 Tutorial 5.3 Files and file formats 5.4 Meta-data 5.5 Spatial data representation 5.6 Data sources 5.7 Exercises 5.8 Solutions", " Chapter 5 Data variety Chapter lead author: Koen Hufkens 5.1 Learning objectives As a scientist you will encounter variety of data (formats). In this section you will learn some of the most common formats around, their structure, and the advantages and disadvantages of using a particular data format. Only singular files are considered in this section, and databases are not covered although some files (formats) might have a mixed use. However, more and more data moves toward a cloud server based model where data is queried from an online database using an Application Programming Interface (API). Although the explicit use of databases is not covered you will learn basic API usage, to query data which is not represented as a file. In this chapter you will learn: how to recognize data/file formats understand data/file format limitations manipulation wise content wise how to read and or write data in a particular file format how to query an API and store it locally 5.2 Tutorial 5.3 Files and file formats 5.3.1 File extensions In order to manipulate data and make some distinctions on what a data file might contain they carry a particular file format extension. This file extensions denotes the intended content and use of a particular file. For example a file ending in .txt suggests that it contains text. A file extension allows you, or a computer, to assess the content of a file without opening the file. File extensions are therefore an important tool in assessing what data you are dealing with, and what tools you will need to manipulated (read / write) the data. NOTE: It is important to note that file extensions can be changed. In some cases the file extension does not represent the content and or use case of the data contained within the file. TIP: If a file doesn’t read it is always wise to check the first couple of lines to verify if the data has a structure which corresponds to the file extension. # On a linux/macos system you can use the below command # to show the first couple of lines of a file head your_file # alternatively you can show the last few lines # of a file using tail your_file 5.3.1.1 Human readable data One of the most important distinctions in data formats falls along the line of it being human readable or not. Human readable data is, as the term specifies, made up of normal text characters. Human readable text has the advantage that it is easily read, and or edited using conventional text processors. This convenience comes at the cost of the files not being compressed in any way, and file sizes can become unsustainable. However, for many applications where file sizes are limited (&lt;50MB), human readable formats are the preferred option. Most human readable data falls in two broad categories, tabulated data and structured data. 5.3.1.1.1 Tabulated data Often, human readable formats provide data in tabulated form using a consistent delimiter. This delimiter is a character separating columns of a table. column_one, column_two, column_three 1, 2, 3 1, 2, 3 1, 2, 3 Common delimiters in this context are the comma (,), as shown in the above example. A file with this particular format often carries the comma-separated values file extension (*.csv). Other delimiters are the tabulation (tab) character. Files with tab delimited values have the* format. TIP: File extensions aren’t always a true indication of the delimiter used. For example, .txt files often contain comma or tab separated data. If reading a file using a particular delimiter fails it is best to check the first few lines of a file. 5.3.1.1.2 Structured data Tabulated delimited data is row and column oriented and therefore doesn’t allow complex structured content, e.g. tables within tables. This issue is sidestepped by for example the JSON format. The JSON format in particular uses attribute-value pairs to store data, and is therefore more flexible in terms of accommodating varying data structures. Below you see an example of details describing a person, with entries being fo varying length and data content. { &quot;firstName&quot;: &quot;John&quot;, &quot;lastName&quot;: &quot;Smith&quot;, &quot;isAlive&quot;: true, &quot;age&quot;: 27, &quot;address&quot;: { &quot;streetAddress&quot;: &quot;21 2nd Street&quot;, &quot;city&quot;: &quot;New York&quot;, &quot;state&quot;: &quot;NY&quot;, &quot;postalCode&quot;: &quot;10021-3100&quot; }, &quot;phoneNumbers&quot;: [ { &quot;type&quot;: &quot;home&quot;, &quot;number&quot;: &quot;212 555-1234&quot; }, { &quot;type&quot;: &quot;office&quot;, &quot;number&quot;: &quot;646 555-4567&quot; } ], &quot;children&quot;: [ &quot;Catherine&quot;, &quot;Thomas&quot;, &quot;Trevor&quot; ], &quot;spouse&quot;: null } NOTE: despite being human readable, a JSON file is considerably harder to read than a comma separated file. Editing such a file is therefore more prone to errors if not automated. Other human readable structured data formats include the eXtensible Markup Language (XML), which is commonly used in web infrastructure. XML is used for storing, transmitting, and reconstructing arbitrary data but uses (text) markup instead of attribute-value pairs. &lt;note&gt; &lt;to&gt;Tove&lt;/to&gt; &lt;from&gt;Jani&lt;/from&gt; &lt;heading&gt;Reminder&lt;/heading&gt; &lt;body&gt;Don&#39;t forget me this weekend!&lt;/body&gt; &lt;/note&gt; 5.3.1.2 Writing and reading human readable files in R There are number of ways to read human readable formats into an R work environment. Here the basic approaches are listed, in particular reading CSV and JSON data. Large volumes of data are still available as CSV files or similar. Understanding how to read in such data into a programming environment is key. In this context the read.table() function is a general purpose tool to read in text data. Depending on the format, additional meta-data or comments, certain parameters need to be specified. Its counterpart is a function to write human readable data to file called, you guessed it, write.table(). Again parameters are required for maximum control over how things are written to file, by default though data are separated by a single empty space ” “, not a comma. Below you find and example in which a file is written to a temporary location, and read in again using the above mentioned functions. # create a data frame with demo data df &lt;- data.frame( col_1 = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), col_2 = c(&quot;d&quot;, &quot;e&quot;, &quot;f&quot;), col_3 = c(1,2,3) ) # write table as CSV to disk write.table( x = df, file = file.path(tempdir(), &quot;your_file.csv&quot;), sep = &quot;,&quot;, row.names = FALSE ) # Read a CSV file df &lt;- read.table( file.path(tempdir(), &quot;your_file.csv&quot;), header = TRUE, sep = &quot;,&quot; ) # help files of both functions can be accessed by # typing ?write.table or ?read.table in the R console In this example a data frame is generated with three columns. This file is then written to a temporary file in the temporary file directory tempdir(). Here, tempdir() returns the location of the temporary R directory, which you can use to store intermediate files. We use the file.path() function to combine the path (tempdir()) with the filename (your_file.csv). Using file.path() is good practice as directory structures are denoted differently between operating systems e.g., using a backslash (\\) no Windows vs. a slash (/) on Unix based systems (Linux/macOS). The file.path() function ensures that the correct directory separator is used. Note that in this command we have to manually set the separator (sep = \",\") and if a header is there (header = TRUE). Depending on the content of a file you will have to alter these parameters. Additional parameters of the read.table() function allow you to specify comment characters, skip empty lines, etc. Similar to this simple CSV file we can generate and read JSON files. For this we do need an additional library, as default R install does not provide this capability. However, the rest of the example follows the above workflow. # we&#39;ll re-use the data frame as generated for the CSV # example, so walk through the above example if you # skipped ahead # install the required package install.packages(&quot;jsonlite&quot;) # load the library library(&quot;jsonlite&quot;) # write the file to a temporary location jsonlite::write_json( x = df, path = file.path(tempdir(), &quot;your_json_file.json&quot;) ) # read the freshly generated json file df_json &lt;- jsonlite::read_json( file.path(tempdir(), &quot;your_json_file.json&quot;), simplifyVector = TRUE ) # check if the two data sets # are identical (they should be) identical(df, df_json) Note that the reading and writing JSON data is easier, as the structure of the data (e.g., field separators) are more strictly defined. While reading the data we use the simplifyVector argument to return a data frame rather than a nested list. This works as our data has a tabulated structure, this might not always be the case. Finally we compare the original data with the data read in using identical(). TIP: In calling the external library we use the :: notation. Although by loading the library with library() makes all jsonlite functions available, the explicit referencing of the origin of the function makes debugging often easier. 5.3.1.3 Binary data All digital data which is not represented as text characters can be considered binary data. Binary data can vary in its content from an executable, which runs a program, to the digital representation of an image (jpeg images). However, in all cases the data is represented as bytes (made of eight bits) and not text characters. One of the advantages of binary data is that it is an efficient representation of data, saving space. This comes at the cost of requiring a dedicated software, other than a text editor, to manipulate the data. For example, digital images in a binary format require image manipulation software. More so than human readable data, the file format (extension) determines how to treat the data. Knowing common data formats and their use cases is therefore key. 5.3.2 File formats Environmental sciences have particular file formats which dominate the field. Some of these file formats relate to the content of the data, some of these formats are legacy formats due to the history of the field itself. Here we will list some of the most common formats you will encounter. File format (extension) Format description Use case R Library *.csv comma separated tabulated data General purpose flat files with row and column oriented data base R *.txt tabulated data with various delimiters General purpose flat files with row and column oriented data base R *.json structured human readable data General purpose data format. Often used in web application. Has geospatial extensions (geojson). jsonlite *.nc NetCDF data array data Array-oriented data (matrices with &gt; 2 dimensions). Commonly used to store climate data or model outputs. Alternative to HDF data. ncdf4, terra, raster *.hdf HDF array data Array-oriented data (matrices with &gt; 2 dimensions). Commonly used to store climate data or model outputs. hdf *.tiff, *.geotiff Geotiff multi-dimensional raster data (see below) Layered (3D) raster (image) data. Commonly used to represent spatial (raster) data. terra, raster *.shp Shapefile of vector data (see below) Common vector based geospatial data. Used to describe data which can be captured by location/shape and attribute values. sp, sf 5.4 Meta-data Meta-data, or data which is associated with the main data file is key to understanding the content and context of a data file (or the data set to which the file belongs). In some cases you will find this data only as a general description referencing the file(s) itself. In other cases, meta-data is included in the file itself. For example, many tabular CSV data files contain a header specifying the content of each column, and at times a couple of lines of data specifying the content of the file itself - or context within which the data should be considered. # This is meta-data associated with the tabulated CSV file # for which the data is listed below. # # In addition to some meta-data, the first row of the data # contains the column header data column_one, column_two, column_three 1, 2, 3 1, 2, 3 1, 2, 3 In the case of binary files it will not be possible to read the meta-data as plain text. In this case, specific commands can be used to read the meta-data included in a file. The example below shows how you would list the meta-data of a GeoTiff file. # list geospatial data for a geotiff file gdalinfo your_geotiff.tiff TIP: Always keep track of your meta-data by including it, if possible, in the file itself. If this is not possible, meta data is often provided in a file called README. Meta-data is key in making science reproducible and guaranteeing consistency between projects. Key meta-data to retain are: the source of your data (URL, manuscript, DOI) the date when the data was downloaded manipulations on the data before using the data in a final workflow 5.5 Spatial data representation Environmental data often has an explicit spatial and temporal component. For example, climate data is often represented as 2D maps which vary over time. This spatial data requires an additional level of understanding of commonly used data formats and structures. In general, we can distinguish two important data models when dealing with spatial data, the raster and vector data model. Both data have their typical file formats (see above) and particular use cases. The definition of these formats, optimization of storage and math/logic on such data are the topic of Geographic Information System (GIS) science and beyond the scope of this course. We refer to other elective GIS courses for a greater understanding of these details. However, a basic understanding of both raster and vector data is provided here. 5.5.1 Raster data model The basic raster model represents geographic (2D) continuous data as a two-dimensional array, where each position has a geographic (x, y) coordinate, a cell size (or resolution) and a given extent. Using this definition any image adheres to the raster model. However, in most geographic applications, coordinates are referenced and correspond to a geographic position, e.g., a particular latitude longitude. Often, the model is expanded with a time dimension, stacking various two-dimensional arrays into a three-dimensional array. The raster data model is common for all data sources which use either imaging sensors, such as satellites or unmanned aerial vehicles (UAVs), or model based output which operates on a fixed grid, such as climate and weather models. Additional meta-data stores both the geographic reference system, the time components as well as other data which might be helpful to end users. Within the environmental sciences, NetCDF and GeoTiff are common raster data file formats. 5.5.2 Vector data model The vector data model, in contrast to the raster data model, describes (unbound) features using a geometry (location, shape) using coordinates and linked feature attributes. Geometries can be points, lines, polygons, or even volumes. Vector data does not have a defined resolution, making them scale independent. This makes the vector data model ideal for discrete features such as roads or building outlines. Conversely, vector data is poorly suited for continuous data. Conversions between the vector and raster model are possible, but limitations apply. For example, when converting vector data to raster data a resolution needs to be specified, as you lose scale independence of the vector format. Conversions from raster to vector are similarly limited by the original resolution of the raster data. 5.6 Data sources The sections above assume that you have inherited some data from someone, or have data files on disk (in a particular format). Yet, most of the time, gathering data is the first step in any analysis. Depending on where data is hosted you can simply download data through your web browser or use the internal download.file() function to grab data. But, many of the data described in previous sections are today warehoused in large cloud facilities. These data (and their underlying data formats) are stored in large databases and displayed through various applications. For example, Google Maps displays remote sensing (satellite) raster image data in addition to street level vector based labels. These services allow you to access the underlying (original) data using an API, hence programmatically using code. Mastering the use of these services has become key in gathering research data. 5.6.1 Direct downloads Before diving into a description of APIs we remind you that some file reading functions in R are web aware, and can not only read local files but also remote ones (i.e., URLs). Getting ahead of ourselves a bit (see tutorials below), the example code shows you how to read the content of a URL directly into your R environment. Although using this functionality isn’t equivalent to using an API, the concept is the same. I.e., you load a remote data source. # define a URL with data of interest # in this case annual mean CO2 levels at Mauna Loa url &lt;- &quot;https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_annmean_mlo.csv&quot; # read in the data directly from URL df &lt;- read.table( url, header = TRUE, sep = &quot;,&quot; ) 5.6.2 APIs Web based Application Programming Interfaces (APIs) offer a way to specify the scope of the returned data, and ultimately, the processing which goes on behind the scene in response to a (data) query. APIs are a way to, in a limited way, control a remote server to execute a certain (data) action. In most (RESTful) APIs, such query takes the form of an HTTP URL via an URL-encoded scheme using an API endpoint (or base URL). To reduce some of the complexity of APIs, it is common that a wrapper is written around an API in the language of choice (e.g., R, python). These dedicated API libraries make it easier to access data and limit coding overhead, as more concisely written. 5.6.2.1 Dedicated API libraries As an example of a dedicated library we use the {MODISTools} R package which queries remote sensing data generated by the MODIS mission from the Oak Ridge National Laboratories data archive. # install the MODISTools package install.packages(&quot;MODISTools&quot;) # load the library library(&quot;MODISTools&quot;) # list all available products products &lt;- MODISTools::mt_products() # print the first few lines # of available products print(head(products)) # download a demo dataset # download data subset &lt;- MODISTools::mt_subset( product = &quot;MOD11A2&quot;, lat = 40, lon = -110, band = &quot;LST_Day_1km&quot;, start = &quot;2004-01-01&quot;, end = &quot;2004-02-01&quot;, km_lr = 1, km_ab = 1, internal = TRUE ) # print the dowloaded data print(head(subset)) A detailed description of all functions of the MODISTools R package is beyond the scope of this course. However, the listed command show you what a dedicated API package does. It is a shortcut to functional elements of an API. For example mt_products() allows you to quickly list all products without any knowledge of an API URL. Although more complex, as requiring parameters, the mt_subset() routine allows you to query remote sensing data for a single location (specified with a latitude lat and longitude lon), and a given date range (e.g. start, end parameters), a physical extent (in km left-right and above-below). 5.6.2.2 GET Depending on your data source, you will either need to rely on a dedicated R package to query the API or study the API documentation. A dedicated package to query an API is for example the {ecmwfr} package to download ECMWF climate data from the Copernicus data services. The general scheme for using an API follows the use of the GET() command of the {httr} R library. You define a query using API parameters, as a named list, and then use a GET() statement to download the data from the endpoint (url). # formulate a named list query to pass to httr query &lt;- list( &quot;argument&quot; = &quot;2&quot;, &quot;another_argument&quot; = &quot;3&quot; ) # create url string (varies per product / param) url &lt;- &quot;https://your.service.endpoint.com&quot; # download data using the # API endpoint and query data status &lt;- httr::GET( url = url, query = query, httr::write_disk( path = &quot;/where/to/store/data/filename.ext&quot;, overwrite = TRUE ) ) Below, we provide an example of using the GET command to download data from the Regridded Harmonized World Soil Database (v1.2) as hosted on the Oak Ridge National Laboratory computer infrastructure. In this case we download a subset of a global map of topsoil sand content (T_SAND). # set API URL endpoint # for the total sand content url &lt;- &quot;https://thredds.daac.ornl.gov/thredds/ncss/ornldaac/1247/T_SAND.nc4&quot; # formulate query to pass to httr query &lt;- list( &quot;var&quot; = &quot;T_SAND&quot;, &quot;south&quot; = 32, &quot;west&quot; = -81, &quot;east&quot; = -80, &quot;north&quot; = 34, &quot;disableProjSubset&quot; = &quot;on&quot;, &quot;horizStride&quot;= 1, &quot;accept&quot;=&quot;netcdf4&quot; ) # download data using the # API endpoint and query data status &lt;- httr::GET( url = url, query = query, httr::write_disk( path = file.path(tempdir(), &quot;T_SAND.nc&quot;), overwrite = TRUE ) ) # to visualize the data # we need to load the {terra} # library library(&quot;terra&quot;) r &lt;- terra::rast(file.path(tempdir(), &quot;T_SAND.nc&quot;)) ## Warning: [rast] unknown extent terra::plot(r) 5.6.2.2.1 Authentication Depending on the API, authentication using a user name and a key or password is required. Then, the template should be slightly altered to accommodate for these requirements. Note that instead of the GET() command we use POST() as we need to post some authentication data before we can get the data in return. # an authenticated API query status &lt;- httr::POST( url = url, httr::authenticate(user, key), httr::add_headers(&quot;Accept&quot; = &quot;application/json&quot;, &quot;Content-Type&quot; = &quot;application/json&quot;), body = query, encode = &quot;json&quot; ) 5.7 Exercises 5.7.1 Files and file formats While not leaving your R session, download and open the files at location the following locations: https://github.com/xxx/data/demo1.csv https://github.com/xxx/data/demo2.csv https://github.com/xxx/data/demo3.csv Once loaded into your R environment, combine and save all data as a temporary CSV file. Read in the new temporary CSV file, and save it as a JSON file in your current working directory. # Your solution script Download and open the following file: https://github.com/xxx/xxx/some_data.nc What does this file contain? Write this file to disk (use the R documentation). # Your solution script Download and open the following file: https://github.com/xxx/xxx/some_data.tiff What does this file contain? Does this data seem familiar, and how can you tell? What are your conclusions? # Your solution script 5.7.2 API use 5.7.3 GET Download the HWSD total sand content data for the extent of Switzerland following the tutorial example. Visualize the data as a simple map. Download the HWSD topsoil silt content for the extent of Switzerland. 5.7.4 Dedicated library Use the {hwsdr} library (a dedicated package for the API) to download the same data. How does this compare to the previous code written? List how many data products there are on the ORNL MODIS data repository. Download the MODIS land cover map for the canton of Bern. 5.8 Solutions "],["code_mgmt.html", "Chapter 6 Code management 6.1 Learning objectives 6.2 Tutorial 6.3 Exercises 6.4 Solutions", " Chapter 6 Code management Chapter lead author: Koen Hufkens 6.1 Learning objectives In this chapter you will learn how to manage your code with common version control tools, i.e. git. You will learn how to: create a git project (new or from a template) track changes in your code project collaborate with others ensure reproducibility of your project by openly sharing your work and progress. 6.2 Tutorial Code management is a key of managing any data science project, especially when collaborating. Proper code management limits mistakes such as code loss and increases efficiency by structuring projects. In this chapter, we will discuss the management of code in both the location sense, where things are kept, and tracking temporal changes over time using a version control system. Current version control of code is dominated by the software tool git. However, version control has a long history and can be found not only in code development practices.For example, whenever you use track changes in a text document you apply a form of version control i.e., you track changes in your text over time and selectively accepted changes. In this respect git, as a tool for version control of code, does not differ much from track changes of a text document, but follows a manual modify –&gt; staged –&gt; commit workflow. Git workflow - by Paola Corrales and Elio Campitelli Git allows for the collaboration of multiple people on the same code, while being consistent in how changes are implemented. Built upon git are cloud based platforms such as github, gitlab and codeberg which make these collaborative decisions and operations even easier. Remote git workflow - by Paola Corrales and Elio Campitelli In this chapter, you will learn how to use git and github to manage your project and collaborate on code. NOTE: Coding style, and documentation practices of the code itself have been covered previously in the programming primers. Although the tutorial below focuses on github the jargon and operations are transferable to other platforms such as gitlab and codeberg. 6.2.1 Git and local version control Git allows for the tracking of changes in code (or any file) within a git project. A git project is defined by the topmost directory in which a git project is created. For example the following project is not tracked for changes using git. project/ ├─ YOUR_PROJECT.Rproj You can start tracking a project by initiating a local git repository using the following code in R. We’ll use the {usethis} package to make some of the setup a project easier. # initiate a github repository usethis::use_git() This will create a github repository in your project. It will also create a .gitignore file which specifies which files NOT to track (even if asked to). In addition it will make an first commit. 6.2.1.1 git add Before we can track anything we need to tell git which files to track. We therefore have to add them to an index of tracked files. You can either do this on the command line using: git add your_file.csv Or using the RStudio git panel. In this panel you will see all un-tracked files or directories highlighted with a yellow question mark. You select the file tick boxes to the left to stage all files for inclusion into the git repository. Once staged, the next step will be to finally commit these staged files to be included in git tracking. 6.2.1.2 git commit To store any changes to the files which were staged we need to commit these changes. We therefore hit the commit button, this will pop up a new window. Each commit needs a brief message describing what you have included in the staged files, or the commit message, as shown in the panel on the right. You need to provide this small message before pressing the commit button once more. This will let git track the changes to these files. A message will be shown if the commit is successful. With this you will track all files locally. Any new changes to a file will need to be committed to the git repository once more. So, unlike cloud services such as Dropbox, your files are not automatically tracked, this is a manual step. As with normal documents you are advised to save (commit) your changes to your project frequently. More so, if you create a new file you will need to add it before you can commit it. You can commit changes of staged files using the command line as well using the following command. git commit -m &quot;A message&quot;&quot; 6.2.2 Remote version control Local files limit the degree in which you can collaborate with people. This is where remote cloud based git solutions such as github, gitlab and codeberg come in. They provide a cloud based git repository which you can associate with your local project (see figure above). To create a remote project and successfully associate it with an R project we first have to specify some details, such as the username and email you used is singing up for github. To not leave your R session you can use the {usethis} package for this. # Configure your project library(usethis) usethis::use_git_config(user.name = &quot;Jane Doe&quot;, user.email = &quot;jane@example.org&quot;) For security reasons the use of your github password is not allowed in remote actions. You therefore need to generate a personal access token (PAT) which can be restricted in time and functionality. To proceed first generate a github PAT using these instructions. To create a new project on github hit the “+” sign top left on the github main page (once logged in), and select the “new repository” from the dropdown menu. A new interface will open up in which you should not use any template, but specify your own project name and brief description. Make sure your project is public, and all other settings are kept as is before you hit the “Create repository” button. Note the URL that is generated for your project, you will need it when creating a new RStudio project. Next we’ll setup an R project which is associated with the repository. Use: File &gt; New Project &gt; Version Control &gt; Git. In the “repository URL” paste the URL of your new GitHub repository, in the example above this would be https://github.com/khufkens/YOUR_PROJECT.git. select a location where to store the project, select the “Open in new session” option and click “Create Project”. This will create a *.Proj file as well as a .gitignore file (6.2.1.1). You can add both files as you would otherwise (see 6.2.1.1), and these files are tracked locally. 6.2.2.1 git push Once a remote git service has been configured you can push your local git repository to this remote repository, i.e. syncing both. You can use both the push buttons in the RStudio panel for this or the command linen using git push. At the end of a day or a session it is always advised to push your changes to your remote repository to store any changes. Remote git workflow - by Paola Corrales and Elio Campitelli NOTE: Syncing between github and your local repository is a manual task. If not performed the repository will not be synced. To retain all your changes sync both repositories often! 6.2.2.2 git pull and merge conflicts Git pull compares your local git repository with the remote one and implements more recent changes if there are any. Note, that if you make changes on both sides, i.e. your remote and local repository, at the same time you will generate a merge conflict. A merge conflict states that remote and local changes can’t be reconciled without supervised intervention on your part. Changes will be made to your local repository, but the files will include the below syntax for highlighting conflicting differences. &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Adding some content to mess with it later Append this text to initial commit ======= Changing the contents of text file from new branch &gt;&gt;&gt;&gt;&gt;&gt;&gt; new_branch_for_merge_conflict You will need to remove the &lt;&lt;&lt;, === and &gt;&gt;&gt; brackets and retain the changes you want to resolve the conflict before committing the changes again. Content which is currently on file is separated by === to that which provides a conflict and to be merged. 6.2.2.3 git clone You can create a new copy of your project on github using the git clone command. For example, on the command line, you can use: # create a local copy of the remote github repository git clone https://github.com/khufkens/YOUR_PROJECT.git to create a local copy of your remote repositories. You can then start working on this repository by using the modify -&gt; staged -&gt; commit -&gt; push workflow. 6.2.2.4 git fork and pull request You can also create a copy of any public github project from within your github account by creating a fork. You can create a fork of a project by pressing the fork button top right on any public github project page. The number of forks of a project is displayed next to the button, in case of the {rpmodel} package there are 24 forks of this project. You can give the forked project a new name and description if so desired. A fork allows you to experiment with the code stored in the original project without affecting it. However, the relation to the original project is maintained. If you want to contribute changes to the original project you can do so with a pull request. NOTE: to make changes to a fork project you will first have to clone it to your local system! See workflow above. In a forked project, go to the “Pull requests” tab and press the green “New pull request” button. You will then have to provide description of the changes you made. This information will be forwarded to the original owner of the project, who can accept these changes and accept the pull request and “pull” in the changes. 6.2.3 Location based code management - github templates Both code (and data) management require you to be conscientious about where you store your code (and data). Structuring your projects using a the same template will allow you to understand where all pieces of an an analysis are stored. This has been mentioned in the @ref(open_science) chapter. In our R project template we provide a project structure for both data and code which removes the mental overhead out of structuring data projects. This project structure sorts code, data and reporting in a consistent way. You can use the template in combination with a github based version (??) control approach to manage your projects. Simply create a new project from this template and clone the project to your local computer. Any changes to the project can be tracked by the workflows described above. To use the template create a new repository on github, as you otherwise would using the big green button. If you are in the project on github you can hit the green button top right (Use this template). Otherwise you can select the repository from the template dropdown menu, select computationales/R-project-template. Proceed as usual by naming your repository. However, be careful to select the correct owner of the project if you have multiple identities. Rename the default .Proj file. 6.3 Exercises 6.3.0.1 Location based code management Create a github account (if you do not have one already). Create a new R project using the R project template. Make some changes to the README.md Put a small data set in the appropriate directory Make sure that both local and remote repositories (projects) are synced 6.3.0.2 Github This is a team exercise: team up with someone else in the classroom. Person 1 - clone the github project of Person 2 Person 1 - create a new file in this project Person 1 - with changes made on github create a pull request Person 2 - review the pull request, and accept the pull request integrating new code in the project of Person 2 Person 2 - add a new file to your own project, and update the github project Person 1 - integrate these changes in your project Create a merge conflict and resolve the merge conflict To complete the exercise, reverse rolls between Person 1 and Person 2! 6.4 Solutions "],["open_science.html", "Chapter 7 Open science practices 7.1 Learning objectives 7.2 Tutorial 7.3 Exercises 7.4 References", " Chapter 7 Open science practices Chapter lead author: Koen Hufkens 7.1 Learning objectives In this chapter you will learn the reasons for practicing open science, and some of the basic methodological techniques that we can use to facilitate an open science workflow. In this chapter you will learn how to: structure a project manage a project workflow capture a session or machine state use dynamic reporting ensure data/project retention 7.2 Tutorial The scientific method relies on repeated testing of hypothesis. When dealing with data and formal analysis one can reduce this problem to the question: could an independent scientist attain the same results given the data and code. Although this seems trivial, this issue has vexed the scientific community. These days, many scientific publications are based on complex analyses with often large data sets. More so, methods in publications often are insufficiently detailed to really capture the scope of an analysis. Even from this technical point of view, the reproducibility crisis or the inability to reproduce experimental results, is a complex problem. This is further compounded by social aspects and incentives. The last decades scientific research has seen a steady increase in speed due to the digitization of many fields and the comodification of science. Although digitization has opened up new research possibilities it also had less desired outcomes such as the reproducibility crisis, overall decreasing research quality, and outright fraud. Historically research (output) has been confined to academic journals with a limited reach into the public (civil) domain. Digitzation made both the academic output visible, as well as the issues that stem from it (e.g. fraud). In many ways the reproducibility crisis as digital tools have made this position untenable. Open and reproducible science is in part a counter movement to make scientific research (output) accessible to the larger public, increase research transparency and countering accusations of fraud and limiting disinformation (to some extent). Open science therefore aims to be as open as possible about the whole scientific process, and as closed as desirable. It is important to acknowledge that there is a spectrum of reproducible data and code workflows which depends on the state or source of the data and the output of the code (or analysis). Within the context of this course and the discussion of open science we focus solely on the practical aspects for reproducible science, i.e. ensuring that given the same data and code the results will be similar. For further reading on the topic of reproducibility from an epistemic, or the science of doing science, point of view and general open science practices facilitating other reproducibility modes as shown in the table above I refer to the reference list at the end of this chapter. The basics of open science coding and data practices rely on a number of simple concepts. The below sections describe a selection of the most important ones. Sticking to these principles and/or tools will increase the reproducibility of your work greatly. 7.2.1 Project structure Reproducible science relies on a number of key components. Data management and the tracking of required meta-data is the first step in an open science workflow. Although current computers make it easy to “find” your data and are largely file location agnostic this is not the case in many research environments. Here files need a precise, structured, location. This structure allows you to determine both the function and or order of a workflow. It is good practice to have a consistent project structure within and between projects. This allows you to find most project components regardless of when you return to a particular project. Structuring a project in one folder also makes projects portable. All parts reside in one location making it easy to create a github project from this location, or just copy the project to a new drive. An example data structure for raw data processing is given below and we provided an R project template to work from and adjust on the lab website. A full description on using the template is provided in the next section (??). data-raw/ ├─ raw_data_product/ ├─ 00_download_raw_data.R ├─ 01_process_raw_data.R 7.2.2 Managing workflows Although some code is agnostic to the order of execution many projects are effectively workflows, where the output of one routine is required for the successful execution of the next routine. In order to make sure that a future you, or a collaborator, understands in which order things should be executed it is best to number scripts / code properly. This is the most basic approach to managing workflows. In the example below all statistics code is stored in the statistics folder in an overall analysis folder (which also includes code for figures). All statistical analysis are numbered, to ensure that the output of a first analysis is available to the following one. analysis/ ├─ statistics/ │ ├─ 00_random_forest_model.R │ ├─ 01_random_forest_tuning.R ├─ figures/ │ ├─ global_model_results_map.R │ ├─ complex_process_visualization.R 7.2.2.1 Automating and visualizing workflows with targets To sidestep some of the manual management in R you can use dedicated pipeline tool like the {targets} package in R. The targets package learns how your pipeline fits together, skips tasks that are already up to date, runs only the necessary computation. Given the highly controlled environment {targets} can also visualize the (progress of) your workflow. by rOpenSci {targets} Due to the added complexity of the targets package we won’t include extensive examples of such a workflow but refer to the excellent documentation of the package for simple examples. https://books.ropensci.org/targets/walkthrough.html 7.2.3 Capturing your session state Often code depends on various components, packages or libraries. These libraries and all software come in specific versions, which might or might not alter the behaviour of the cod which relies on them. If you want to ensure full reproducibility, especially across several years, you will need to capture the state of the system and libraries with which you ran the original analysis. In R the {renv} package serves this purpose and will provide an index of all the packages used in your project as well as their version.For a particular project it will create a local library of packages with a static version. These static packages will not be updated over time, and therefore ensure consistent results. This makes your analysis, isolated, portable and reproducible. The analogue in python would be the virtual environments, or venv program. When setting up your project you can run: # Initiate a {renv} environment renv::init() To initiate your static R environment. Whenever you want to save the state of your project (and its packages) you can call: # Save the current state of the environment / project renv::snapshot() To save any changes made to your environment. All data will be saved in a project description file called a lock file (i.e. renv.lock). It is advised to update the state of your project regularly, and in particular before closing a project. When you move your project to a new system, or share a project on github with collaborators, you can revert to the original state of the analysis by calling: # On a new system, or when inheriting a project # from a collaborator you can use a lock file # to restore the session/project state using renv::restore() NOTE: as mentioned in the {renv} documentation. “For development and collaboration, the .Rprofile, renv.lock and renv/activate.R files should be committed to your version control system. But the renv/library directory should normally be ignored. Note that renv::init() will attempt to write the requisite ignore statements to the project .gitignore.” We refer to 7.1 for details on github and its use. 7.2.4 Capturing a system state Although R projects and the use of targets make your workflow consistent the package versions used between various systems (e.g. your home computer, a cluster at the university etc. might vary). To address issues with changes in the versions of package you can use the {renv} package which manages package version (environments) for you. When tasks are even more complex and include components outside of R you can use Docker to provide containerization of an operating system and the included ancillary application. The {rocker} project provides access to some of these features within the context of reproducible R environments. Using these tools you can therefore emulate the state of a machine independent of the machine on which the docker file is run. These days Machine Learning applications are often deployed as docker sessions to limit the complexity of installing required software components. The application of docker based installs is outside the scope of the current course, but feel free to explore these resources as they are widespread in data science. 7.2.5 Readable reporting using Rmarkdown Within Rstudio you can use Rmarkdown dynamic documents to combine both text and code. Rmarkdown is ideal for reporting i.e., writing your final document presenting your analysis results. A Rmarkdown documents consists of a header document properties, such as how it should be rendered (as an html page, a docx file or a pdf), and the actual content. Below you see the header file of an Rmarkdown document that should be rendered as an html page. --- title: My R Markdown Report author: You output: html_document --- The remainder of the document shows a code chunk outlined by ``` quotes and the chunk arguments in {} brackets. Inline operations, the evaluation of code, in text is also possible by using single quotes around a variable or code. ```r x &lt;- 5 # radius of a circle ``` For a circle with the radius 5, its area is 78.5398163. The document can be rendered by calling rmarkdown::render() on the command line or hitting the “Knit” button in the RStudio IDE. Depending on your settings a html file, pdf or docx file will be generated in your current directory (and or displayed in the IDE viewer). # render the document on the command line rmarkdown::render() 7.2.5.1 Referencing and finding files In R projects all files can be referenced relative to the top most path of the project. When opening your_project.Rproj in RStudio you can load data in the console as such read.table(\"data/some_data.csv\"), specifying a “soft” relative path for some_data.csv. project/ ├─ your_project.Rproj ├─ statistics/ │ ├─ your_dynamic_document.Rmd ├─ data/ │ ├─ some_data.csv Rmarkdown files are rendered relative to the file path where to document resides. This means that data which resides in data can’t be accessed by statistics/your_dynamic_document.Rmd even when using an R project and soft relative paths (which work for scripts and functions). As such trying to render the your_dynamic_document.Rmd will fail as the file some_data.csv will not be found. --- title: Your dynamic document author: You output: html_document --- ```r data &lt;- read.table(&#39;data/some_data.csv&#39;) ``` We need to explicitly take the project’s base path into the fold using the {here} package. The here package gathers the absolute path of files inside an R project. As such, here::here(\"data/some_data.csv\") will return the full path of the data (e.g. /your_computer/project/data/some_data.csv). This absolute path will be a valid path for the read.table() function, making the Rmarkdown file render correctly. The correct Rmarkdown code to read the data therefore reads: --- title: Your dynamic document author: You output: html_document --- ```r data &lt;- read.table(here::here(&#39;data/some_data.csv&#39;)) ``` But why not use absolute paths to begin with? Portability! When I would run your *.Rmd file with an absolute path on my computer it would not render as the file some_data.csv would then be located at: /my_computer/project/data/some_data.csv 7.2.5.2 Limitations The file referencing issue and the common use of Rmarkdown as a one size fits all solution, containing all aspects from data cleaning to reporting, makes Rmarkdown files not portable or reproducible. The one size fits all approach to Rmarkdown also encourages bad project management practices. As illustrated above, if no session management tools such as the package {here} are used this automatically causes files to pile up in the top most level of a project, undoing most efforts to physically structure data and code as highlighted in 7.2.1. At the heart of this discussion are not only practical considerations but also the fact that R markdown documents mix two cognitive tasks, writing text content (i.e. reporting) and writing code. Switching between these two modes comes with undue overhead. If you code, you should not be writing prose, and vise versa. If your R markdown file contains more code than it does text, it should be considered an R script or function (with comments or documentation). Conversely, if your markdown file contains more text than code it probably is easier to collaborate on a true word processing file (or cloud based solution). The use case where the notebooks might serve some importance is true reporting of general statistics. R markdown files have their function for reporting concise results, once generated (through functions or analysis scripts) but should be generally be avoided to develop code &amp; ideas as it encourages bad project management practices. 7.2.6 Data retention Coding practices and documenting all moving parts in a coding workflow is only one practical aspect of open science. An additional component is long term data and code retention and versioning. For code online collaboration tools, such as github, gitlab or codeberg, provide a way to provide access to code. However, these tools should only be considered collaborative aids not a place to store code into perpetuity. Furthermore, these services mostly have a limit to how much data can realistically be stored in a repository (mostly ~2GB). For small projects data can be included in the repository itself, for larger projects this won’t be possible. To ensure long term storage of code and data, outside of commercial for profit services, it is best to rely on for example Zenodo. Zenodo is an effort by the European commission, but accessible to all, to facilitate archiving of science projects of all nature (code and data) up to 50GB. In addition, Zenodo provides a citable digital object identifier or DOI. This allows data and code, even if not formally published in a journal, to be cited. Other noteworthy open science storage options include Dryad and the Center for Open Science. 7.3 Exercises 7.3.1 Data and code management 7.3.1.1 External data You inherit a project folder which contains the following files. ~/project/ ├─ survey.xlsx ├─ xls conversion.csv ├─ xls conversion (copy 1).csv ├─ Model-test_1.R ├─ Model-test-final.R ├─ Plots.R ├─ Figure 1.png ├─ test.png ├─ Rplot01.png ├─ Report.Rmd ├─ Report.html What are your steps to make this project more reproducible? Write down how and why you would organize your project. 7.3.1.2 A new project What are the basic steps to create a reproducible workflow from a file management perspective? Create your own project using these principles and provide details the on steps involved and why they matter. The project should be a reproducible workflow: to download and plot a MODIS land cover map for Belgium using skills you learned in @ref(data_variety) contain a function to count the occurrences of land cover classes in the map as a formal function using skills you learned in @ref(data_wrangling) create a plot of the land cover map (see @ref(data_vis)) contain a dynamic report describing your answers to the above questions regarding how to structure a reproducible workflow 7.3.1.3 Tracking the state of your project Track the packages you use in the project you created using {renv} Install any additional library and update the state of your project create a simple {targets} project using the above workflow make changes to the API download routine rerun the targets project 7.4 References complete references "],["regression_classification.html", "Chapter 8 Regression and classification 8.1 Learning objectives 8.2 Tutorial 8.3 Exercises 8.4 Solutions", " Chapter 8 Regression and classification Chapter lead author: Pepa Aran Contents: Difference between regression and classification Linear regression Logistic regression Regression metrics Classification metrics, another link Comparing models (AIC, …) Detecting outliers: identification via distributions [hist(), boxplot(), qqnorm()], multivariate (Cook’s Distance to get influential values) Feature selection, stepwise regression, multi-colinearity (vif) 8.1 Learning objectives After completing this tutorial, you will be able to: Understand the basics of regression and classification models Fit linear and logistic regression models in R Choose and calculate relevant model performance metrics Evaluate and compare regression models Detect data outliers Select best predictive variables 8.2 Tutorial # Load necessary packages library(tidyr) library(dplyr) library(readr) library(lubridate) library(ggplot2) library(caret) library(broom) 8.2.1 Types of models Models try to explain relationships between variables through a mathematical formulation, particularly to predict a given target variable using other explanatory variables, also called predictors. Generally, we say that the target variable \\(Y\\) is a function (denoted \\(f\\)) of a set of explanatory variables \\(X_1, X_2, \\dots, X_p\\) and some model parameters \\(\\beta\\). Models can be represented as: \\[Y \\sim f(X_1, X_2, \\dots, X_p, \\beta)\\] This is a very general notation and depending on the structure of these components, we get to different modelling approaches. The first distinction comes from the type of target variable. Whenever \\(Y\\) is a continuous variable, we are facing a regression problem. If \\(Y\\) is categorical, we talk about classification. Regression Classification Target variable Continuous Categorical Common models Linear regression, polynomial regression, kNN, tree-based regression Logistic regression, kNN, SVM, tree classifiers Metrics RMSE, \\(R^2\\), adjusted \\(R^2\\), AIC, BIC Accuracy, precision, AUC, F1 8.2.2 Regression In this section, we will introduce the most basic regression model: linear regression. We’ll explain how to fit the model with R, how to include categorical predictors and polynomial terms. Finally, several performance metrics for regression models are presented. 8.2.2.1 Linear regression Theory Let’s start with the simplest model: linear regression. You probably have studied linear regression from a statistical perspective, here we will take a data-fitting approach. For example, we can try to explain the relationship between GPP and short wave radiation, like in the visualisation tutorial. The figure below shows a cloud of data points, and a straight line predicting GPP based on observed shortwave radiation values. # read and format data from Ch 3 hhdf &lt;- readr::read_csv(&quot;./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN.csv&quot;) ## Rows: 52608 Columns: 20 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (18): TA_F, SW_IN_F, LW_IN_F, VPD_F, PA_F, P_F, WS_F, GPP_NT_VUT_REF, N... ## dttm (2): TIMESTAMP_START, TIMESTAMP_END ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. set.seed(2023) gg1 &lt;- hhdf |&gt; sample_n(2000) |&gt; # to reduce the dataset ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(size = 0.75, alpha=0.4) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() segment_points &lt;- data.frame(x0 = 332, y0 = 3.65, y_regr = 8.77) gg1 + geom_segment(aes(x = x0, y = y0, xend = x0, yend = y_regr), data = segment_points, color = &quot;blue&quot;, lwd = 1.2, alpha = 0.8) ## `geom_smooth()` using formula = &#39;y ~ x&#39; We want to find the best straight line that approximates a cloud of data points. For this, we assume a linear relationship between a single explanatory variable \\(X\\) and our target \\(Y\\): \\[ Y_i \\sim \\beta_0 + \\beta_1 X_i, \\;\\;\\; i = 1, 2, ...n \\;, \\] where \\(Y_i\\) is the i-th observation of the target variable, and \\(X_i\\) is the i-th value of the (single) predictor variable. \\(n\\) is the number of observations we have and \\(\\beta_0\\) and \\(\\beta_1\\) are constant coefficients (model parameters). We call \\(\\beta_0\\) the intercept and \\(\\beta_1\\) the slope of the regression line. Generally, \\(\\hat{Y}\\) denotes the model prediction. Fitting a linear regression is finding the values for \\(\\beta_0\\) and \\(\\beta_1\\) such that, on average over all points, the distance between the line at \\(X_i\\), that is \\(\\beta_0 + \\beta_1 X_i\\) (in blue), and the observed value \\(Y_i\\), is as small as possible. Mathematically, this is minimizing the sum of the square errors, that is: \\[ \\min_{\\beta_0, \\beta_1} \\sum_i (Y_i - \\beta_0 - \\beta_1 X_i)^2 .\\] This linear model can be used to make predictions on new data, which are obtained by \\(\\hat{Y}_\\text{new} = \\beta_0 + \\beta_1 X_\\text{new}\\). When the new data comes from the same distribution as the data used to fit the regression line, this should be a good prediction. It’s not hard to imagine that the univariate linear regression can be generalized to a multivariate linear regression, where we assume that the target variable is a linear combination of \\(p\\) predictor variables: \\[Y \\sim \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\; ... \\; + \\beta_p X_p \\;.\\] Note that here, \\(X_1, \\dots, X_p\\) and \\(Y\\) are vectors of length corresponding to the number of observations in our data set (\\(n\\) - as above). Analogously, calibrating the \\(p+1\\) coefficients \\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_p\\) is to minimize the sum of square errors \\(\\min_{\\beta} \\sum_i (Y_i - \\hat{Y}_i)^2\\). While the regression is a line in two-dimensional space for the univariate case, it is a plane in three-dimensional space for bi-variate regression, and hyperplanes in higher dimensions. Implementation in R To fit a univariate linear regression model in R, we can use the lm() function. Already in Chapter 3, we made linear models by doing: # numerical variables only, remove NA df &lt;- hhdf %&gt;% dplyr::select(-starts_with(&quot;TIMESTAMP&quot;)) %&gt;% tidyr::drop_na() # fit univariate linear regression linmod1 &lt;- lm(GPP_NT_VUT_REF ~ SW_IN_F, data = df) Here, GPP_NT_VUT_REF is \\(Y\\), and SW_IN_F is \\(X\\). We can include multiple predictors for a multivariate regression, for example as: # fit multivariate linear regression linmod2 &lt;- lm(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = df) or all available features in our data set (all columns other than GPP_NT_VUT_REF in df) as: linmod3 &lt;- lm(GPP_NT_VUT_REF ~ ., data = df) linmod* is now a model object of class \"lm\". It is a list containing the following components: ls(linmod1) ## [1] &quot;assign&quot; &quot;call&quot; &quot;coefficients&quot; &quot;df.residual&quot; ## [5] &quot;effects&quot; &quot;fitted.values&quot; &quot;model&quot; &quot;qr&quot; ## [9] &quot;rank&quot; &quot;residuals&quot; &quot;terms&quot; &quot;xlevels&quot; Enter ?lm in the console for a complete documentation of these components and other details of the linear model implementation. R offers a set of generic functions that work with this type of object. The following returns a human-readable report of the fit. Here the residuals are the difference between the observed target values and the predicted values. summary(linmod1) ## ## Call: ## lm(formula = GPP_NT_VUT_REF ~ SW_IN_F, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -38.699 -2.092 -0.406 1.893 35.153 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.8732273 0.0285896 30.54 &lt;2e-16 *** ## SW_IN_F 0.0255041 0.0001129 225.82 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.007 on 41299 degrees of freedom ## Multiple R-squared: 0.5525, Adjusted R-squared: 0.5525 ## F-statistic: 5.099e+04 on 1 and 41299 DF, p-value: &lt; 2.2e-16 We can also extract coefficients \\(\\beta\\) with coef(linmod1) ## (Intercept) SW_IN_F ## 0.87322728 0.02550413 and the residual sum of squares (which we wanted to minimize) with sum(residuals(linmod1)^2) ## [1] 1035309 Although summary() provides a nice, human-readable output, you may find it unpractical to work with. A set of relevant statistical quantities are returned in a tidy format using tidy() from the broom package: broom::tidy(linmod1) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.873 0.0286 30.5 1.25e-202 ## 2 SW_IN_F 0.0255 0.000113 226. 0 Model advantages and concerns An advantage of linear regression is that the coefficients provide information that is straight-forward to interpret. We’ve seen above, that GPP_NT_VUT_REF increases by 0.0255 for a unit increase in SW_IN_F. Of course, the units of the coefficients depend on the units of GPP_NT_VUT_REF and SW_IN_F. This has the advantage that the data does not need to be normalised. That is, a linear regression model with the same predictive skills can be found, irrespective of whether GPP_NT_VUT_REF is given in g C m\\(^{-2}\\)s\\(^{-1}\\) or in kg C m\\(^{-2}\\)s\\(^{-1}\\). Another advantage of linear regression is that it’s much less prone to overfit than other algorithms. But this can also be a disadvantage, since linear regression can be rather under-fitting. It’s not able to capture non-linearities in the observed relationship and, as we’ll see later in this chapter, it exhibits a poorer performance than more complex models (e.g. polynomial regression) also on the validation data set. A further limitation is that least squares regression requires \\(n&gt;p\\). In words: the number of observations must be greater than the number of predictors. If this is not given, one can resort to step-wise forward regression, where predictors are sequentially added based on which predictor adds the most additional information at each step. You’ll encounter stepwise regression in the Application session 8. When multiple predictors are linearly correlated, then linear regression cannot discern individual effects and individual predictors may appear statistically insignificant when they would be significant if covarying predictors were not included in the model. Such instability can get propagated to predictions. Again, stepwise regression can be used to remedy this problem. However, when one predictor covaries with multiple other predictors, this may not work. For many applications in environmental sciences, we deal with limited numbers of predictors. We can use our own knowledge to examine potentially problematic covariations and make an informed pre-selection rather than throwing all predictors we can possibly think of at our models. Such a pre-selection can be guided by the model performance on a validation data set (more on that below). An alternative strategy is to use dimension reduction methods. Principal Component regression reduces the data to capture only the complementary axes along which our data varies and therefore collapses covarying predictors into a single one that represents their common axis of variation. Partial Least Squares regression works similarly but modifies the principal components so that they are maximally correlated to the target variable. You can read more on their implementation in R here. 8.2.2.2 Regression on categorical variables In the regression within categories section of Chapter 5, we saw that when we separate the data into sub-plots, hidden patterns emerge. This information is very relevant for modeling, because it can be included in our regression model. It is crucial to spend enough time exploring the data before you start modeling, because it helps to understand the fit and output of the model, but also to create models that capture the relationships between variables better. # create month category df_cat &lt;- hhdf |&gt; mutate(MONTH = lubridate::month(TIMESTAMP_START)) |&gt; tidyr::drop_na() |&gt; dplyr::select(MONTH, GPP_NT_VUT_REF, SW_IN_F) So far, we have only used continuous variables as explanatory variables in a linear regression. It is also possible to use categorical variables. To do this in R, such variables cannot be of class numeric, otherwise the lm() function treats them as continuous variables. For example, although the variable NIGHT is categorical with values 0 and 1, the model linmod3 treats it as a number. We must make sure that categorical variables have class character or, even better, factor. # fix class of categorical variables df_cat &lt;- df_cat |&gt; mutate(MONTH = as.factor(MONTH)) Now we can fit the linear model again: linmod_cat &lt;- lm(GPP_NT_VUT_REF ~ MONTH + SW_IN_F, data = df_cat) summary(linmod_cat) ## ## Call: ## lm(formula = GPP_NT_VUT_REF ~ MONTH + SW_IN_F, data = df_cat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36.212 -2.346 -0.223 2.200 34.416 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.6146109 0.0893693 18.067 &lt; 2e-16 *** ## MONTH2 -1.8105447 0.1294675 -13.985 &lt; 2e-16 *** ## MONTH3 -2.8800172 0.1264177 -22.782 &lt; 2e-16 *** ## MONTH4 -2.5667281 0.1278097 -20.082 &lt; 2e-16 *** ## MONTH5 -0.0288745 0.1273491 -0.227 0.820631 ## MONTH6 0.4614556 0.1298069 3.555 0.000378 *** ## MONTH7 0.1697514 0.1283830 1.322 0.186100 ## MONTH8 1.2942463 0.1231252 10.512 &lt; 2e-16 *** ## MONTH9 0.5140562 0.1165474 4.411 1.03e-05 *** ## MONTH10 -0.4807082 0.1152536 -4.171 3.04e-05 *** ## MONTH11 -1.3370277 0.1159059 -11.535 &lt; 2e-16 *** ## MONTH12 -1.2634451 0.1151530 -10.972 &lt; 2e-16 *** ## SW_IN_F 0.0246420 0.0001169 210.810 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.865 on 41288 degrees of freedom ## Multiple R-squared: 0.5776, Adjusted R-squared: 0.5775 ## F-statistic: 4704 on 12 and 41288 DF, p-value: &lt; 2.2e-16 In the fit summary, you can observe that, there are MONTH2 to MONTH12 parameters. MONTH is a factor which can take 12 different values: 1 to 12. lm() uses one of the factor level as the reference, in this case 1, and fits an intercept for the other categories. The result is a set of parallel regression lines, one for each different month. df_cat |&gt; mutate(MONTH_NAME = lubridate::month(as.integer(MONTH), label = TRUE)) |&gt; ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(alpha = 0.2) + geom_smooth(formula = y ~ x + 0, method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + labs(x = &quot;SW&quot;, y = &quot;GPP&quot;) + facet_wrap(~MONTH_NAME) + theme_classic() In the grid image, we can observe that GPP does not increase with SW at the same rate every month. For example, the increase in GPP is less steep in February than in September. To model this, we should consider a variable slope parameter for each month or category. In R, this is implemented by including an interaction term MONTH:SW_IN_F in the regression formula, like this: linmod_inter &lt;- lm(GPP_NT_VUT_REF ~ MONTH + SW_IN_F + MONTH:SW_IN_F, data = df_cat) # equivalently: lm(GPP_NT_VUT_REF ~ MONTH * SW_IN_F, data = df_cat) summary(linmod_inter) ## ## Call: ## lm(formula = GPP_NT_VUT_REF ~ MONTH + SW_IN_F + MONTH:SW_IN_F, ## data = df_cat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -28.891 -2.113 -0.420 1.892 34.029 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.0449603 0.0944991 21.640 &lt; 2e-16 *** ## MONTH2 -1.5386938 0.1369424 -11.236 &lt; 2e-16 *** ## MONTH3 -1.5249304 0.1365863 -11.165 &lt; 2e-16 *** ## MONTH4 -1.0050639 0.1396023 -7.199 6.15e-13 *** ## MONTH5 -0.4502367 0.1412720 -3.187 0.00144 ** ## MONTH6 -1.2559057 0.1474257 -8.519 &lt; 2e-16 *** ## MONTH7 -0.8440097 0.1446838 -5.833 5.47e-09 *** ## MONTH8 -0.2188300 0.1346734 -1.625 0.10419 ## MONTH9 -1.3407190 0.1269387 -10.562 &lt; 2e-16 *** ## MONTH10 -0.9991456 0.1235627 -8.086 6.32e-16 *** ## MONTH11 -1.2124373 0.1230946 -9.850 &lt; 2e-16 *** ## MONTH12 -1.0724209 0.1210819 -8.857 &lt; 2e-16 *** ## SW_IN_F 0.0158600 0.0008758 18.110 &lt; 2e-16 *** ## MONTH2:SW_IN_F -0.0030373 0.0011518 -2.637 0.00837 ** ## MONTH3:SW_IN_F -0.0058229 0.0009713 -5.995 2.05e-09 *** ## MONTH4:SW_IN_F -0.0038333 0.0009469 -4.048 5.17e-05 *** ## MONTH5:SW_IN_F 0.0087370 0.0009305 9.389 &lt; 2e-16 *** ## MONTH6:SW_IN_F 0.0135219 0.0009172 14.743 &lt; 2e-16 *** ## MONTH7:SW_IN_F 0.0110791 0.0009182 12.066 &lt; 2e-16 *** ## MONTH8:SW_IN_F 0.0151014 0.0009317 16.209 &lt; 2e-16 *** ## MONTH9:SW_IN_F 0.0180496 0.0009297 19.415 &lt; 2e-16 *** ## MONTH10:SW_IN_F 0.0097277 0.0009761 9.966 &lt; 2e-16 *** ## MONTH11:SW_IN_F -0.0011415 0.0010932 -1.044 0.29640 ## MONTH12:SW_IN_F -0.0099745 0.0012972 -7.689 1.52e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.593 on 41277 degrees of freedom ## Multiple R-squared: 0.6237, Adjusted R-squared: 0.6234 ## F-statistic: 2974 on 23 and 41277 DF, p-value: &lt; 2.2e-16 8.2.2.3 Polynomial regression Furthermore, the relationships between variables may be non-linear. In the previous example, we see that the increase in GPP saturates as shortwave radiation grows, which suggests that the true relationship could be represented by a curve. There are many regression methods that fit this kind of relationship, like polynomial regression, LOESS (local polynomial regression fitting), etc. Let’s fit a simple quadratic regression model, just for the month of August. For this we use the poly() function which constructs orthogonal polynomials of a given degree: quadmod &lt;- lm(GPP_NT_VUT_REF ~ poly(SW_IN_F, 2), data = df_cat |&gt; filter(MONTH == 8)) summary(quadmod) ## ## Call: ## lm(formula = GPP_NT_VUT_REF ~ poly(SW_IN_F, 2), data = filter(df_cat, ## MONTH == 8)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.367 -2.055 -0.253 1.801 32.375 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.13084 0.07944 89.77 &lt;2e-16 *** ## poly(SW_IN_F, 2)1 447.25113 4.61907 96.83 &lt;2e-16 *** ## poly(SW_IN_F, 2)2 -151.08797 4.61907 -32.71 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.619 on 3378 degrees of freedom ## Multiple R-squared: 0.7556, Adjusted R-squared: 0.7555 ## F-statistic: 5223 on 2 and 3378 DF, p-value: &lt; 2.2e-16 In the following plot, you can see how the model fit for GPP in August improves as we consider higher degree polynomials: df_cat |&gt; filter(MONTH == 8) |&gt; ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(alpha = 0.4) + geom_smooth(formula = y ~ x, method = &quot;lm&quot;, aes(color = &quot;lm&quot;), se = FALSE) + geom_smooth(formula = y ~ poly(x, 2), method = &quot;lm&quot;, aes(color = &quot;poly2&quot;), se = FALSE) + geom_smooth(formula = y ~ poly(x, 3), method = &quot;lm&quot;, aes(color = &quot;poly3&quot;), se = FALSE) + geom_smooth(formula = y ~ poly(x, 4), method = &quot;lm&quot;, aes(color = &quot;poly4&quot;), se = FALSE) + labs(x = &quot;SW&quot;, y = &quot;GPP&quot;, color = &quot;Regression&quot;) + theme_classic() 8.2.2.4 Metrics for regression We have explored several regression models to predict GPP based on SW. Now you may wonder how to choose one of them as your final analysis. Overall, we want to find the simplest model that best explains the data. We seek to find a balance between fitting the data well and generalizing to new data, which can be accomplished by reducing the complexity (for now, the number of parameters) of the model. Visual inspection of the model fit is a good start, but can become uninformative when the number of explanatory variables grows. So how can we measure model quality? In this section, we present some commonly used metrics to assess and compare regression models. MSE: The mean squared error is defined, as its name suggests, as: \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{Y_i})^2 \\] It measures the magnitude of the errors, and is minimized to fit a linear regression or, as we will see in Chapter 9, during model training when used as a loss function. Note that since it scales with the square of the errors, the MSE is particularly sensitive to large errors in single points (including outliers). RMSE: The root mean squared error is, as its name suggests, the root of the MSE: \\[ \\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{Y_i})^2} \\] Like the MSE, the RMSE also measures accuracy (the magnitude of the errors) and is minimized during model training. By taking the square root of mean square errors, the RMSE is in the same units as the data \\(Y\\) and is less sensitive to outliers as the MSE. \\(R^2\\): The coefficient of determination measures how close the fitted values are to the true target, and describes the proportion of variation in \\(Y\\) that is captured by modeled values \\(\\hat{Y}\\). Note that the denominator in the formula below is the variance of \\(Y\\), which is actually the MSE of a model that just predicts the average of \\(Y\\) (using no explanatory variable \\(X\\)). So \\(R^2\\) is basically measuring how much better than just taking the mean of \\(Y\\) our model is. It is traditionally defined as: \\[ R^2 = 1 - \\frac{\\sum_i (Y_i - \\hat{Y}_i)^2}{\\sum_i (Y_i - \\bar{Y})^2} \\] In this case, the goal is to maximize the metric, thus trying the explain as much variation in \\(Y\\) as possible. In contrast to the MSE and RMSE, \\(R^2\\) measures goodness of fit and gives a quantity between 0 and 1 that takes into consideration how variable the target is. We can actually write \\(R^2 = 1 - \\frac{MSE}{\\hat{Var}(Y)}\\). A perfect fit is quantified by \\(R^2 = 1\\) and uninformative models have an \\(R^2\\) approaching zero. For some applications where the data is very noisy, an \\(R^2\\) of 0.6 may already be considered good. Pearson’s \\(r^2\\): The linear correlation between two variables (here \\(Y\\) and \\(Z\\)) is measured by the Pearson’s correlation coefficient \\(r\\) and defined by \\[r_{YZ} = \\frac{\\sum_i (Y_i - \\bar{Y}) (Z_i - \\bar{Z}) }{\\sqrt{ \\sum_i(Y_i-\\bar{Y})^2}\\sqrt{ \\sum_i(Z_i-\\bar{Z})^2 } }.\\] The coefficient of determination and Pearson’s correlation are closely related. The square of the Pearson’s correlation between the observed \\(Y\\) and fitted values \\(\\hat{Y}\\) coincides with the coefficient of determination for a linear regression (see proof here). It holds that \\(R^2 = r_{Y \\hat{Y}}^2\\). The distinction between uppercase and lowercase nomenclature is often not consistent in the literature. The uppercase \\(R^2\\) is commonly used as coefficient of determination, in the context of comparing observed and predicted values. When the correlation between two different variables in a sample is quantified, the lowercase \\(r^2\\) is commonly used. The following scatterplots show different synthetic data points and regression lines. For each one, the Pearson correlation between target and predictor is calculated, together with the \\(R^2\\) and RMSE for the corresponding regressions. x &lt;- rnorm(100) # Function that creates scatterplots with regression metrics plot_regression &lt;- function(d, title){ d |&gt; ggplot(aes(x = x, y = y)) + geom_point(size = 0.75, alpha = 0.8) + labs(title = title, subtitle = (paste(&quot;r^2 =&quot;, round(cor(d$x, d$y), 3), &quot;, R^2 =&quot;, round(summary( lm(y~x, data = d) )$r.squared, 3)))) + geom_smooth(formula = y ~ x, method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + lims(x = c(-4, 4), y = c(-4, 4)) + theme_classic() } p1 &lt;- plot_regression(data.frame(x = x, y = rnorm(100)), &quot;Uncorrelated&quot;) p2 &lt;- plot_regression(data.frame(x = x, y = 0.5 * x), &quot;Perfectly linearly correlated&quot;) ## Warning in summary.lm(lm(y ~ x, data = d)): essentially perfect fit: summary may ## be unreliable p3 &lt;- plot_regression(data.frame(x = x, y = 0.5*x + rnorm(100)), &quot;Low correlation&quot;) p4 &lt;- plot_regression(data.frame(x = x, y = 0.5*x + rnorm(100, sd = 0.2)), &quot;Higher correlation&quot;) cowplot::plot_grid(p1, p2, p3, p4) Metrics for correlation like the ones above should not be used as a loss function because they do not penalize biased models. All of them keep improving as we include more variables, where they are actually informative or not. The \\(R^2\\) always increases when predictors are added to a model. This is particularly critical in the context of machine learning when we compare alternative models that differ by their number of parameters. In other words, the \\(R^2\\) of a model with a large number of predictors tends to give an overconfident estimate of its predictive power. We will introduce cross-validation in Chapter 9, which yields good predictive power. This is the “gold-standard”. But when the number of data points is small, cross validation estimates may not be robust. Without resorting to cross validation, the effect of spuriously improving the evaluation metric by adding uninformative predictors can also be mitigated by penalizing the number of predictors \\(p\\). Different metrics are available: Adjusted \\(R^2\\): The adjusted \\(R^2\\) discounts values by the number of predictors. It is defined as \\[ {R}^2_{adj} = 1 - (1-R^2) \\; \\frac{n-1}{n-p-1} \\;, \\] where \\(n\\) (as before) is the number of observations, \\(p\\) the number of parameters and \\(R^2\\) the usual coefficient of determination. Same as for \\(R^2\\), the goal is to maximize \\(R^2_{adj}\\). For a fitted model in R modl, its value is returned by summary(modl)$adj.r.squared. AIC: the Akaike’s Information Criterion is defined in terms of log-likelihood (covered in Quantitative Methoden) but for linear regression it can be written as: \\[ \\text{AIC} = n \\log \\Big(\\frac{\\text{SSE}}{n}\\Big) + 2(p+2) \\] where \\(n\\) is the number of observations used for estimation, \\(p\\) is the number of explanatory variables in the model and \\(SSE\\) is the sum of squared errors (\\(SSE= \\sum_i (Y_i-\\hat{Y_i})^2\\)). Also in this case we have to minimize it and the model with the minimum value of the AIC is often the best model for forecasting. Since it penalizes having many parameters, it will favor less complex models. AIC\\(_c\\): For small values of \\(n\\) the AIC tends to select too many predictors. A bias-corrected version of the AIC is defined as: \\[ \\text{AIC}_c = \\text{AIC} + \\frac{2(p + 2)(p + 3)}{n-p-3} \\] Also AIC\\(_c\\) is minimized for an optimal predictive model. BIC: the Schwarz’s Bayesian Information Criterion is defined as \\[ \\text{BIC} = n \\log \\Big(\\frac{\\text{SSE}}{n}\\Big) + (p+2) \\log(n) \\] Also in this case our goal is to minimize the BIC. This metric has the feature that if there is a true underlying model, the BIC will select that model given enough data. The BIC tends to select a model with fewer predictors than AIC. Implementation in R Let’s calculate the metrics introduced above for a few of the fitted regression models. Some of these metrics, like \\(R^2\\) and the adjusted \\(R^2\\) are given by the summary() function. Alternatively, the {yardstick} package provides implementations for a few of these metrics, which we compute below: compute_regr_metrics &lt;- function(linmod){ p &lt;- length(linmod$coefficients) sse &lt;- sum(linmod$residuals^2) n &lt;- length(linmod$residuals) round( c(mse = mean(linmod$residuals^2), rmse = sqrt(mean(linmod$residuals^2)), R2 = summary(linmod)$r.squared, R2_adj = summary(linmod)$adj.r.squared, r2 = cor(linmod$fitted, linmod$model$GPP_NT_VUT_REF)^2, AIC = extractAIC(linmod)[2], AIC_adj = extractAIC(linmod)[2] + 2*(p+2)*(p+3)/(n-p-3), BIC = BIC(linmod) # this implementation is based on log-likelihood ), 3) } metrics &lt;- sapply(list(linmod1, linmod2, linmod_cat, quadmod), compute_regr_metrics) colnames(metrics) &lt;- c(&quot;Linear model&quot;, &quot;Linear model 2&quot;, &quot;Linear + categories&quot;, &quot;Quadratic model&quot;) metrics ## Linear model Linear model 2 Linear + categories Quadratic model ## mse 25.067 24.782 23.664 21.317 ## rmse 5.007 4.978 4.865 4.617 ## R2 0.553 0.558 0.578 0.756 ## R2_adj 0.553 0.558 0.577 0.755 ## r2 0.553 0.558 0.578 0.756 ## AIC 133058.006 132589.878 130700.234 10350.160 ## AIC_adj 133058.007 132589.880 130700.246 10350.177 ## BIC 250293.053 249842.182 248030.196 19971.526 8.2.3 Classification We will introduce a classification problem with a binary target, although it’s easy to generalize to categorical variables with more than two classes. As an example, we will try to classify whether it’s day or night (NIGHT) based on shortwave and longwave radiation measurements (SW_IN_F and LW_IN_F). Let’s look at the data first: set.seed(2023) gg2 &lt;- hhdf |&gt; sample_n(1000) |&gt; # to reduce the dataset ggplot(aes(x = SW_IN_F, y = LW_IN_F, color = as.factor(NIGHT))) + geom_point(size = 0.75) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;Longwave radiation (W m&quot;^-2, &quot;)&quot;))) + theme_classic() gg2 At first sight, it’s easy to see that the short wave radiation is close to zero in the night, while longwave radiation doesn’t seem to differ strongly between daytime and nighttime. hhdf |&gt; filter(NIGHT==0) |&gt; pull(SW_IN_F) |&gt; hist(breaks=30) Using this example, we’ll cover logistic regression, its implementation in R and metrics for classification. 8.2.3.1 Logistic regression Theory A classification problem is a bit more difficult to write mathematically than a regression problem. Before, the mathematical representation of GPP_NT_VUT_REF ~ SW_IN_F was GPP_NT_VUT_REF\\(\\;=\\; \\beta_0 + \\beta_1\\)SW_IN_F. With the classification model NIGHT ~ SW_IN_F, we cannot just write NIGHT\\(\\;=\\; \\beta_0 + \\beta_1\\)SW_IN_F because NIGHT is not a number. Hence, the categorical variable must be encoded, in this case 0 means “day” and 1 means “night”. We already loaded the data into R with this encoding. The next issue is that a linear model makes continuous predictions in the entire real numbers space \\((-\\inf, \\inf)\\), but we want the predictions to be in \\(\\{0, 1\\}\\). We can transform these values to be in \\([0,1]\\) with a link function. For a binary response, it’s common to use a logit link function: \\[\\text{logit}(z) = \\frac{\\exp(z)}{1+\\exp(z)}\\] curve(exp(x)/(1+exp(x)), -5, 5, ylab = &quot;logit(x)&quot;) Combining a linear model (with any type of predictors, like for regression) and a logit link function, we arrive to a logistic regression model: \\[f(X, \\beta) = \\text{logit}(\\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p) = \\frac{\\exp(\\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p)}{1 + \\exp(\\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p)}.\\] This predicted value can be understood as the probability of belonging to class 1 (in our example, nighttime). A classification rule is defined such that an observation \\(X_{new}\\) with a predicted probability of belonging to class 1 higher than a given threshold \\(\\tau\\) (i.e. \\(f(X_{new}, \\beta) &gt; \\tau\\)) will be classified as 1; and if the predicted probability is smaller than the threshold, it will be classified as 0. A logistic regression model results in a linear classification rule. This means that the \\(p\\)-dimensional space will be divided in two by a hyperplane, and the points falling in each side of the hyperplane will be classified as 1 or 0. In the example above with shortwave and longwave radiation as predictors, the classification boundary would look something like this: Furthermore, to fit a logistic regression model means to calculate the maximum likelihood estimator with an iterative algorithm. Implementation in R To fit a logistic regression in R we can use the glm() function, which fits a generalized linear model, indicating that our target variable is binary and the link function is a logit function. Let’s see the model output: logmod &lt;- glm(NIGHT ~ SW_IN_F + LW_IN_F, family = binomial(link = logit), data = hhdf) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred summary(logmod) ## ## Call: ## glm(formula = NIGHT ~ SW_IN_F + LW_IN_F, family = binomial(link = logit), ## data = hhdf) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.3046 0.0000 0.0000 0.1853 8.4904 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 7.6170522 0.2326894 32.73 &lt;2e-16 *** ## SW_IN_F -1.1869830 0.0222507 -53.35 &lt;2e-16 *** ## LW_IN_F -0.0110463 0.0007195 -15.35 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 72825.2 on 52607 degrees of freedom ## Residual deviance: 9728.7 on 52605 degrees of freedom ## AIC: 9734.7 ## ## Number of Fisher Scoring iterations: 15 This model results in a linear classification boundary that splits the predictor variables space in two. You can see it plotted below: beta &lt;- coef(logmod) gg2 + geom_abline(slope = -beta[2]/beta[3], intercept = (0.5-beta[1])/beta[3]) Most blue points fall to one side of the dashed classification line and most red points to the other side; this is what we wanted. The points that are in the wrong side of the line are misclassified by the logistic regression model, we’re trying to minimize that. Note that, just like for linear regression, a logistic regression model allows to use categorical explanatory variables and polynomial transformations of the predictors to achieve better-fitting classification models. Model advantages and concerns One advantage of logistic regression is simplicity. It’s part of the generalized linear regression family of models and the concept of a link function used to build such a model can also be used for various types of response variables (not only binary, but also count data…). You can find more details in this Wikipedia article. Furthermore, logistic regression allows for an interesting interpretation of its model parameters: log-odds and log-odds ratios. Logg-odds ratios represent how much likely it is to find one class versus the other (e.g. class 1 is twice as likely than class 0 whenever we have probabilities \\(66\\%\\) vs \\(33\\%\\)). Increases in the values of the predictors affect the log-odds multiplicatively. It is easy to extend a logistic regression model to more than two classes by fitting models iteratively. For example, first you classify class 1 against classes 2 and 3; then another logistic regression classifies class 2 against 3. Nevertheless, logistic regression relies on statistical assumptions to fit the parameters and interpret the fitted parameters. So whenever these assumptions are not met, one must be careful with the conclusions drawn. Other machine learning methods, that will be covered in Chapters 9 and 10, can also be used for classification tasks. These offer more flexibility than logistic regression (are not necessarily linear) and don’t need to satisfy strict statistical assumptions. 8.2.3.2 Metrics for classification Measuring the quality of a classification model is based on counting how many observations were correctly classified, rather than the distance between the values predicted by a regression and the true observed values. These can be represented in a confusion matrix: \\(Y = 1\\) \\(Y = 0\\) \\(\\hat{Y} = 1\\) True positives (TP) False positives (FP) \\(\\hat{Y} = 0\\) False negatives (FN) True negatives (TN) In a confusion matrix, correctly classified observations are on the diagonal and off-diagonal values correspond to different types of errors. Some of these error types are more relevant for certain applications. Imagine that you want to classify whether the water of a river is safe to drink based on measurements of certain particles or chemicals in the water (Y=1 means safe, Y=0 means unsafe). It’s much worse to tag as “safe” a polluted river than to tag as “unsafe” a potable water source, one must be conservative. In this case, we would prioritize avoiding false positives and wouldn’t care so much about false negatives. The following metrics are widely used and highlight different aspects of our modeling goals. Accuracy is simply the proportion of outputs that were correctly classified: \\[ \\text{Accuracy}=\\frac{\\text{TP} + \\text{TN}}{N},\\] where \\(N\\) is the number of observations. This is a very common metric for training ML models and treats both classes as equally important. It’s naturally extended to multi-class classification and usually compared to the value \\(\\frac{1}{C}\\) where \\(C\\) is the number of classes. Classification models are usually compared to randomness: how much better is our model compared to throwing a coin for classification? At random, we would assign each class \\(50\\%\\) of the time. So if we assume that both classes are as likely to appear, that is, they are balanced, the accuracy of a random guess would be around \\(0.5\\). Hence, we want the accuracy to be “better than random”. If there are \\(C\\) different classes and the observations are balanced, we want the accuracy to be above \\(\\frac{1}{C}\\times 100 \\%\\). Keep in mind that for a dataset where \\(90\\%\\) of the observations are from class 1 and \\(10\\%\\) from class 0, always predicting 1 would lead to a \\(0.9\\) accuracy. This value may sound good, but that model is rubbish because it doesn’t use any information from predictors. Be careful when working with imbalanced classes and interpreting your results. Precision measures how often our “positive” predictions are correct: \\[\\text{precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}.\\] The true positive rate (TPR), also called Recall or sensitivity measures the proportion of real “positives” (\\(Y = 1\\)) we are able to capture: \\[ \\text{TPR} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}.\\] The false positive rate (FPR) is defined by \\[\\text{FPR} = \\frac{\\text{FP}}{\\text{FP}+\\text{TN}}.\\] and is related to another metric called specificity by \\(\\text{FPR} = 1 - \\text{specificity}\\). ROC curve: To evaluate the performance of a binary classification model, it’s common to plot the ROC curve. The TPR is plotted against the FPR, for varying values of the threshold used in the classification rule. When we decrease the threshold, we get more positive values (more observations are classified as 1), increasing both the true positive and false positive rate. The following image describes clearly how to interpret a ROC curve plot: ROC curves and how they compare, from Wikipedia. AUC: The “area under the curve” is defined as the area left below the ROC curve. For a random classifier we would have AUC=0.5 and for the perfect classifier, AUC=1. It’s good to try to increase the AUC and it’s used often as a reporting metric. Nevertheless, a visual inspection of the ROC curve can say even more. F1: The F1 score is a more sophisticated metric, defined as the harmonic mean of precision and sensitivity, or in terms of the confusion matrix values: \\[F1= 2 \\times \\frac{\\text{prec} \\times \\text{recall}}{\\text{prec} + \\text{recall}} = \\frac{2 \\text{TP}}{2 \\text{TP} + \\text{FP} + \\text{FN}}.\\] This metric provides good results for both balanced and imbalanced datasets and takes into account both the model’s ability to capture positive cases (recall) and be correct with the cases it does capture (precision). It takes values between 0 and 1, with 1 being the best and values of 0.5 and below being bad. These metrics can be used to compare the quality of different classifiers but also to understand the behaviour of a single classifier from different perspectives. This was an introduction of the most basic classification metrics. For a more information on the topic, check out this book chapter. Implementation in R Let’s take a look at the previous metrics for the logistic regression model we fitted before. The confusionMatrix() function from the {caret} library provides most of the statistics introduced above. # Make classification predictions Y &lt;- as.factor(logmod$data$NIGHT) Y_pred &lt;- as.factor(round(logmod$fitted.values)) # Use 0.5 as threshold # Change class names levels(Y) &lt;- levels(Y_pred) &lt;- c(&quot;DAY&quot;, &quot;NIGHT&quot;) # plot confusion matrix conf_matrix &lt;- caret::confusionMatrix(data = Y_pred, reference = Y) conf_matrix ## Confusion Matrix and Statistics ## ## Reference ## Prediction DAY NIGHT ## DAY 26218 19 ## NIGHT 1261 25110 ## ## Accuracy : 0.9757 ## 95% CI : (0.9743, 0.977) ## No Information Rate : 0.5223 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.9513 ## ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.9541 ## Specificity : 0.9992 ## Pos Pred Value : 0.9993 ## Neg Pred Value : 0.9522 ## Prevalence : 0.5223 ## Detection Rate : 0.4984 ## Detection Prevalence : 0.4987 ## Balanced Accuracy : 0.9767 ## ## &#39;Positive&#39; Class : DAY ## Now we can visualize the confusion matrix as a mosaic plot. This is quite helpful when we work with many classes. mosaicplot(conf_matrix$table, main = &quot;Confusion matrix&quot;) 8.2.4 Model evaluation Model evaluation refers to several techniques that help you understand how the model performs, whether this behavior is what you expect and how you can improve it. You can use metrics and plots to get an overview of the weaknesses of your model. This section covers model comparison, variable selection and outlier detection, and more concepts related to model evaluation (overfitting, data pre-processing, cross-validation…) are explained in the remaining chapters. Concepts will be explained using regression as an example, but are directly translated to classification problems. 8.2.4.1 Model comparison Be systematic with your model comparisons. Three key ideas in model selection are: Comparisons should be hierarchical: compare a model to another that “contains it”, i.e. compare GPP_NT_VUT_REF ~ SW_IN_F to GPP_NT_VUT_REF ~ SW_IN_F + LW_IN_F, not GPP_NT_VUT_REF ~ SW_IN_F to GPP_NT_VUT_REF ~ NIGHT + TA_F. Complexity must be increased slowly: add one variable at a time, not three variables all at once. This helps avoid collinearity in the predictors. Choose the most appropriate metric: if possible, a metric that accounts for model complexity and represents the goal of your analysis (e.g. recall for a classification where you don’t want to miss any positives). If you’re considering different model approaches for the same task, you should first fit the best possible model for each approach, and then compare those optimized models to each other. For example, fit the best linear regression with your available data, the best kNN non-parametric regression model and a random forest; then compare those three final models and choose the one that answers your research question the best. One must be careful not to keep training or improving models until they fit the data perfectly, but maintain the models’ ability to generalize to newly available data. Chapter 9 introduces the concept of overfitting, which is central to data science. Think of model interpretation and generalization when comparing them, not only of performance. Simple models can be more valuable than very complex ones because they tell a better story about the data (e.g. by having few very good predictors rather than thousands of mediocre ones, from which we cannot learn the underlying relationships). 8.2.4.2 Variable selection Let’s think of variable selection in the context of linear regression. A brute force approach to variable selection would be: Fit a linear regression for each combination of available predictors, calculate a metric (e.g. AIC) and choose the best one (lowest AIC). The problem is, if you have 8 predictors, you would fit 40320 different regression models. This can be very computationally expensive. Instead, take a hierarchical greedy approach, starting with an empty model (just an intercept) and adding one variable at a time. This is called stepwise (forward) regression. The algorithm goes as follows: First, you fit all regression models with just one variable and compute the \\(R^2\\). Then, select the one leading to the greatest \\(R^2\\) (best fitting model) and compute the AIC (or BIC). In the next step, you add a second variable, one at a time, fitting the regression models and calculating their \\(R^2\\). Choose as second variable the one leading to the best \\(R^2\\). Then, compute the AIC. If the AIC (which accounts for model fit and complexity) is worse, that is, bigger, stop and keep the univariate linear model. If the AIC is better, that is, smaller, add the second variable and repeat the previous steps to include a third variable. The method finishes once you cannot reduce the AIC anymore, or when you run out of variables. In the end, you’ll have more or less the best possible linear regression model. The function step() implements the stepwise algorithm in R. This stepwise approach can be done backwards, starting with a full model (all available variables) and removing one at a time. Or even with a back-and-forth approach, where you look at both including a new or removing an existing variable at each step (optimizing AIC). Furthermore, this algorithm can be applied to fitting a polynomial regression. We want to increase the degree of the polynomials unit by unit. For a model with categorical variables, interaction terms should only be considered after having the involved variables as “intercept only”. Multicollinearity exists when there is a correlation between multiple explanatory variables in a multivariate regression model. This is problematic because it makes the estimated coefficients corresponding to the variables that are highly correlated very unstable. Since two highly correlated variables explain almost the same, it doesn’t matter whether we include one or the other in the model (the performance metrics will be similar) or even if we include both of them. Hence, it becomes difficult to say which variables actually influence the target. The variance inflation factor (VIF) is a score from economics that measures the amount of multicollinearity in regression, based on how the estimated variance of a coefficient is inflated due to its correlation with another predictor. It’s calculated as \\[\\text{VIF}_i = \\frac{1}{1 - R^2_i},\\] where \\(R^2_i\\) is the coefficient of determination for regressing the i\\(^{th}\\) predictor on the remaining ones. A VIF\\(_i\\) is computed for each predictor in the multivariate regression model we are evaluating, meaning: if \\(\\text{VIF}_i = 1\\) variables are not correlated; if \\(1 &lt; \\text{VIF}_i &lt; 5\\) there is moderate collinearity; and if \\(\\text{VIF}_i \\geq 5\\) they are highly correlated. Because they can be almost fully explained by all the other predictors (high \\(R^2_i\\)), these variables are redundant in our final model. When we work with high-dimensional data (that is, we have more variables than observations) there are better techniques to do variable selection than stepwise regression. Since the predictors space is so large, we could fit a line that passes through all the observations (a perfect fit), but does the model generalize? We don’t know. For example, Lasso and Ridge regression incorporate variable selection in the fitting process (you can check this post if you’re curious). 8.2.4.3 Outlier detection Detecting outliers is very important, because they can affect the fit of a model greatly. Take a look at the two linear regressions below and how one single weird observation can throw off the fit. Whenever an observation is very distant from the center of the predictor’s distribution, it becomes very influential (it has a bit leverage). If the observed response for that data point is in harmony with the rest of points, nothing happens, but if it’s also off, the regression model will be affected greatly. set.seed(2023) hhdf_small &lt;- hhdf |&gt; sample_n(100) |&gt; # reduce dataset select(SW_IN_F, GPP_NT_VUT_REF) gg3 &lt;- hhdf_small |&gt; ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(size = 0.75) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, fullrange = TRUE) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() + ylim(-20, 40) + xlim(0, 2000) gg4 &lt;- hhdf_small |&gt; add_row(SW_IN_F = 2000, GPP_NT_VUT_REF = -20) |&gt; # add outlier ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(size = 0.75) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, fullrange = TRUE) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() + geom_point(aes(x = 2000, y = -20), colour=&#39;blue&#39;) + ylim(-20, 40) + xlim(0, 2000) cowplot::plot_grid(gg3, gg4) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; The first step to identifying outliers is to look at your data, one variable at a time. Plot a histogram to see the rough distribution of a variable, this will help identify what kind of values to expect. In Chapters 3 and 4 it was introduced how to identify values that fell out of this distribution using histograms and boxplots. Checking in the histogram if the distribution has fat tails helps to discern whether the values that pop out of a boxplot should be considered outliers or not. gg5 &lt;- hhdf_small |&gt; add_row(SW_IN_F = 2000, GPP_NT_VUT_REF = -20) |&gt; # add outlier ggplot(aes(x = GPP_NT_VUT_REF, y = after_stat(density))) + geom_histogram(fill = &quot;grey70&quot;, color = &quot;black&quot;) + geom_density(color = &#39;red&#39;)+ labs(title = &#39;Histogram, density and boxplot&#39;, x = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() gg6 &lt;- hhdf_small |&gt; add_row(SW_IN_F = 2000, GPP_NT_VUT_REF = -20) |&gt; # add outlier ggplot(aes(x = &quot;&quot;, y = GPP_NT_VUT_REF)) + geom_boxplot(fill = &quot;grey70&quot;, color = &quot;black&quot;) + coord_flip() + theme_classic() + theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) + labs(y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) gg7 &lt;- hhdf_small |&gt; add_row(SW_IN_F = 2000, GPP_NT_VUT_REF = -20) |&gt; # add outlier ggplot(aes(x = SW_IN_F, y = after_stat(density))) + geom_histogram(fill = &quot;grey70&quot;, color = &quot;black&quot;) + geom_density(color = &#39;red&#39;)+ labs(title = &#39;Histogram, density and boxplot&#39;, x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;))) + theme_classic() gg8 &lt;- hhdf_small |&gt; add_row(SW_IN_F = 2000, GPP_NT_VUT_REF = -20) |&gt; # add outlier ggplot(aes(x = &quot;&quot;, y = SW_IN_F)) + geom_boxplot(fill = &quot;grey70&quot;, color = &quot;black&quot;) + coord_flip() + theme_classic() + theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) + labs(y = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;))) cowplot::plot_grid(gg5, gg7, gg6, gg8, ncol = 2, rel_heights = c(2,1), align = &#39;v&#39;, axis = &#39;lr&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. A Q-Q Plot depicts the sample quantiles of a variable against the theoretical quantiles of a distribution of our choice, usually a normal distribution. In the histograms above, GPP looks somewhat Gaussian but with fatter tails and slightly skewed to the right, while shorwave radiation is skewed to the right, resembling an exponential distribution. This is also visible in the Q-Q plots below, because outliers deviate greatly from the straight line (which represents a match between the observed values and the theoretical distribution): gg9 &lt;- hhdf_small |&gt; add_row(SW_IN_F = 2000, GPP_NT_VUT_REF = -20) |&gt; # add outlier |&gt; ggplot(aes(sample = GPP_NT_VUT_REF)) + geom_qq() + geom_qq_line() + labs(y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), x = &quot;Theoretical normal quantiles&quot;) + theme_classic() gg10 &lt;- hhdf_small |&gt; add_row(SW_IN_F = 2000, GPP_NT_VUT_REF = -20) |&gt; # add outlier |&gt; ggplot(aes(sample = SW_IN_F)) + geom_qq() + geom_qq_line() + labs(y = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), x = &quot;Theoretical normal quantiles&quot;) + theme_classic() cowplot::plot_grid(gg9, gg10, ncol=2) For linear (and logistic) regression, we would like variables to look as normal as possible. You’ve probably learned some of the reasons for this in quantitative methods courses, but are beyond the scope of this class. It’s common to study the distribution of the regression residuals with QQ-plots to assess if model assumptions are met. Above, you can see the distributions of our target and predictor (with outliers). And it’s very easy to see the weird value for the shortwave radiation but for GPP it doesn’t stick out so much. This already points to how important it is to check their multivariate distribution. R provides some useful plots from the fitted regression objects, in particular the “Residuals vs Leverage” plot: # Fit regression with outlier linmod_outlier &lt;- lm(GPP_NT_VUT_REF ~ SW_IN_F, data = add_row(hhdf_small, SW_IN_F = 2000, GPP_NT_VUT_REF = -20)) plot(linmod_outlier, 5) This plot shows the leverage (see the mathematical definition here) of each observation against the corresponding residual from the fitted linear regression. Points with high leverage, i.e. far from the center of the predictor distribution, and big residuals, i.e. far from the fitted regression line, are very influential. Cook’s distance (definition here) is an estimate of the influence of a data point in a linear regression and observations with Cook’s distance &gt; 1 are candidates for being outliers. See in the plot above how the point with index 101 (our added outlier) has a very large Cook’s distance. Boundary regions for Cook’s distance equal to 0.5 (suspicious) and 1 (certainly influential) are painted with a dotted line. Finally, it’s very important that, before you remove a value because it may be an outlier, you understand where the data came from and if such an abnormal observation is possible. If it depicts an extraordinary but possible situation, this information can be very valuable and it’s wiser to keep it in the model. Interesting research questions arise when data doesn’t align with our preconceptions, so keep looking into it and potentially collect more data. 8.3 Exercises Performance assessment: Exercise for stepwise regression link, link 8.4 Solutions "],["supervised_ml_i.html", "Chapter 9 Supervised machine learning I 9.1 Learning objectives 9.2 Tutorial 9.3 Exercises 9.4 Solutions", " Chapter 9 Supervised machine learning I Chapter lead author: Benjamin Stocker Lecture (Beni): Overfitting, training, and cross-validation (link) K nearest neighbour models Data splitting Preprocessing, standardization, imputation, dimension reduction, as part of the model training workflow formula notation, recipes, generic train() Training and loss function Hyperparameters Resampling Performance assessment: Exercise comparing performance on test set of linear regression and KNN with different hyperparameter choices (like this), discuss link to overfitting example 9.1 Learning objectives In this Chapter, we use ecosystem flux data and parallel measurements of meteorological variables to model ecosystem gross primary production (the ecosystem-level CO2 uptake by photosynthesis). These data and prediction task is used to introduce fundamental methods of machine learning (data preprocessing, data splitting, model formulation, and model training) and their implementations in R. After this course, you will … Understand how overfitting models can happen and how it can be avoided. Implement a typical workflow using a machine learning model for a supervised regression problem. Evaluate the power of the model. Contents of this Chapter are inspired by the excellent book Hands-On Machine Learning in R by Boehmke &amp; Greenwell. 9.2 Tutorial 9.2.1 Required packages use_pkgs &lt;- c(&quot;dplyr&quot;, &quot;tidyr&quot;, &quot;readr&quot;, &quot;ggplot2&quot;, &quot;caret&quot;, &quot;yardstick&quot;, &quot;lubridate&quot;, &quot;rsample&quot;, &quot;recipes&quot;, &quot;modelr&quot;, &quot;forcats&quot;) new_pkgs &lt;- use_pkgs[!(use_pkgs %in% installed.packages()[, &quot;Package&quot;])] if (length(new_pkgs) &gt; 0) install.packages(new_pkgs) invisible(lapply(use_pkgs, require, character.only = TRUE)) 9.2.2 Overfitting This example is based on this example from scikit-learn. Machine learning (ML) may appear magical. The ability of ML algorithms to detect patterns and make predictions is fascinating. However, several challenges have to be met in the process of formulating, training, and evaluating the models. In this tutorial we will discuss some basics of supervised ML and how to achieve best predictive results. In general, the aim of supervised ML is to find a model \\(\\hat{y} = f(x)\\) that is trained (calibrated) using observed relationships between a set of features (also known as predictors, or labels, or independent variables) \\(x\\) and the target variable \\(y\\). Note, that \\(y\\) is observed. The hat on \\(\\hat{y}\\) denotes an estimate. Some algorithms can even handle predictions of multiple target variables simultaneously (e.g., neural networks). ML algorithms consist of (more or less) flexible mathematical models with a certain structure and set of parameters. At the simple extreme end of the model spectrum is the bivariate linear regression. You may not want to call this a ML algorithm because there is no iterative learning involved. Nevertheless, also bivariate linear regression provides a prediction \\(\\hat{y} = f(x)\\), just like other (proper) ML algorithms do. The functional form of a linear regression is not particularly flexible (just a straight line for the best fit between predictors and targets) and it has only two parameters (slope and intercept). At the other extreme end are, for example, deep neural networks. They are extremely flexible, can learn highly non-linear relationships and deal with interactions between a large number of predictors. They also contain very large numbers of parameters, typically on the order of \\(10^4 - 10^5\\) . You can imagine that this allows these types of algorithms to very effectively learn from the data, but also bears the risk of overfitting. What is overfitting? The following example illustrates it. Let’s assume that there is some true underlying relationship between a single predictor \\(x\\) and the target variable \\(y\\). We don’t know this relationship (in the code below, this is true_fun()) and the observations contain a (normally distributed) error (y = true_fun(x) + 0.1 * rnorm(n_samples)). Based on our training data (df_train), we fit three polynomial models of degree 1, 4, and 15 to the observations. A polynomial of degree \\(N\\) is given by: \\[ y = \\sum_{n=0}^N a_n x^n \\] \\(a_n\\) are the coefficients, i.e., model parameters. The goal of the training is to find the coefficients \\(a_n\\) so that the predicted \\(\\hat{y}\\) fits observed \\(y\\) best. From the above definition, the polynomial of degree 15 has 16 parameters, while the polynomial of degree 1 has two parameters (and corresponds to a simple bivariate linear regression). You can imagine that the polynomial of degree 15 is much more flexible and should thus yield the closest fit to the training data. This is indeed the case. We can use the same fitted models on data that was not used for model fitting - the validation data. This is what’s done below. Again, the same true underlying relationship is used, but we sample a new set of data points \\(x\\) and add a new sample of errors on top of the true relationship. You see that, using the validation set, we find that “poly4” actually performs the best - it has a much lower RMSE that “poly15”. Apparently, “poly15” was overfitted. Apparently, it indeed used its flexibility to fit not only the shape of the true underlying relationship, but also the observation errors on top of it. This has obviously the implication that, when this model is used to make predictions for data that was not used for training (calibration), it will yield misguided predictions that are affected by the errors in the training set. In the above pictures we can also conclude that “poly1” was underfitted. It gets even worse when applying the fitted polynomial models to data that extends beyond the range in \\(x\\) that was used for model training. Here, we’re extending just 20% to the right. You see that the RMSE for “poly15” literally explodes. The model is hopelessly overfitted and completely useless for prediction, although it looked like it fit the data best when we considered only the training results. This is a fundamental challenge in ML - finding the model with the best generalisability. That is, a model that not only fits the training data well, but also performs well on unseen data. The phenomenon of fitting and overfitting as a function of the model complexity is also referred to as the bias-variance trade-off. The bias describes how well a model matches the training set (average error). A model with low bias will match the data set closely and vice versa. The variance describes how much a model changes when you train it using different portions of your data set. “poly15” has a high variance, but much of its variance is the result of misled training on observation errors. On the other extreme, “poly1” has a high bias. It’s not affected by the noise in observations, but its predictions are also far off the observations. In ML, we are challenged to balance this trade-off. This chapter introduces the methods to achieve the best model generalisability and find the sweet spot between high bias and high variance. The steps to get there include the preprocessing of data, splitting the data into training and testing sets, and model training that “steers” the model towards what is considered a good model fit in terms of its generalisability to data that was not used during training. In this chapter, you learn how all these steps can be implemented in R. Depending on your application or research question, it may also be of interest to evaluate the relationships embodied in \\(f(x)\\) or to quantify the importance of different predictors in our model. This is referred to as model interpretation and is introduced in Chapter @ref(interpretable_ml). Of course, a plethora of algorithms exist that do the job of \\(y = f(x)\\). Each of them has its own strengths and limitations. It is beyond the scope of this course to introduce a larger number of ML algorithms. Subsequent Chapters will focus primarily on the Random Forest algorithm and Neural Networks (NN). For illustration purposes in this chapter, we will use and introduce the K-nearest-Neighbors (KNN) algorithm and compare its performance to a multivariate linear regression for illustration purposes. 9.2.3 Data and the modelling challenge We’re returning to ecosystem flux data that we’ve used Chapters @ref(data_wrangling) and @ref(data_vis). Here, we’re using daily data from the evergreen site in Davos, Switzerland (CH-Dav) to avoid effects of seasonally varying foliage cover for which the data does not contain information. To address such additional effects, we would have to, for example, combine the flux and meteorological data with remotely sensed surface greenness data. The data set FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv contains a time series of the ecosystem gross primary production (GPP) and a range of meteorological variables, measured in parallel. In this chapter, we formulate a model for predicting GPP from a set of covariates (other variables that vary in parallel, here the meteorological variables). This is to say that GPP_NT_VUT_REF is the target variable, and other variables that are available in our dataset are the predictors. Let’s read the data, select suitable variables, and interpret missing value codes, and select only good-quality data (where at least 80% of the underlying half-hourly data was good quality measured data, and not gap-filled). ddf &lt;- read_csv(&quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot;) |&gt; # select only the variables we are interested in dplyr::select(TIMESTAMP, GPP_NT_VUT_REF, # the target ends_with(&quot;_QC&quot;), # quality control info ends_with(&quot;_F&quot;), # includes all all meteorological covariates -contains(&quot;JSB&quot;) # weird useless variable ) |&gt; # convert to a nice date object dplyr::mutate(TIMESTAMP = ymd(TIMESTAMP)) |&gt; # set all -9999 to NA dplyr::na_if(-9999) |&gt; # retain only data based on &gt;=80% good-quality measurements # overwrite bad data with NA (not dropping rows) dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC &lt; 0.8, NA, GPP_NT_VUT_REF), TA_F = ifelse(TA_F_QC &lt; 0.8, NA, TA_F), SW_IN_F = ifelse(SW_IN_F_QC &lt; 0.8, NA, SW_IN_F), LW_IN_F = ifelse(LW_IN_F_QC &lt; 0.8, NA, LW_IN_F), VPD_F = ifelse(VPD_F_QC &lt; 0.8, NA, VPD_F), PA_F = ifelse(PA_F_QC &lt; 0.8, NA, PA_F), P_F = ifelse(P_F_QC &lt; 0.8, NA, P_F), WS_F = ifelse(WS_F_QC &lt; 0.8, NA, WS_F)) |&gt; # drop QC variables (no longer needed) dplyr::select(-ends_with(&quot;_QC&quot;)) The steps above are considered data wrangling and are not part of the modelling process. After completing this tutorial, you will understand this distinction. (-&gt; exercise explain the distinction) 9.2.4 K-nearest neighbours Before we start with the model training workflow, let’s introduce the K-nearest neighbour (KNN) algorithm which is used here for demonstration purposes. It serves the purpose of demonstrating the bias-variance trade-off (see below). As the name suggests, KNN uses the \\(k\\) observations that are “nearest” to the new record for which we want to make a prediction. It then calculates their average (for regression) or most frequent value (for classification) and uses it as the prediction of the target value. “Nearest” is determined by some distance metric evaluated based on the values of the predictors. In our example (GPP_NT_VUT_REF ~ .), KNN would determine the \\(k\\) days where conditions, given by our set of predictors, were most similar (nearest) to the day for which we seek a prediction. Then, it calculates the prediction as the average (mean) GPP value of these days. Determining “nearest” neighbors is commonly based on either the Euclidean or Manhattan distances between two data points \\(x_a\\) and \\(x_b\\), considering all \\(P\\) predictors \\(j\\). Euclidean distance: \\[ \\sqrt{ \\sum_{j=1}^P (x_{a,j} - x_{b,j})^2 } \\\\ \\] Manhattan distance: \\[ \\sum_{j=1}^P | x_{a,j} - x_{b,j} | \\] In two-dimensional space, the Euclidean distance measures the length of a straight line between two points (remember Pythagoras!). The Manhattan distance is called this way because it measures the distance you would have to walk to get from point \\(a\\) to point \\(b\\) in Manhattan, New York, where you cannot cut corners but have to follow a rectangular grid of streets. \\(|x|\\) is the absolute value of \\(x\\) ( \\(|-x| = x\\)). KNN is a simple algorithm that uses knowledge of the “local” data structure for prediction. A drawback is that the model training has to be done for each prediction step and the computation time of the training increases with \\(x \\times p\\). KNNs are used, for example, to impute values (fill missing values) and have the advantage that predicted values are always within the range of observed values of the target variable. 9.2.5 Model formulation The aim of supervised ML is to find a model \\(\\hat{y} = f(x)\\) so that \\(\\hat{y}\\) agrees well with observations \\(y\\). We typically start with a research question where \\(y\\) is given - naturally - by the problem we are addressing and we have a data set at hand where one or multiple predictors (or “features”) \\(x\\) are recorded along with \\(y\\). From our data, we have information about how GPP (ecosystem-level photosynthesis) depends on set of abiotic factors, mostly meteorological measurements. 9.2.5.1 Formula notation In R, it is common to use the formula notation to specify the target and predictor variables. You have encountered formulas before, e.g., for a linear regression using the lm() function. To specify a linear regression model for GPP_NT_VUT_REF with three predictors SW_F_IN, VPD_F, and TA_F, to be fitted to data ddf, we write: lm(GPP_NT_VUT_REF ~ SW_F_IN + VPD_F + TA_F, data = ddf) 9.2.5.2 The generic train() The way we formulate a model can be understood as being independent of the algorithm, or engine, that takes care of fitting \\(f(x)\\). The R package caret provides a unified interface for using different ML algorithms implemented in separate packages. In other words, it acts as a wrapper for multiple different model fitting, or ML algorithms. This has the advantage that it unifies the interface - the way arguments are provided and outputs are returned. caret also provides implementations for a set of commonly used tools for data processing, model training, and evaluation. We’ll use caret here for model training with the function train(). Note however, that using a specific algorithm, which is implemented in a specific package outside caret, also requires that the respective package be installed and loaded. Using caret for specifying the same linear regression model as above, the base-R lm() function, can be done with caret in a generalized form as: caret::train( form = GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = ddf |&gt; drop_na(), # drop missing values trControl = caret::trainControl(method = &quot;none&quot;), # no resampling method = &quot;lm&quot; ) ## Linear Regression ## ## 2729 samples ## 3 predictor ## ## No pre-processing ## Resampling: None Note the argument specified as trControl = trainControl(method = \"none\"). This suppresses the default approach to model fitting in caret - to resample using bootstrapping. More on that below. Note also that we dropped all rows that contained at least one missing value - necessary to apply the least squares method for the linear regression model fitting. It’s advisable to apply this data removal step only at the very last point of the data processing and modelling workflow. Alternative algorithms may be able to deal with missing values and we want to avoid losing information along the workflow. Of course, it is an overkill compared to write this as in the chunk above compared to just writing lm(...). But the advantage of the unified interface is that we can simply replace the method argument to use a different model fitting algorithm. For example, to use KNN, we just can write: caret::train( form = GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = ddf |&gt; drop_na(), trControl = caret::trainControl(method = &quot;none&quot;), method = &quot;knn&quot; ) ## k-Nearest Neighbors ## ## 2729 samples ## 3 predictor ## ## No pre-processing ## Resampling: None 9.2.6 Data splitting The introductory example demonstrated the importance of validating the fitted model with data that was not used for training. Thus, we can test the model’s generalisability to new (“unseen”) data. The essential step that enables us to assess the model’s generalization error is to hold out part of the data from training and set it aside (leaving it absolutely untouched!) for testing. There is no fixed rule for how much data are to be used for training and testing, respectively. We have to balance a trade-off: Spending too much data for training will leave us with too little data for testing and the test results may not be robust. In this case, the sample size for getting robust validation statistics is not sufficiently large and we don’t know for sure whether we are safe from an over-fit model. Spending too much data for validation will leave us with too little data for training. In this case, the ML algorithm may not be successful at finding real relationships due to insufficient amounts of training data. Typical splits are between 60-80% for training. However, in cases where the number of data points is very large, the gains from having more training data are marginal, but come at the cost of adding to the already high computational burden of model training. In environmental sciences, the number of predictors is often smaller than the sample size (\\(p &lt; n\\)), because it is typically easier to collect repeated observations of a particular variable than to expand the set of variables being observed. Nevertheless, in cases where the number \\(p\\) gets large, it is important, and for some algorithms mandatory, to maintain \\(p &lt; n\\) for model training. An important aspect to consider when splitting the data is to make sure that all “states” of the system for which we have data are well represented in training and testing sets. A particularly challenging case is posed when it is of particular interest that the algorithm learns relationships \\(f(x)\\) under rare conditions \\(x\\), for example meteorological extreme events. If not addressed with particular measures, model training tends to achieve good model performance for the most common conditions. A simple way to put more emphasis for model training on extreme conditions is to compensate by sampling overly proportional from such cases for the training data set. Several alternative functions for the data splitting step are available from different packages in R. We use the the rsample package here as it allows to additionally make sure that data from the full range of a given variable’s values (VPD_F in the example below) are well covered in both training and testing sets. set.seed(123) # for reproducibility split &lt;- rsample::initial_split(ddf, prop = 0.7, strata = &quot;VPD_F&quot;) ddf_train &lt;- rsample::training(split) ddf_test &lt;- rsample::testing(split) Plot the distribution of values in the training and testing sets. ddf_train |&gt; dplyr::mutate(split = &quot;train&quot;) |&gt; dplyr::bind_rows(ddf_test |&gt; dplyr::mutate(split = &quot;test&quot;)) |&gt; tidyr::pivot_longer(cols = 2:9, names_to = &quot;variable&quot;, values_to = &quot;value&quot;) |&gt; ggplot(aes(x = value, y = ..density.., color = split)) + geom_density() + facet_wrap(~variable, scales = &quot;free&quot;) 9.2.7 Pre-processing Data pre-processing is aimed at preparing the data for use in a specific model fitting procedure and at improving the effectiveness of model training. The splitting of the data into a training and test set makes sure that no information from the test set is used during or before model training. It is important that absolutely no information from the test set finds its way into the training set (data leakage). In a general sense, pre-processing involve data transformations where the transformation functions use parameters that are determined on the data itself. Consider, for example, the standardization. That is, the linear transformation of a vector of values to have zero mean (data is centered, \\(\\mu = 0\\)) and a standard deviation of 1 (data is scaled to \\(\\sigma = 1\\)). In order to avoid data leakage, the mean and standard deviation have to be determined on the training set only. Then, the normalization of the training and the test sets both use set of (\\(\\mu, \\sigma\\)) determined on the training set. Data leakage would occur if the (\\(\\mu, \\sigma\\)) would be determined on data containing values from the test set. Often, multiple splits of the data are considered during model training. Hence, an even larger number of data transformation parameters (\\(\\mu, \\sigma\\) in the example of normalization) have to be determined and transformations applied to the multiple splits of the data. caret deals with this for you and the transformations do not have to be “manually” applied before applying the train() function call. Instead, the data pre-processing is considered an integral step of model training and instructions are specified as part of the train() function call and along with the un-transformed data. The recipes package provides an even more powerful way for specifying the formula and pre-processing steps in one go. It is compatible with the train() function of {caret}. For the same formula as above, and an example where the data ddf_train is to be normalized (centered and scaled), we can specify a “recipe” using the pipe operator as: pp &lt;- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = ddf_train) |&gt; recipes::step_center(all_numeric(), -all_outcomes()) |&gt; recipes::step_scale(all_numeric(), -all_outcomes()) The first line with the recipe() function call assigns roles to the different variables. GPP_NT_VUT_REF is an outcome (in “{recipes} speak”). Then, we used selectors to apply the recipe step to several variables at once. The first selector, all_numeric(), selects all variables that are either integers or real values. The second selector, -all_outcomes() removes any outcome (target) variables from this recipe step. The returned object pp does not contain a normalized version of the data frame ddf_train, but rather the information that allows us to apply a specific set of pre-processing steps also to any data set. The object pp can then be supplied to train() as its first argument: caret::train( pp, data = ddf_train, method = &quot;knn&quot;, trControl = caret::trainControl(method = &quot;none&quot;) ) The example above showed data standardization as a data pre-processing step. Data pre-processing may be done with different aims, as described in sub-sections below. 9.2.7.1 Standardization Several algorithms explicitly require data to be standardized so that values of all predictors vary within a comparable range. The necessity of this step becomes obvious when considering KNN, where the magnitude of the distance is strongly influenced by the order of magnitude of the predictor values. To get a quick overview of the distribution of all variables (columns) in our data frame, we can use the {skimr} package. ddf |&gt; summarise(across(where(is.numeric), ~quantile(.x, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE))) |&gt; t() |&gt; as_tibble(rownames = &quot;variable&quot;) |&gt; setNames(c(&quot;variable&quot;, &quot;min&quot;, &quot;q25&quot;, &quot;q50&quot;, &quot;q75&quot;, &quot;max&quot;)) ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if ## `.name_repair` is omitted as of tibble 2.0.0. ## ℹ Using compatibility `.name_repair`. ## # A tibble: 8 × 6 ## variable min q25 q50 q75 max ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GPP_NT_VUT_REF -4.23 0.773 2.87 5.45 12.3 ## 2 TA_F -21.9 -1.47 3.51 8.72 20.7 ## 3 SW_IN_F 3.30 77.8 135. 214. 366. ## 4 LW_IN_F 138. 243. 279. 308. 365. ## 5 VPD_F 0.001 0.959 2.23 4.06 16.6 ## 6 PA_F 80.4 83.2 83.7 84.1 85.6 ## 7 P_F 0 0 0 1.6 92.1 ## 8 WS_F 0.405 1.56 1.93 2.34 6.54 We see for example, that typical values of LW_IN_F are by a factor 100 larger than values of VPD_F. A distance calculated based on these raw values would therefore be strongly dominated by the difference in LW_IN_F values, and differences in VPD_F would hardly affect the distance. Therefore, the data must be standardized before using it with the KNN algorithm (and other algorithms, including Neural Networks). Standardization is done to each variable separately, by centering and scaling each to have \\(\\mu = 0\\) and \\(\\sigma = 1\\). The steps for centering and scaling using the recipes package are described above. Standardization can be done not only by centering and scaling (as described above), but also by scaling to within range, where values are scaled such that the minimum value within each variable (column) is 0 and the maximum is 1. As seen above for the feature engineering example, the object pp does not contain a standardized version of the data frame ddf_train, but rather the information that allows us to apply the same standardization also to other data. In other words, recipe(..., data = ddf_train) |&gt; step_center(...) |&gt; step_scale(...) doesn’t actually transform ddf_train. There are two more steps involved to get there. This might seem bothersome at first but their separation is critical in the context of model training and data leakage, and translates the conception of the pre-processing as a “recipe” into the way we write the code. To actually transform the data, we first have to “prepare” the recipe: pp_prep &lt;- recipes::prep(pp, training = ddf_train) Finally we can actually transform the data. That is, “bake” the prepared recipe. ddf_baked &lt;- recipes::bake(pp_prep, new_data = ddf_train) The effect is of standardization is illustrated by comparing original and transformed variables: gg1 &lt;- ddf_train |&gt; dplyr::select(one_of(c(&quot;SW_IN_F&quot;, &quot;VPD_F&quot;, &quot;TA_F&quot;))) |&gt; tidyr::pivot_longer(cols = c(SW_IN_F, VPD_F, TA_F), names_to = &quot;var&quot;, values_to = &quot;val&quot;) |&gt; ggplot(aes(val, ..density..)) + geom_density() + facet_wrap(~var) gg2 &lt;- ddf_baked |&gt; dplyr::select(one_of(c(&quot;SW_IN_F&quot;, &quot;VPD_F&quot;, &quot;TA_F&quot;))) |&gt; tidyr::pivot_longer(cols = c(SW_IN_F, VPD_F, TA_F), names_to = &quot;var&quot;, values_to = &quot;val&quot;) |&gt; ggplot(aes(val, ..density..)) + geom_density() + facet_wrap(~var) cowplot::plot_grid(gg1, gg2, nrow = 2) ## Warning: Removed 545 rows containing non-finite values (`stat_density()`). ## Removed 545 rows containing non-finite values (`stat_density()`). 9.2.7.2 Handling missing data Several machine learning algorithms require missing values to be removed. That is, if any of the cells in one row has a missing value, the entire cell gets removed. This can lead to severe data loss. In cases where missing values appear predominantly in only a few variables, it may be advantageous to drop the affected variable from the data for modelling. In other cases, it may be advantageous to fill missing values (data imputation, see next section). Although such imputed data is “fake”, it may be preferred to impute values than to drop entire rows and thus get the benefit of being able to use the information contained in available (real) values of affected rows. Whether or not imputation is preferred should be determined based on the model skill for an an out-of-sample test (more on that later). Visualizing missing data is the essential first step in making decisions about dropping rows with missing data versus removing predictors from the model (which would imply too much data removal). visdat::vis_miss( ddf, cluster = FALSE, warn_large_data = FALSE ) Here, the variable LW_IN_F (longwave radiation) if affected by a lot of missing data. Note that we applied a data cleaning step along with the data read-in at the very top of this Chapter. There, we applied a filtering criterion where values are only retained if at least 80% of the underlying half-hourly data is actual measured (and not gap-filled) data. Whether to drop the variable for further modelling should be informed also by our understanding of the data and the processes relevant for the modelling task. Here, the modelling target is GPP and the carbon cycle specialists among the readers may know that longwave radiation is not a known important control on GPP (ecosystem photosynthesis). Therefore, we may consider dropping this variable from the dataset for our modelling task. The remaining variables are affected by less frequent missingness with which we will deal otherwise. 9.2.7.3 Imputation Imputation refers to the replacement of missing values with with a “best guess” value (Boehmke &amp; Greenwell). Different approaches exist for determining that best guess. The most basic approach is to impute missing values with the mean or median of the available values of the same variable, which can be implemented using step_impute_*() from the {recipes} package. For example, to impute the median for all predictors separately: pp |&gt; step_impute_median(all_predictors()) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 3 ## ## Operations: ## ## Centering for all_numeric(), -all_outcomes() ## Scaling for all_numeric(), -all_outcomes() ## Median imputation for all_predictors() Imputing by the mean or median is “uninformative”. We may use information about the co-variation of multiple variables for imputing missing values. For example, for imputing missing VPD values, we may consider the fact that VPD tends to be high when air temperature is high. Therefore, missing VPD values can be modeled as a function of other co-variates (predictors). Several approaches to modelling missing values are available through the recipes package (see here). For example, we can use KNN with five neighbors as: pp |&gt; step_impute_knn(all_predictors(), neighbors = 5) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 3 ## ## Operations: ## ## Centering for all_numeric(), -all_outcomes() ## Scaling for all_numeric(), -all_outcomes() ## K-nearest neighbor imputation for all_predictors() 9.2.7.4 One-hot encoding For ML algorithms that require that all predictors be numerical (e.g., neural networks, or KNN), categorical predictors have to be pre-processed and converted into new numerical predictors. The most common such transformation is one-hot encoding, where a categorical predictor variable that has \\(N\\) levels is replaced by \\(N\\) new variables that contain either zeros or ones depending whether the value of the categorical feature corresponds to the respective column. Because this creates perfect collinearity between these new column, we can also drop one of them. This is referred to as dummy encoding. # original data frame df &lt;- tibble(id = 1:4, color = c(&quot;red&quot;, &quot;red&quot;, &quot;green&quot;, &quot;blue&quot;)) df ## # A tibble: 4 × 2 ## id color ## &lt;int&gt; &lt;chr&gt; ## 1 1 red ## 2 2 red ## 3 3 green ## 4 4 blue # after one-hot encoding dmy &lt;- dummyVars(&quot;~ .&quot;, data = df, sep = &quot;_&quot;) data.frame(predict(dmy, newdata = df)) ## id colorblue colorgreen colorred ## 1 1 0 0 1 ## 2 2 0 0 1 ## 3 3 0 1 0 ## 4 4 1 0 0 Note that in a case where color is strictly one of c(\"red\", \"red\", \"green\", \"blue\") (and not, for example, \"yellow\"), then one of the columns added by dummyVars() is obsolete (if it’s neither \"red\", nor \"green\", it must be \"blue\") - columns are collinear. This can be avoided by setting fullRank = FALSE. Using the recipes package, one-hot encoding is implemented by: recipe(GPP_NT_VUT_REF ~ ., data = ddf) |&gt; step_dummy(all_nominal(), one_hot = TRUE) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 8 ## ## Operations: ## ## Dummy variables from all_nominal() 9.2.7.5 Zero-variance predictors Sometimes, the data generation process yields variables that have the same value in each observation. And sometimes this is due to failure of the measurement device or some other bug in the data collection pipeline. Either way, this may cause some algorithms to crash or become unstable. Such “zero-variance” predictors are usually removed altogether. The same applies also to variables with “near-zero variance”. That is, variables where only a few unique values occur with a high frequency in the entire data set. The danger is that, when data is split into training and testing sets, the variable may effectively become a “zero-variance” variable within the training subset. We can test for zero-variance or near-zero variance predictors by quantifying the following metrics: Frequency ratio: Ratio of the frequency of the most common predictor over the second most common predictor. This should be near 1 for well-behaved predictors and get very large for problematic ones. Percent unique values: The number of unique values divided by the total number of rows in the data set (times 100). For problematic variables, this ratio gets small (approaches 1/100). The function nearZeroVar of the {caret} package flags suspicious variables (zeroVar = TRUE or nzv = TRUE). In our data set, we don’t find any: caret::nearZeroVar(ddf, saveMetrics = TRUE) ## freqRatio percentUnique zeroVar nzv ## TIMESTAMP 1.000000 100.000000 FALSE FALSE ## GPP_NT_VUT_REF 1.000000 93.732887 FALSE FALSE ## TA_F 1.000000 83.951932 FALSE FALSE ## SW_IN_F 1.500000 95.375723 FALSE FALSE ## LW_IN_F 1.000000 43.170064 FALSE FALSE ## VPD_F 1.142857 60.450259 FALSE FALSE ## PA_F 1.090909 37.906906 FALSE FALSE ## P_F 10.268072 5.978096 FALSE FALSE ## WS_F 1.083333 34.758138 FALSE FALSE Using the recipes package, we can add a step that removes zero-variance predictors by: pp |&gt; step_zv(all_predictors()) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 3 ## ## Operations: ## ## Centering for all_numeric(), -all_outcomes() ## Scaling for all_numeric(), -all_outcomes() ## Zero variance filter on all_predictors() 9.2.7.6 Target engineering Target engineering refers to pre-processing of the target variable. Its application can enable improved predictions, particularly for models that make assumptions about prediction errors (e.g., normally distributed errors in linear regression) or when the target variable follows a “special” distribution (e.g., heavily skewed distribution, or where the target variable is a fraction that is naturally bounded by 0 and 1). A simple log-transformation of the target variable can often resolve issues with skewed distributions. An implication of a log-transformation is that errors in predicting values in the upper end of the observed range are “discounted” in their weight compared to errors in the lower range. In our data set, the variable WS_F (wind speed) is skewed. The target variable that we have considered so far (GPP_NT_VUT_REF) is not skewed. In a case where we would consider WS_F to be our target variable, we would thus consider applying a log-transformation. gg1 &lt;- ddf |&gt; ggplot(aes(x = WS_F, y = ..density..)) + geom_histogram() + labs(title = &quot;Original&quot;) gg2 &lt;- ddf |&gt; ggplot(aes(x = log(WS_F), y = ..density..)) + geom_histogram() + labs(title = &quot;Log-transformed&quot;) cowplot::plot_grid(gg1, gg2) Log transformation as part of the pre-processing is specified using the step_log() function, here applied to the model target variable (all_outcomes()). recipes::recipe(WS_F ~ ., data = ddf) |&gt; # it&#39;s of course non-sense to model wind speed like this recipes::step_log(all_outcomes()) A log-transformation doesn’t necessarily result in a perfect normal distribution of transformed values. The Box-Cox can get us closer. It can be considered a generalization of the log-transformation. Values are transformed according to the following function: \\[ y(\\lambda) = \\begin{cases} \\frac{Y^\\lambda-1}{\\lambda}, &amp;\\; y \\neq 0\\\\ \\log(Y), &amp;\\; y = 0 \\end{cases} \\] \\(\\lambda\\) is treated as a parameter that is fitted such that the resulting distribution of values \\(y\\) approaches the normal distribution. To specify a Box-Cox-transformation as part of the pre-processing, we can use step_BoxCox() from the {recipes} package. pp &lt;- recipe(WS_F ~ ., data = ddf_train) |&gt; step_BoxCox(all_outcomes()) How do transformed values look like? prep_pp &lt;- prep(pp, training = ddf_train |&gt; drop_na()) ddf_baked &lt;- bake(prep_pp, new_data = ddf_test |&gt; drop_na()) ddf_baked |&gt; ggplot(aes(x = WS_F, y = ..density..)) + geom_histogram() + labs(title = &quot;Box-Cox-transformed&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Note that the Box-Cox-transformation can only be applied to values that are strictly positive. In our example, wind speed (WS_F) is. If this is not satisfied, a Yeo-Johnson transformation can be applied. recipe(WS_F ~ ., data = ddf) |&gt; step_YeoJohnson(all_outcomes()) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 8 ## ## Operations: ## ## Yeo-Johnson transformation on all_outcomes() 9.2.8 Putting it all together (half-way) Let’s recap. We have a dataset ddf and we want to predict ecosystem GPP (GPP_NT_VUT_REF) from a set of predictors - environmental covariates that were measured in parallel to GPP. Let’s compare the performance of a multivariate linear regression and KNN model in terms of its generalisation to data that was not used for model fitting. The following pieces are implemented: Missing data: We’ve seen that the predictor LW_IN_F has lots of missing values and - given a priori knowledge is not critical for predicting GPP and we’ll drop it. Data cleaning: Data (ddf) was cleaned based on quality control information upon reading the data at the beginning of this Chapter. Before modelling, we’re checking the distribution of the target value here again to make sure it is “well-behaved”. Imputation: We drop rows with missing data for model training, instead of imputing them. Some of the predictors are distintively not normally distributed. Let’s Box-Cox transform all predictors as a pre-processing step. We have to standardize the data in order to use it for KNN. We have no variable, where zero-variance was detected and we have no categorical variables that have to be transformed by one-hot encoding to be used in KNN. We use a data split, whithholding 30% for testing. Take \\(k=10\\) for the KNN model. Other choices are possible and will affect the prediction error on the training and the testing data in different manners. We’ll learn more about the optimal choice of \\(k\\) (hyperparameter tuning) in the next chapter. Fit models to minimize the root mean square error (RMSE) between predictions and observations. More on the choice of the metric argument in train() (loss function) in the next chapter. For the KNN model, use \\(k=5\\). These steps are implemented by the code below. # Data cleaning: looks ok, no obviously bad data # no long tail, therefore no further target engineering ddf |&gt; ggplot(aes(x = GPP_NT_VUT_REF, y = ..count..)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 395 rows containing non-finite values (`stat_bin()`). # Data splitting set.seed(1982) # for reproducibility split &lt;- rsample::initial_split(ddf, prop = 0.7, strata = &quot;VPD_F&quot;) ddf_train &lt;- rsample::training(split) ddf_test &lt;- rsample::testing(split) # Model and pre-processing formulation, use all variables but LW_IN_F pp &lt;- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F + PA_F + P_F + WS_F, data = ddf_train |&gt; drop_na()) |&gt; recipes::step_BoxCox(all_predictors()) |&gt; recipes::step_center(all_numeric(), -all_outcomes()) |&gt; recipes::step_scale(all_numeric(), -all_outcomes()) # Fit linear regression model mod_lm &lt;- caret::train( pp, data = ddf_train |&gt; drop_na(), method = &quot;lm&quot;, trControl = caret::trainControl(method = &quot;none&quot;), metric = &quot;RMSE&quot; ) ## Warning: Non-positive values in selected variable. ## Warning: Non-positive values in selected variable. ## Warning: No Box-Cox transformation could be estimated for: `TA_F`, `PA_F`, `P_F` ## Warning: Non-positive values in selected variable. ## Non-positive values in selected variable. ## Warning: No Box-Cox transformation could be estimated for: `TA_F`, `PA_F`, `P_F` # Fit KNN model mod_knn &lt;- caret::train( pp, data = ddf_train |&gt; drop_na(), method = &quot;knn&quot;, trControl = caret::trainControl(method = &quot;none&quot;), tuneGrid = data.frame(k = 5), metric = &quot;RMSE&quot; ) ## Warning: Non-positive values in selected variable. ## Warning: Non-positive values in selected variable. ## Warning: No Box-Cox transformation could be estimated for: `TA_F`, `PA_F`, `P_F` ## Warning: Non-positive values in selected variable. ## Non-positive values in selected variable. ## Warning: No Box-Cox transformation could be estimated for: `TA_F`, `PA_F`, `P_F` We can use the model objects mod_lm and mod_knn to add the fitted values to the training data and add the predicted values to the test data, both using the generic function predict(..., newdata = ...). The code below implements the prediction step, the measuring of the prediction skill, and the visualisation of predicted versus observed values on the test and training sets, bundled into the function eval_model(), which we will re-use for each fitted model object. ## make model evaluation into a function to reuse code eval_model &lt;- function(mod, df_train, df_test){ ## add predictions to the data frames df_train &lt;- df_train %&gt;% drop_na() %&gt;% mutate(fitted = predict(mod, newdata = .)) df_test &lt;- df_test %&gt;% drop_na() %&gt;% mutate(fitted = predict(mod, newdata = .)) ## get metrics tables metrics_train &lt;- df_train %&gt;% yardstick::metrics(GPP_NT_VUT_REF, fitted) metrics_test &lt;- df_test %&gt;% yardstick::metrics(GPP_NT_VUT_REF, fitted) ## extract values from metrics tables rmse_train &lt;- metrics_train %&gt;% filter(.metric == &quot;rmse&quot;) %&gt;% pull(.estimate) rsq_train &lt;- metrics_train %&gt;% filter(.metric == &quot;rsq&quot;) %&gt;% pull(.estimate) rmse_test &lt;- metrics_test %&gt;% filter(.metric == &quot;rmse&quot;) %&gt;% pull(.estimate) rsq_test &lt;- metrics_test %&gt;% filter(.metric == &quot;rsq&quot;) %&gt;% pull(.estimate) ## visualise with a hexagon binning and a customised color scale, ## adding information of metrics as sub-titles gg1 &lt;- df_train %&gt;% ggplot(aes(GPP_NT_VUT_REF, fitted)) + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;) + geom_abline(slope = 1, intercept = 0, linetype = &quot;dotted&quot;) + labs(subtitle = bquote( italic(R)^2 == .(format(rsq_train, digits = 2)) ~~ RMSE == .(format(rmse_train, digits = 3))), title = &quot;Training set&quot;) + theme_classic() gg2 &lt;- df_test %&gt;% ggplot(aes(GPP_NT_VUT_REF, fitted)) + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;) + geom_abline(slope = 1, intercept = 0, linetype = &quot;dotted&quot;) + labs(subtitle = bquote( italic(R)^2 == .(format(rsq_test, digits = 2)) ~~ RMSE == .(format(rmse_test, digits = 3))), title = &quot;Test set&quot;) + theme_classic() out &lt;- cowplot::plot_grid(gg1, gg2) return(out) } # linear regression model eval_model(mod = mod_lm, df_train = ddf_train, df_test = ddf_test) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; # KNN eval_model(mod = mod_knn, df_train = ddf_train, df_test = ddf_test) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; It is advisable to keep workflow notebooks (this RMarkdown file) light and legible. Therefore, code chunks should not be excessively long and functions should be kept in a ./R/*.R file, which can be loaded. This also facilitates debugging code inside the function. Here, the function eval_model() is part of the book’s git repository, stored in the sub-directory ./R/, and used also in later chapters. 9.3 Exercises 9.4 Solutions "],["supervised_ml_ii.html", "Chapter 10 Supervised machine learning II 10.1 Learning objectives 10.2 Tutorial 10.3 Exercises 10.4 Solutions", " Chapter 10 Supervised machine learning II Chapter lead author: Benjamin Stocker Contents: Lecture (Beni): Wisdom of the crowds, from decision trees to random forests Performance assessment: Competition for best-performing model, given training-testing split of data; others should be able to reproduce performance 10.1 Learning objectives 10.2 Tutorial 10.2.1 Model training Model training in supervised machine learning is guided by the match (or mismatch) between the predicted and observed target variable(s), that is, between \\(\\hat{y}\\) and \\(y\\). The loss function quantifies this mismatch (\\(L(\\hat{y}, y)\\)), and the algorithm takes care of progressively reducing the loss during model training. Let’s say the machine learning model contains two parameters and predictions can be considered a function of the two (\\(\\hat{y}(w_1, w_2)\\)). \\(y\\) is actually constant. Thus, the loss function is effectively a function \\(L(w_1, w_2)\\). Therefore, we can consider the model training as a search of the parameter space of the machine learning model \\((w_1, w_2)\\) to find the minimum of the loss. Common loss functions are the root mean square error (RMSE), or the mean square error (MSE), or the mean absolute error (MAE). Loss minimization is a general feature of ML model training. Figure 10.1: Visualization of a loss function as a plane spanned by the two parameters \\(w_1\\) and \\(w_2\\). Model training is implemented in R for different algorithms in different packages. Some algorithms are even implemented by multiple packages (e.g., nnet and neuralnet for artificial neural networks). As described in Chapter 9.2.7, the caret package provides “wrappers” that handle a large selection of different ML model implementations in different packages with a unified interface (see here for an overview of available models). The caret function train() is the centre piece. Its argument metric specifies the loss function and defaults to the RMSE for regression models and accuracy for classification (see sub-section on metrics below). 10.2.1.1 Hyperparameter tuning Practically all ML algorithms have some “knobs” to turn in order to achieve efficient model training and predictive performance. Such “knobs” are the hyperparameters. What these knobs are, depends on the ML algorithm. For KNN, this is k - the number of neighbours to consider for determining distances. There is always an optimum \\(k\\). Obviously, if \\(k = n\\), we consider all observations as neighbours and each prediction is simply the mean of all observed target values \\(Y\\), irrespective of the predictor values. This cannot be optimal and such a model is likely underfit. On the other extreme, with \\(k = 1\\), the model will be strongly affected by the noise in the single nearest neighbour and its generalisability will suffer. This should be reflected in a poor performance on the validation data. For random forests from the ranger package, hyperparameters are: mtry: the number of variables to consider to make decisions, often taken as \\(p/3\\), where \\(p\\) is the number of predictors. min.node.size: the number of data points at the “bottom” of each decision tree splitrule: the function applied to data in each branch of a tree, used for determining the goodness of a decision Hyperparameters usually have to be “tuned”. The optimal setting depends on the data and can therefore not be known a priori. In caret, hyperparameter tuning is implemented as part of the train() function. Values of hyperparameters to consider are to be specified by the argument tuneGrid, which takes a data frame with column(s) named according to the name(s) of the hyperparameter(s) and rows for each combination of hyperparameters to consider. ## do not run train( form = GPP_NT_VUT_REF ~ SW_F_IN + VPD_F + TA_F, data = ddf, method = &quot;ranger&quot;, tuneGrid = expand.grid( .mtry = floor(6 / 3), .min.node.size = c(3, 5, 9,15, 30), .splitrule = c(&quot;variance&quot;, &quot;maxstat&quot;)), ... ) Here, expand.grid() is used to provide a data frame with all combinations of values provided by individual vectors. 10.2.1.2 Resampling The goal of model training is to achieve the best possible model generalisability. That is, the best possible model performance when predicting to data that was not used for training - the test data. Resampling mimicks the comparison of predictions to the test data. Instead of using all training data, the training data is resampled into a number further splits into pairs of training and validation data. Model training is then guided by minimising the average loss determined on each resample of the validation data. Having multiple resamples (multiple folds of training-validation splits) avoids the loss minimization from being misguided by random peculiarities in the training and/or validation data. A common resampling method is k-fold cross validation, where the training data is split into k equally sized subsets (folds). Then, there will be k iterations, where each fold is used for validation once (while the remaining folds are used for training). An extreme case is leave-one-out cross validation, where k corresponds to the number of data points. To do a k-fold cross validation during model training in R, we don’t have to implement the loops around folds ourselves. The resampling procedure can be specified in the caret function train() with the argument trControl. The object that this argument takes is the output of a function call to trainControl(). This can be implemented in two steps. For example, to do a 10-fold cross-validation, we can write: ## do not run train( pp, data = ddf_train, method = &quot;ranger&quot;, tuneGrid = expand.grid( .mtry = floor(6 / 3), .min.node.size = c(3, 5, 9,15, 30), .splitrule = c(&quot;variance&quot;, &quot;maxstat&quot;)), trControl = trainControl(method = &quot;cv&quot;, number = 10), ... ) In certain cases, data points stem from different “groups”, and generalisability across groups is critical. In such cases, data from a given group must not be used both in the training and validation sets. Instead, splits should be made along group delineations. The caret function groupKFold() offers the solution for this case. 10.3 Exercises 10.4 Solutions "],["neural_nets.html", "Chapter 11 Neural networks 11.1 Learning objectives 11.2 Tutorial 11.3 Exercises 11.4 Solutions", " Chapter 11 Neural networks Chapter lead author: Benjamin Stocker Contents: Lecture (Beni): General introduction Performance assessment: Competition for best-performing model, given training-testing split of data; others should be able to reproduce performance 11.1 Learning objectives 11.2 Tutorial 11.3 Exercises 11.4 Solutions "],["interpretable_ml.html", "Chapter 12 Interpretable machine learning 12.1 Learning objectives 12.2 Tutorial 12.3 Exercises 12.4 Solutions", " Chapter 12 Interpretable machine learning Chapter lead author: Benjamin Stocker Contents: Variable importance Partial dependency Performance assessment: Compare partial dependency to a given predictor, detected with RF and with NN. 12.1 Learning objectives 12.2 Tutorial 12.3 Exercises 12.4 Solutions "],["references-1.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
