[["index.html", "Applied Geodata Science Overview About this book About this course System information and package list", " Applied Geodata Science Benjamin Stocker (lead), Koen Hufkens (contributing), Pepa Arán (contributing), Pascal Schneider (contributing) 2023-02-17 Overview About this book This book serves as the basis for the series of courses in Applied Geodata Science, taught at the Institute of Geography, University of Bern. The starting point of this book were the tutorials edited by Benjamin Stocker, Loïc Pellissier, and Joshua Payne for the course Environmental Systems Data Science (D-USYS, ETH Zürich). The present book was written as a collaborative effort led by Benjamin Stocker, with contributions by Pepa Arán and Koen Hufkens, and exercises by Pascal Schneider. The target of this book are people interested in applying data science methods for research. Methods, example data sets, and prediction challenges are chosen to make the book most relatable to scientists and students in Geography and Environmental Sciences. No prior knowledge of coding is required. Respective essentials are briefly introduced as primers. The focus of this book is not on the theoretical basis of the methods. Other “classical” statistics courses serve this purpose. Instead, this book introduces essential concepts, methods, and tools for applied data science in Geography and Environmental Sciences with an emphasis on covering a wide breadth. It is written with a hands-on approach using the R programming language and should enable an intuitive understanding of concepts with only a minimal reliance on mathematical language. Worked examples are provided for typical steps of data science applications in Geography and Environmental Sciences. The aim of this book is to teach the diverse set of skills needed as a basis for data-intensive research in academia and outside. We also use this book as a reference and on-boarding resource for group members of Geocomputation and Earth Observation (GECO), at the Institute of Geography, University of Bern. Images and other materials used here were made available under non-restrictive licenses. Original sources are attributed. Content without attribution is our own and shared under the license below. If there are any errors or any content you find concerning with regard to licensing or other, please contact us. Any feedback, positive or negative, is welcome and can be posted here. This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. About this course This book contains the lecture notes and exercises for the following courses, offered for Geography and for Climate Sciences students at the University of Bern, Switzerland: Applied Geodata Science I This is a course for Bachelors students in their second or third year of studies in Geography. Chapters 1 to Chapter 11 Applied Geodata Science II This is a course for Master students in Geography or Climate Sciences. Chapters are in development. Course goal The overall goal of this set of courses is that students and other readers learn to tell a story with (environmental and geo-) data. Learning Objectives The overall learning objectives are: Design and communicate your research project as a reproducible workflow. Find, access, process, and visualise large environmental and geographic data. Write legible code and manage collaborative code and data-centered projects. Manage analysis code for long-term reproducibility. Identify, quantify, and interpret patterns in large environmental and geographic data. Devise suitable data visualisations. Determine suitable model formulations and implement effective model training. Describe the challenges of model fitting with large data. Implement and make use of Open Science practices and resources to support data science projects in Geography and Environmental Sciences. Course contents This course covers all steps along the data science workflow (see Fig. 0.1) and introduces methods and tools to learn the most from data, to effectively communicate insights, and to make your workflow reproducible. By following this course, you will be well equipped for joining the Open Science movement. Figure 0.1: The data science workflow and keywords of contents covered in Applied Geodata Science I. Figure adapted from: Wickham and Grolemund R for Data Science This chapter starts by providing the context for this course: Why Applied Geodata Science? Why now? Chapters 1 and 2 serve as primers to get readers with a diverse background and varying data science experience up to speed with the basics for programming in R, which we rely on in later chapters. Chapter 3 introduces efficient handling and cleaning of large tabular data with the R tidyverse “programming dialect”. The focus is on non-geospatial data. Closely related to transforming data and its multiple axes of variation is data visualisation, covered in Chapter 4. Chapters 5, 7, and 6 introduce essential tools for the daily work with diverse data, for collaborative code development, and for an Open Science practice. With Chapters 8, Chapter 9, Chapter 10, and Chapter 11, we will get into modelling and identifying patterns in the data. Chapters 1-11 serve as lecture notes for Applied Geodata Science I and as learning material for students and scientists in any data-intensive research domain. These chapters are not explicitly dealing with geospatial data and modelling. Modelling with geospatial and temporal data is the subject of the course Applied Geodata Science II and will be introduced with a focus on typical applications and modelling tasks in Geography and Environmental Sciences. Respective materials are not currently contained in this book but will be added here later. All tutorials use the R programming language. System information and package list This book was compiled with the following environment: sessioninfo::session_info() ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.2.2 (2022-10-31) ## os Ubuntu 22.04.1 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate C.UTF-8 ## ctype C.UTF-8 ## tz UTC ## date 2023-02-17 ## pandoc 2.19.2 @ /usr/bin/ (via rmarkdown) ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## ! package * version date (UTC) lib source ## P bookdown 0.31 2022-12-13 [?] RSPM (R 4.2.0) ## bslib 0.4.2 2022-12-16 [1] RSPM (R 4.2.0) ## cachem 1.0.6 2021-08-19 [1] RSPM (R 4.2.0) ## cli 3.6.0 2023-01-09 [1] RSPM (R 4.2.0) ## digest 0.6.31 2022-12-11 [1] RSPM (R 4.2.0) ## P evaluate 0.19 2022-12-13 [?] RSPM (R 4.2.0) ## fastmap 1.1.0 2021-01-25 [1] RSPM (R 4.2.0) ## glue 1.6.2 2022-02-24 [1] RSPM (R 4.2.0) ## highr 0.10 2022-12-22 [1] RSPM (R 4.2.0) ## htmltools 0.5.4 2022-12-07 [1] RSPM (R 4.2.0) ## jquerylib 0.1.4 2021-04-26 [1] RSPM (R 4.2.0) ## jsonlite 1.8.4 2022-12-06 [1] RSPM (R 4.2.0) ## P knitr 1.41 2022-11-18 [?] RSPM (R 4.2.0) ## lifecycle 1.0.3 2022-10-07 [1] RSPM (R 4.2.0) ## magrittr 2.0.3 2022-03-30 [1] RSPM (R 4.2.0) ## R6 2.5.1 2021-08-19 [1] RSPM (R 4.2.0) ## P renv 0.16.0 2022-09-29 [?] RSPM (R 4.2.0) ## rlang 1.0.6 2022-09-24 [1] RSPM (R 4.2.0) ## P rmarkdown 2.19 2022-12-15 [?] RSPM (R 4.2.0) ## rstudioapi 0.14 2022-08-22 [1] RSPM (R 4.2.0) ## P sass 0.4.4 2022-11-24 [?] RSPM (R 4.2.0) ## P sessioninfo 1.2.2 2021-12-06 [?] RSPM (R 4.2.0) ## stringi 1.7.12 2023-01-11 [1] RSPM (R 4.2.0) ## stringr 1.5.0 2022-12-02 [1] RSPM (R 4.2.0) ## P vctrs 0.5.1 2022-11-16 [?] RSPM (R 4.2.0) ## P xfun 0.36 2022-12-21 [?] RSPM (R 4.2.0) ## P yaml 2.3.6 2022-10-18 [?] RSPM (R 4.2.0) ## ## [1] /home/runner/work/agds/agds/renv/library/R-4.2/x86_64-pc-linux-gnu ## [2] /home/runner/work/agds/agds/renv/sandbox/R-4.2/x86_64-pc-linux-gnu/99f2a2ef ## ## P ── Loaded and on-disk path mismatch. ## ## ────────────────────────────────────────────────────────────────────────────── "],["introduction.html", "Introduction What is Applied Geodata Science? The data science workflow Why now? A new modelling paradigm Reading and link collection", " Introduction The sheer volume of data that is becoming available today bears a huge potential for answering long-standing questions in all fields of environmental and geo-sciences. This gives rise to a new set of tools that can be used and a new set of challenges when applying them. What is Applied Geodata Science? Data science is interdisciplinary by nature. It sits at the intersection between domain expertise, Statistics and Mathematics knowledge, and coding skills. Data science generates new insights for applications in different fields by combining these three realms (Fig. 0.2). Combining only two of the three realms falls short of what data science is (Conway, 2013). Figure 0.2: The Venn diagram of data science. Adapted from Conway, 2013. Dealing with data requires coding (but not a degree in computer science). Coding skills are essential for file and data manipulation and for thinking algorithmically. Basic knowledge in Statistics and Mathematics are needed for extracting insights from data and for applying appropriate statistical methods. An overview of methods, a general familiarity, and an intuitive understanding of the basics are more important for most data science projects than having a PhD in Statistics. Statistics plus data yields machine learning, but not “data science”. In data science, questions and hypotheses are motivated by the scientific endeavor in different domains or by applications in the public or private sectors. To emphasize the distinctively applied and domain-oriented approach to data science of this course, we call it Applied Geodata Science. Of course, empirical research has always relied on data. The essential ingredient of a course in (Applied Geo-) data science is that it emphasizes the methodological aspects that are unique and critical for data-intensive research in Geography and Environmental Sciences, and for putting Open Science into practice. This course is also supposed to teach you how to stay out of the “danger zone” - where data is handled and models are fitted with a blind eye to fundamental assumptions and relations. The aim of data science projects is to yield credible (“trustworthy”) and robust results. The data science workflow The red thread of this course is the data science workflow (Fig. 0.3). Applied (geo-) data science projects typically start with research questions and hypotheses, and some data at hand, and (ideally) end with an answer to the research questions and the communication of results in textual, visual, and reproducible forms. What lies in between is not a linear process, but a cycle. One has to “understand” the data in order to identify appropriate analyses for answering the research questions. Before we’ve visualized the data, we don’t know how to transform it. And before we’ve modeled it, we don’t know the most appropriate visualization. In practice, we approach answers to our research questions gradually, through repeated cycles of exploratory data analysis - repeated cycles of transforming the data, visualizing it, and modelling relationships. More often than not, the exploratory data analysis generates insights about missing pieces in the data puzzle that we’re trying to solve. In such cases, the data collection and modelling task may have to be re-defined (dashed line in Fig. 0.3), and the exploratory data analysis cycle re-initiated. Figure 0.3: The data science workflow. Figure adapted from: Wickham and Grolemund R for Data Science As we work our way through repeated cycles of exploratory data analysis, we take decisions based on our data analysis, modelling, and visualizations. And we write code. The final conclusions we draw, the answers to research questions we find, and the results we communicate rest on the combination of all steps of our data processing, analysis, and visualization. Simply put, it rests on the reproducibility (and legibility) of our code (encapsulated by ‘Program’ in Fig. 0.3). Why now? Three general developments set the stage for this course. First, Geography and Environmental Sciences (as many other realms of today’s world) have entered a data-rich era (Chapters 5). Second, machine learning algorithms have revolutionized the way we can extract information from large volumes of data (this Chapter and Chapters 10 - 11). Third, Open Science principles (Chapter 6) - essential for inclusive research, boundless progress, and for diffusing science to society - are becoming a prerequisite for getting research funded and published. The skill set required to make use of the potentials of a data-rich world is diverse and is often not taught as part of the curriculum in the natural sciences (as of year 2023). This course fills this open space. A new modelling paradigm What is ‘modelling’? Models are an essential part of the scientific endeavor. They are used for describing the world, explaining observed phenomena, and for making predictions that can be tested with data. Models are thus a device for translating hypotheses of how the world operates into a form that can be confronted with how the world is observed. Models can be more or less explicit and more or less quantitative. Models can come in the form of vague mental notions that underpin our view of the world and our interpretation of observations. Towards the more specific end of this spectrum, models can be visualizations. For example a visualization of how elements in a system are connected. At the arguably most explicit and quantitative end of the spectrum are models that rely on mathematical descriptions of how elements of a system are connected and how processes operate. Examples of such models include General Circulation Models of the climate system or models used for Numerical Weather Prediction. Such models are often referred to as mechanistic models. A further distinction within mechanistic models can be made between dynamic models that describe a temporal evolution of a system (e.g., the dynamics of the atmosphere and the ocean in a General Circulation Model) and “static” models (e.g., a model for estimating the power generation of a solar photovoltaics station). In a dynamic model, we need to specify an initial state and the model (in many cases given additional inputs) predicts the evolution of the system from that. In a static model, the prediction can be described as a function of a set of inputs, without temporal dependencies between the inputs and the model prediction. Often, mechanistic and empirical models (or, here used as synonym, statistical models) are distinguished. Empirical models can be viewed as somewhere closer towards the less explicit end of the spectrum described above. In mechanistic models, the mathematical descriptions of relationships are informed by theory or by independently determined relationships (e.g., laboratory measurements of metabolic rates of an enzyme). In contrast, empirical models rely on no, or only a very limited amount of a priori knowledge that is built into the model formulation. However, it should be noted that mechanistic models often also rely on empirical or statistical descriptions for individual components (e.g., the parametrisation of convection in a climate model), and statistical models may, in some cases, also be viewed as a representation of mechanisms that reflects our theoretical understanding. For example, depending on whether a relationship between two variables is linear or saturating by nature, we would chose a different structure of an empirical model. An specific example is the light use efficiency model (Monteith, 1972) that linearly relates vegetation productivity to the amount of absorbed solar radiation. It simply has the form of a bivariate linear regression model. Vice-versa, traditional statistical models also rely on assumptions regarding the data generating process(es) and the resulting distribution of the data. Supervised machine learning models can be regarded as empirical models that are even more “assumption free” than traditional statistical models. In contrast to mechanistic models where rules and hypotheses are explicitly and mathematically encoded, and in contrast to statistical models where assumptions of the data distribution are made for specifying the model, machine learning approaches modelling from the flip side: from the data to the insight (Breiman, 2001). Rules are not encoded by a human, but discovered by the machine. Machine learning models learn from patterns in the data for making new predictions, rather than relying on theory and a priori knowledge of the system. In that sense, machine learning follows a new modelling paradigm. The learning aspect in machine learning refers to the automatic search process and the guidance of the model fitting by some feedback signal (loss function) that are employed in machine learning algorithms (see also Chapter 10). The aspect of “patterns in the data” is key here. Often, these patterns are fuzzy. Rule-based algorithms have a limited capacity for dealing with such problems. Symbolic artificial intelligence is based on rules and underlies, for example, a computer playing chess (Chollet &amp; Allaire, 2018). However, where rules cannot be encoded from the outset, symbolic artificial intelligence has reached its limits. A breakthrough in learning from fuzzy patterns in the data has been enabled by deep learning. Through multiple layers of abstraction of the data, deep learning models identify underlying, abstract, relationships and use them for prediction. Deep learning has been extremely successful in solving problems, e.g., in image classification, speech recognition, or language translation. However, the abstraction comes at the cost of interpretability. Deep learning models and machine learning models in general are used with an emphasis on prediction and have seen particularly wide adoption in fields where a false prediction has acceptable consequences (An inappropriate book recommendation based on your previous purchases is not grave.) (Knüsel et al., 2019). The model itself remains a black box and its utility for hypothesis testing is limited. This challenge has spurred the field of interpretable machine learning, where solutions are sought for uncovering the black box and probe the model for its trustworthiness. Chapters 8-11 lead into the world of machine learning and introduce the essential steps of the modelling workflow without delving into deep learning. Together with its preceeding chapters, this completes the toolbox required for making the first data scientific steps for applications in Geography and Environmental Sciences. This may be only just the beginning… Reading and link collection Foundations Leo Breiman: “Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author).” Statist. Sci. 16 (3) 199 - 231, August 2001. https://doi.org/10.1214/ss/1009213726 A paper describing the paradigm shift in statistical modelling - from traditional approaches to machine learning. Written in accessible language by the inventor of the Random Forest algorithm. Data wrangling Hadley Wickham and Garrett Grolemund: “R for Data Science”, https://r4ds.had.co.nz/ A comprehensive resource for data analysis (and visualisation) using R tidyverse. Covers contents of Chapters 1, 2, and 3 - but in more depth. Data visualisation Claus O. Wilke: “Fundamentals of Data Visualization”, https://clauswilke.com/dataviz/ A comprehensive resource for data visualisation - not specific to any programming language, but specific about the grammar of graphics. Covers concepts of Chapter 4 - but in more depth. The go-to resource for the implementation of data visualisation using the {ggplot2} R library is Wickham and Grolemund (see above). Machine learning Bradley Boehmke and Brandon Greenwell: “Hands-On Machine Learning with R”, https://bradleyboehmke.github.io/HOML/ A great entry point for machine learning in R. It demonstrates concepts and a range of algorithms of varying complexity - from linear regression to Random Forest - with examples in R. This book served as an inspiration and starting point for the (in some ways reduced) contents of this course, covered in Chapters 8-11. Chollet &amp; Allaire “Deep learning with R”, Manning Publications, Accessed February 17, 2023. https://www.manning.com/books/deep-learning-with-r. This is the next step after you’ve studied Applied Geodata Science I. It introduces machine learning with deep neural networks using the {keras} machine learning library (with its wrapper in R). "],["gettingstarted.html", "Chapter 1 Getting started 1.1 Learning objectives 1.2 Tutorial 1.3 Exercises", " Chapter 1 Getting started Chapter lead author: Pepa Arán 1.1 Learning objectives After you’ve gone over the lecture and solved the exercises, you should be able to: Work with R and RStudio on your computer Know some R objects and basic classes Follow basic good coding practices Organize your workspace using R projects Save your code and progress in an organized way 1.2 Tutorial 1.2.1 Working with R and RStudio R is a free, open-source programming language and software environment for statistical computing and graphics. It is widely used, not only among statisticians and data miners for developing statistical software, but also by scientist in various domains for data analysis, visualisation, and modelling. RStudio is an integrated development environment (IDE) that provides a user-friendly “center stage” for your work in R (and Python, see here). 1.2.1.1 Installing R and RStudio To use R and RStudio, you will first need to download and install them on your computer. To install R, go to the CRAN website and download the latest version of R for your operating system. Once the download is complete, follow the on-screen installation instructions for your operating system to install R. To install RStudio, go to the RStudio website and download the latest version of RStudio for your operating system. Once the download is complete, follow the installation instructions for your operating system to install RStudio. 1.2.1.2 The RStudio interface RStudio provides a user-friendly interface for writing, running, and debugging R code. When you open RStudio, you will see the following: RStudio interface. The interface is divided into four main panels: The source editor is where you can write, edit, and save your R code. The console is where you can enter R commands and see the output. The environment panel shows you the objects (variables, data frames, etc.) that are currently in your R session, as well as their values. The files, plots, help, etc. panel shows you the files, plots, and other items that are currently in your R workspace, as well as help and documentation for R functions and packages. We will cover this in more detail later in this course. 1.2.1.3 Running R code Once you have both programs installed, you can open RStudio and begin a new R session. To run R code using R Studio, follow these steps: In the source editor panel, type your R code. To run the code, you can either press the Run button or use the keyboard shortcut Ctrl + Enter (Windows) or Command + Enter (Mac). The code will be executed in the console panel, and any output will be displayed there. Alternatively, you can directly type single-statement R commands in the console and run them by pressing Enter. For example, let’s say you want to calculate the sum of the numbers 1, 2, and 3. You can write the following code in the console or in the source editor: # Calculate the sum of 1, 2, and 3 1 + 2 + 3 ## [1] 6 If you’ve entered it in the console, press Enter. If you’ve entered it in the source editor, you can press the Run button or use the keyboard shortcut to run the code. The output will be displayed in the console: &gt; 1 + 2 + 3 [1] 6 1.2.1.4 Base R operations The R {base} package contains the basic functions which let R function as a programming language: arithmetic, input/output, basic programming support, etc. Its contents are always available when you start an R session. Here we introduce the main binary operators, which work on vectors, matrices and scalars. Arithmetic operators: + addition - subtraction * multiplication / division ^ or ** exponentiation Logical operators: &gt; greater than &gt;= greater than or equal to == exactly equal to &lt; less than &lt;= less than or equal to != not equal 1.2.2 R objects In addition to running single statements in the R console, the output of a statement can be saved as a new object. There are many kinds of R objects, some of which are covered here and in future chapters. 1.2.2.1 Types of data First, we will introduce the different types of data that one can encounter. We can classify variables according to what values they take. Numerical: These variables can be measured quantitatively and their value is a number. Continuous: We say that a variable is continuous when it can take an infinite number of real values within an interval. One could consider unbounded variables (height above sea level) or restricted variables, like positive variables (weight of a person) or an interval (a proportion between 0 and 1). Discrete: When the variable can only take a finite number of values in an interval, we say it is discrete. A common example is count data, like the population of a city. Categorical: The values are characteristics that cannot be quantified. Binary: These variables have two possible values: TRUE or FALSE (a variable indicating whether the person has siblings or not). Nominal: They describe a name, label, or category without a natural order (for example, the name of a person). Ordinal: Like their name indicates, ordinal variables are categorical and follow a natural order. For example, “terrible”, “bad”, “neutral”, “good”, “great”. A numerical variable can sometimes be discretized and put into categories, like dividing a person’s age into age groups (bins) “toddler”, “child”, “teenager”, “adult”. Next, we will see how these different types of variables can be treated in R. 1.2.2.2 Variables and classes In R, a variable is a named location in memory that stores a value. To create a variable, you simply assign a value to a name using the &lt;- operator (or the = operator, which has an equivalent role when assigning values to a variable, but &lt;- is preferred). For example: my_variable &lt;- 5 This code creates a variable called my_variable and assigns the value 5 to it. You can access the value of a variable or any other object by simply referring to its name, like this: my_variable ## [1] 5 When you run this code, the value of my_variable will be printed to the console. Running print(my_variable) is an alternative syntax, using the print() function. In R, every object and value has a class that determines how it is stored and how it behaves. For example, the 5 in our example above is a number, so its class is numeric. To find out the class of a value or a variable, you can use the class() function, like this: class(5) ## [1] &quot;numeric&quot; class(my_variable) ## [1] &quot;numeric&quot; The most basic classes are: numeric (num) - any real number, e.g. 2.375 integer (int) - integer numbers, e.g. 2 character (chr) - any string, e.g., \"fluxes\" logical (logi) - binary, i.e., either TRUE or FALSE. factor (Factor) - categorical data, the variable can only be one of a defined number of options, e.g., one of C3, C4, or CAM (the three pathways of photosynthesis). Factors may also be given an order. function - a set of statements organized to perform a specific task, for example mean() By default, any number is coerced as \"numeric\". So if you want an integer value to have class \"integer\", you need to specify it like this: my_variable &lt;- as.integer(5) class(my_variable) ## [1] &quot;integer&quot; Sometimes, you need to convert the class of an object, for example turning an \"integer\" number into a \"character\". You can do so as follows: my_variable &lt;- as.character(my_variable) my_variable ## [1] &quot;5&quot; class(my_variable) ## [1] &quot;character&quot; Note that now, the values are in quotes \"5\". This way, R interprets it as a text and you will not be able to do any numeric calculations with it anymore. 1.2.2.3 Vectors A vector in R is a sequence of data elements of the same class. Vectors can be created with the c() function, which stands for concatenate, i.e., to link together in a series or chain. For example, the following code creates a numeric vector: x &lt;- c(1, 2, 3, 4, 5) To access the elements of a vector, you can use the square bracket notation. For example, the following code retrieves the second element of the vector x: x[2] ## [1] 2 You can also use the square bracket notation to extract a sub-vector from a larger vector. For example, you can extract the second to fourth elements of the vector x: x[2:4] ## [1] 2 3 4 Another useful property of vectors in R is that they can be easily combined using arithmetic operators. For example, adding the elements of two vectors x and y element-wise: x &lt;- c(1, 2, 3) y &lt;- c(4, 5, 6) x + y ## [1] 5 7 9 R also supports vectors of other classes, for example character vectors. Since all elements must be of the same class, the most general class will be adopted. The following code concatenates the vectors x and y, followed by new character elements: z &lt;- c(x, y, &quot;seven&quot;, &quot;eight&quot;) z ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;seven&quot; &quot;eight&quot; class(z) ## [1] &quot;character&quot; Operations on vectors are performed element-wise. For example, if we ask what numbers in x are greater than 2, we obtain a vector of logical values (and class \"logical\"): x &gt; 2 ## [1] FALSE FALSE TRUE Vectors that contain sequences of numbers are often needed in programming. They are easily created in R, e.g., by: 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 More flexibility is offered by the seq() function: seq(from = 0, to = 10, by = 2) ## [1] 0 2 4 6 8 10 Often, we need to evaluate multiple elements in a vector. We’ve learned that the operator &gt; tests whether the value left to it is greater than the value on its right and returns a logical. We can subset a vector based on a vector of equal length that contains logicals. x &gt; 1 ## [1] FALSE TRUE TRUE x ## [1] 1 2 3 x[x &gt; 1] ## [1] 2 3 We can also determine the indices (positions of elements in the vector) that evaluate to TRUE, or that have the lowest value: which(x &gt; 1) ## [1] 2 3 which.min(x) ## [1] 1 1.2.2.4 Lists Lists are R objects, of class \"list\". They are a bit like vectors, but more flexible. They allow us to store different types of data, even if they are of different lengths or of different classes. They are created with the function list() and can be named or not. Here is an example where each element of the list is named. mylist &lt;- list( temperatures = c(2.234, 1.987, 4.345), my_favourite_function = mean, my_favourite_course = &quot;Applied Geodata Science&quot; ) Similar to vectors, we can extract elements from lists, either by index [[1]] or by the name using [[\"temperatures\"]] or $temperatures. Note the double [[]] here, indicating an element of a list as opposed to [] indicating an element of a vector. To get the entire vector of temperatures, do either of the three: mylist[[1]] ## [1] 2.234 1.987 4.345 mylist[[&quot;temperatures&quot;]] ## [1] 2.234 1.987 4.345 mylist$temperatures ## [1] 2.234 1.987 4.345 Note below how, if we index the list like we would index a vector, a list with just one element would be returned, rather than the element itself. [ is used to subset a list (and a list is returned). In contrast, [[ or $ extract a single element from a list. A thorough explanation of these differences is given here and here. mylist[1] # returns a subset of the list as a new list ## $temperatures ## [1] 2.234 1.987 4.345 mylist[[1]] # extracts the first element of the list (a vector) ## [1] 2.234 1.987 4.345 To get the first temperature value, which is an element of the vector (at the same time an element of the list), we can run: mylist[[&quot;temperatures&quot;]][1] ## [1] 2.234 You can also append elements to the list (either way is possible): mylist[[&quot;my_second_favourite_function&quot;]] &lt;- median mylist$my_second_favourite_function &lt;- median This was a very condensed introduction to vectors and lists. A more complete introduction is given here. 1.2.2.5 Data frames A data frame, an object of class \"data.frame\", is essentially a table, consisting of named columns and rows. A data frame can be created as follows: df &lt;- data.frame(name = c(&quot;Maria&quot;, &quot;Peter&quot;, &quot;Alex&quot;), age = c(13, 56, 30)) df ## name age ## 1 Maria 13 ## 2 Peter 56 ## 3 Alex 30 A data frame can also be understood as a list of vectors of equal length, whereby each vector vector makes up a column and each of these vectors (columns) contains values of the same type. This notion makes it also evident that the elements of a data frame can be accessed the same way like we access elements of lists. To get the vector corresponding to the column named age, we can do: df$age ## [1] 13 56 30 Data frames can be also be treated as a matrix. Note that the first index refers to rows and the second to columns. For example: df[, 1] # first column ## [1] &quot;Maria&quot; &quot;Peter&quot; &quot;Alex&quot; df[2, ] # second row ## name age ## 2 Peter 56 df[2,2] # age of Peter ## [1] 56 There are many more things you can do with data frames. Since they are central to analyzing data with R, we go into more detail in Chapter 2 and have dedicated all of Chapter 3 to teach you how to work with data frames in a tidy way. 1.2.2.6 Functions R functions can be applied to an object (or several objects) and return another object. For example, the mean() function can take a numeric vector as input and output the mean of its elements. mean(df$age) ## [1] 33 Functions are also R objects and have class \"function\". They are covered in more detail in Chapter 2. 1.2.2.7 Missing values R has two representations for missing values: NA and NULL. Similar objects also exist in other programming languages. NA is an identifier to mark missing data and stands for not available. You will encounter this when reading data into a data frame, and some of its cells show NA because that value is missing. Also, if you ask for the fourth element of a vector of length 3, R returns NA. x[4] ## [1] NA In general, operations on vectors that contain at least one NA value return NA. For example: mean(c(1, 2, NA)) ## [1] NA To remove all missing values in the function evaluation, the common argument to set in the respective function call is na.rm. By default, it is usually set to FALSE, but we can do: mean(c(1, 2, NA), na.rm = TRUE) ## [1] 1.5 Furthermore, NA counts as an element in vectors. A variable assigned just NA would have length 1 (of class \"logical\") and the vector above has length 3, as can be determined using the length() function, and has class \"numeric\". By contrast, NULL is the R null object or empty space. You can also assign NULL to a variable, which will then have length zero because it is empty. Functions may return NULL when no output was defined, or if an error occurred. 1.2.2.8 R environment The set of objects (variables, data frames, etc.) defined during an R session are referred to as the environment. You can view the objects in RStudio in the environment panel in R Studio, grouped as Data, Values and Functions. After closing an existing R session (e.g., after quitting RStudio), the environment defined by the used during that session will not be saved automatically and will be lost. To save your environment, go to the Session menu and select Save Workspace As…. This will save all your objects in a .RData file in your working directory. The working directory is the default location to which R writes to and reads files from. You can check what your current working directory is running getwd() in the command line. However, this workflow is not recommended. Next, we will go over some more sophisticated ways of writing code and saving your progress. 1.2.2.9 Read and save objects The function save() allows to save multiple R objects of any form as a single .RData file. This is how the environment of your R session is saved. This is how we would save several R objects: save(df, df_small, file = &quot;data/data_frames.RData&quot;) .RData files are read into your environment using the load() function. This function loads the objects with the name that they were saved with. load(&quot;data/data_frames.RData&quot;) Alternatively, the function saveRDS() allows you save single R objects that can then be read into with a specific (potentially new) variable name. This is more transparent than using save() and load() and gives the user more control. saveRDS(df_small, file = &quot;data/df_small.rds&quot;) df_small_2 &lt;- readRDS(&quot;data/df_small.rds&quot;) save() and saveRDS() create binary files that are fast to write and read, but only intelligible to R. Such files are commonly identified by the suffix .rds. It is recommended to name the .rds files according to the single object they contain. When publishing and sharing data, avoid file formats that are not readable across different platforms and programming languages. We will learn more about open source binary file formats in Chapter 5. 1.2.3 R scripts Usually, multiple statements are needed to get, e.g., from reading data into R to final numbers and figures that make up a further analysis. Together, these multiple statements constitute a workflow. It is essential that all workflows that underlie results of publications are reproducible, that is, that another person can replicate your results using your code and certain data. To make a workflow reproducible, the sequence of statements that you needed to carry out your analysis and produce outputs can be saved as an R script. A script is a text file named with the suffix .R to indicate that it is executable by R. It contains a sequence of R commands, which you can be executed, line by line, starting from the top. To create a new script in RStudio, go to the File menu and select New File &gt; R Script. This will open a new script file in the source editor. You can then type your R code in the script file and save it to your computer. To run a script, you can either use the Source button in the source editor or use the keyboard shortcut Ctrl + Shift + Enter (Windows) or Command + Shift + Enter (Mac). This will run all of the commands in the script file, in the order they are written, in the console. Alternatively, you can type into the console: &gt; source(&quot;my_r_script.R&quot;) Note that, to be able to run the code above, the file my_r_script.R must be in your current working directory. 1.2.4 Style your code Nice code is clean, readable, consistent, and extensible (easily modified or adapted). Ugly code works, but is hard to work with. There is no right or wrong about coding style, but certain aspects make it easier to read and use code. Here are a few points to consider. 1.2.4.1 Spaces and breaks Adding enough white spaces and line breaks in the right locations greatly helps the legibility of code. Cramming variables, operators, and brackets without spaces leaves an unintelligible sequence of characters and it will not be clear what parts go together. Therefore, consider the following points: Use spaces around operators (=, +, -, &lt;-, &gt;, etc.). Use &lt;-, not =, for allocating a value to a variable. Code inside curly brackets should be indented (recommended: two white spaces at the beginning of each line for each indentation level - don’t use tabs). For example: if (temp &gt; 5.0){ growth_temp &lt;- growth_temp + temp } 1.2.4.2 Variable naming It is preferable to use concise and descriptive variable names. Different variable naming styles are being used. In this course, we use lowercase letters, and underscores (_) to separate words within a name (_). Avoid (.) as they are reserved for certain types of objects in R. Also, avoid naming your objects with names of common functions and variables since your re-definition will mask already defined object names. For example, df_daily is a data frame with data at a daily resolution. Or clean_daily is a function that cleans daily data. Note that a verb is used as a name for a function and an underscore (_) is used to separate words. It is also recommendable to avoid variable names consisting of only one character. This makes it practically impossible to search for that variable. # Good day_01 # Bad DayOne day.one first_day_of_the_month djm1 # Very bad mean &lt;- function(x) sum(x)/length(x) # mean() itself is already a function T &lt;- FALSE # T is an abbreviation of TRUE c &lt;- 10 # c() is used to create a vector (example &lt;- c(1, 2, 3)) 1.2.4.3 Script style Load libraries at the very beginning of a script, followed, by reading data from files. Functions should be defined in separate .R files, unless they are only a few lines long. Then, place the sequence of statements. The name of the script should be short and concise and indicate what the script does. Use comments to describe in human-readable text what the code does. Comments are all that appears to the right of a # and are code parts that are not interpreted by R and not executed. Adding comments in the code greatly helps you and others to read your code, understand what it does, modify it, and resolve errors (bugs). To visually separate parts of a script, use commented lines (e.g., #----). The RStudio text editor automatically recognizes ---- added to the right end of a commented line and interprets it as a block of content which can be navigated by using the document (Outline button in the top right corner of the editor panel). Avoid reading entire workspace environments (e.g., load(old_environment.RData)), deleting environments rm(list=ls()), loading hidden dependencies (e.g., .Rprofile), or changing the working directory (setwd(\"~/other_dir/\") as part of a script. Note that information about the author of the script, its creation date, and modifications, etc. should not be added as commented text in the file. In Chapter 7, we will learn about the code versioning control system git, which keeps track of all this as meta information associated with files. A good and comprehensive best practices guide is given by the tidyverse style guide. 1.2.5 R Markdown R Markdown files are an enhanced version of scripts. They combine formatted text and executable code chunks. They can either be compiled (knitted) into an HTML or PDF output, where code chunks are executed upon compilation and visualization outputs are directly placed into the output, or they can be run like a script entirely or each code chunk separately. Rmarkdown is ideal for reporting, i.e., writing your final document presenting your analysis results. When opened in RStudio, RMarkdown files appear in the Editor like this: Figure 1.1: R Markdown document opened in the source panel. As shown in Fig. 1.1, an RMarkdown file consists of a header that specifies the document properties, such as how it should be rendered (as an html page, a docx file or a pdf). --- title: &quot;Simple global map&quot; author: Alex Humboldt output: html_document --- Below the header follow the contents as either text or code chunks. Text is formatted using the Markdown syntax. Nice brief guides are provided here or here. For example, a top-level section title is specified by # and a title of a section one level lower by ## Code chunks that contain executable R code are opened by a line ```{r}. The document can be rendered by calling rmarkdown::render() on the command line or hitting the Knit button in the RStudio IDE. Depending on your settings a html file, pdf or docx file will be generated in your current directory (and or displayed in the IDE viewer). The RMarkdown source file shown in Fig. 1.1 is rendered to: Figure 1.2: Rendered HTML output. Note that the code chunk produced a figure as an output which was placed directly in the rendered html. This demonstrates the usefulness of RMarkdown as notebooks to document entire workflows and make their outputs reproducible. To create a new RMarkdown file, select from the drop-down menu in the top-left corner of RStudio as shown below: Figure 1.3: Create a new RMarkdown file 1.2.6 Workspace management Using R projects in combination with Git is the essence of efficient workspace management in R. All files that belong together are organised within one directory. This can be regarded as the project directory and is typically congruent with what belongs to the respective Git repository. By keeping an organized workspace, another person (or your future self) can find relevant files, run your code and reproduce your analysis workflow easily. 1.2.6.1 Installing Git Git is a version control system (covered in detail in Chapter 7) which is integrated in RStudio. Code repositories can then be hosted in the cloud, for example in GitHub. To use it later in this course (and throughout your career), let’s set up these tools: First, follow the Git download and installation instructions for your OS outlined here. Once Git is installed, set up RStudio for version control by going to Tools &gt; Global Options &gt; Git/SVN and tick “Enable version control interface…”. You can check that the path to your Git installation is correct. If you don’t have one already, create a GitHub account on https://github.com and sign in. Connect your GitHub account with Git locally, using SSH keys. You should sequentially follow the instructions to check for existing SSH keys, generate a new SSH key, add a new SSH key to your GitHub account and finally test your SSH connection. Note that, at the top of these websites, there’s an option to choose your OS because the steps vary depending on the type of computer your work with. Now you should be able to use Git in the Terminal panel in RStudio, whenever your R Project is also a repository. As mentioned above, we will start working with these tools in a few weeks and they will be a crucial part of the course. 1.2.6.2 R projects RStudio also allows you to work with R projects. An R project is a collection of files and folders that you use for a specific analysis or data project. An R project makes it easier to organize and manage your files and keep track of your work. To create a new R project, go to the File menu and select New Project…. This will open the New Project dialog, where you can choose where to save your project and what type of project to create. The current project that you are working on is shown on the upper right corner of the RStudio interface. Here you can also switch between existing projects or create a new one. R Project menu. When starting a new project, a file &lt;project_name.Rproj&gt; is created. It sits in the project directory and stores information about your last session (settings, open files, etc.) and optionally (not recommended) the environment of that session. The use of R projects also automatically enables useful features in RStudio for easy package, website, or book building and lets you manage Git for the repository corresponding to the project. When you want to continue working on an existing R project, you can start a new session by clicking on your &lt;project_name.Rproj&gt; file. This restores settings from your last R session, including the variables in your environment, and sets your working directory to the project directory. Nevertheless, we recommend to start by emptying your environment and loading your data and variables using the code you previously wrote. That way, you ensure that your results are reproducible. 1.2.6.3 Folder structure Once you have created an R project, you can create new scripts and other files within the project. These files will be organized in a folder structure, which you can view and manage in the files, plots, help, etc. panel. For example, keep source files where R functions are defined in ./R (where . refers to the current project directory), data files in ./data and visualizations in ./fig. It’s advisable to write output files, created by the code of your project, to sub-directories within the project directory. To read and write from/to files you should use relative paths (relative to the project’s root directory), like any of the two equivalent following options: &gt; source(&quot;./R/my_r_script.R&quot;) &gt; source(&quot;R/my_r_script.R&quot;) A project directory should only contain code, data and outputs that belong to this one project. Stuff that may belong to multiple projects should be kept somewhere else. For example, keep original data (e.g., the raw data files that you created when collecting the data in the field, or data files you downloaded from the web) outside the project directory. Exceptions are small data files, which you can keep in ./data_raw. It is advisable to create a separate data directory outside the project (e.g., ~/data/, where ~ refers to your home directory) that holds all the original data you ever downloaded, or obtained from peers, or gathered yourself. Within such a data directory, you can put files from different sources into separate sub-directories and add a description file (e.g., ~/data/some_data_source/README) defining by whom, from where and when the data was obtained and defining data use policy. You can find an R project template in the GECO GitHub page. It shows an example of how you can organize your files into folders. Using such a template removes the overhead of designing a structure for each new project and can help you keep your work organized and make it easier to reuse and share your code. 1.3 Exercises Dimensions of a circle Given the radius of a circle r, write a few lines of code that calculates its area and its circumference. Run your code with different values assigned to r. Print the solution as text. Hint: Enter pi in your console. Hint: Entering print(\"agds\") in your console returns \"agds\". Combining (concatenating) multiple strings into a single one can be done using paste(). Sequence of numbers Generate a sequence of numbers from 0 and \\(\\pi\\) as a vector with length 5. Hint: Consult the manual of the function seq() by entering ?seq in your terminal. Gauss sum Rumors have it that young Carl Friedrich Gauss was asked in primary school to calculate the sum of all natural numbers between 1 and 100. He did it in his head in no time. We’re very likely not as intelligent as young Gauss. But we have R. What’s the solution? Gauss calculated the sum with a trick. The sum of 100 and 1 is 101. The sum of 99 and 2 is 101. You do this 50 times, and you get \\(50 \\times 101\\). Demonstrate Gauss’ trick with vectors in R. Magic trick algorithm Define a variable named x that contains an integer value and perform the following operations in sequence: Redefine x by adding 1. Double the resulting number, over-writing x. Add 4 to x and save the result as x. Redefine x as half of the previous value of x. Subtract the originally chosen arbitrary number from x. Print x. Restart the algorithm defined above by choosing a new arbitrary natural number. Vectors Print the object datasets::rivers and consult the manual of this object. What is the class of the object? What is the length of the object? Calculate the mean, median, minimum, maximum, and the 33%-quantile across all values. Hint: If you don’t know how to solve a problem, help yourself on the internet. Data frames Print the object datasets::quakes and consult the manual of this object. Determine the dimensions of the data frame using the respective function in R. Extract the vector of values in the data frame that contain information about the Richter Magnitude. Determine the value largest value in the vector of event magnitudes. Determine the geographic position of the epicenter of the largest event. RMarkdown Create an RMarkdown file and implement your solutions to above exercises in it. Give the file a title, implement some structure in the document, and write some text explaining what your code does. "],["programmingprimers.html", "Chapter 2 Programming primers 2.1 Learning objectives 2.2 Tutorial 2.3 Exercises", " Chapter 2 Programming primers Chapter lead author: Pepa Aran 2.1 Learning objectives After you’ve gone over the lecture and solved the exercises, you should be able to: Install and load libraries and packages Read, inspect, visualise and write data frames Use loops, conditional statements and functions in your code Organize your R project for data analysis Look for help 2.2 Tutorial 2.2.1 Libraries Packages, sometimes called libraries, are collections of R functions, data, and complied code in a well-defined format. R comes with a standard set of packages (including base R, utils, stats…) and other packages targeted for specific applications are available for download and installation. Once installed, you need to load them each time you start a new R session to use them. For example, the {tidyverse} package is used for data wrangling and will be covered in this course. This is a special package which loads many other packages in the background (like {readr}, {ggplot2}, etc.). You can install a new package as follows: install.packages(&quot;tidyverse&quot;) Then, you can load it with the following code. Note that now the name of the package is not in quotation marks. library(tidyverse) You can now use the functions and features provided by the {tidyverse} package in your R scripts. At any time, you can see a list of your installed packages on the source panel with the following command: library() And a list of the packages currently loaded: search() ## [1] &quot;.GlobalEnv&quot; &quot;package:stats&quot; &quot;package:graphics&quot; ## [4] &quot;package:grDevices&quot; &quot;package:datasets&quot; &quot;renv:shims&quot; ## [7] &quot;package:utils&quot; &quot;package:methods&quot; &quot;Autoloads&quot; ## [10] &quot;package:base&quot; This information can also be found on the Packages panel in RStudio. The loaded packages are shown with a tick mark. Finally, let’s install all the missing packages and load all required packages for this course: use_pkgs &lt;- c(&quot;dplyr&quot;, &quot;tidyr&quot;, &quot;readr&quot;, &quot;lubridate&quot;, &quot;stringr&quot;, &quot;purrr&quot;, &quot;ggplot2&quot;, &quot;tidyverse&quot;, &quot;visdat&quot;, &quot;terra&quot;, &quot;hexbin&quot;, &quot;jsonlite&quot;, &quot;MODISTools&quot;, &quot;forcats&quot;, &quot;yardstick&quot;, &quot;recipes&quot;, &quot;caret&quot;, &quot;broom&quot;, &quot;skimr&quot;, &quot;cowplot&quot;, &quot;scico&quot;, &quot;hwsdr&quot;, &quot;usethis&quot;, &quot;renv&quot;, &quot;rsample&quot;, &quot;modelr&quot;, &quot;rmarkdown&quot;, &quot;rpart&quot;, &quot;rpart.plot&quot;, &quot;ranger&quot;, &quot;sessioninfo&quot;) new_pkgs &lt;- use_pkgs[!(use_pkgs %in% installed.packages()[, &quot;Package&quot;])] if (length(new_pkgs) &gt; 0) install.packages(new_pkgs) ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/MODISTools_1.1.4.tar.gz&#39; ... ## OK [downloaded 387.7 Kb in 0.8 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/sf_1.0-9.tar.gz&#39; ... ## OK [downloaded 3.3 Mb in 0.9 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/classInt_0.4-8.tar.gz&#39; ... ## OK [downloaded 487 Kb in 0.5 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/s2_1.1.2.tar.gz&#39; ... ## OK [downloaded 2.1 Mb in 0.9 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/wk_0.7.1.tar.gz&#39; ... ## OK [downloaded 1.6 Mb in 0.8 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/units_0.8-1.tar.gz&#39; ... ## OK [downloaded 928.4 Kb in 0.9 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/sp_1.6-0.tar.gz&#39; ... ## OK [downloaded 1.7 Mb in 0.8 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/skimr_2.1.5.tar.gz&#39; ... ## OK [downloaded 1.2 Mb in 1.3 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/repr_1.1.6.tar.gz&#39; ... ## OK [downloaded 118.8 Kb in 1 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/hwsdr_1.0.tar.gz&#39; ... ## OK [downloaded 21.9 Kb in 0.8 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/raster_3.6-14.tar.gz&#39; ... ## OK [downloaded 3.1 Mb in 1 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/usethis_2.1.6.tar.gz&#39; ... ## OK [downloaded 772.2 Kb in 0.8 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/gert_1.9.2.tar.gz&#39; ... ## OK [downloaded 2.7 Mb in 0.9 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/credentials_1.3.2.tar.gz&#39; ... ## OK [downloaded 168 Kb in 0.9 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/zip_2.2.2.tar.gz&#39; ... ## OK [downloaded 606.7 Kb in 0.9 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/gh_1.3.1.tar.gz&#39; ... ## OK [downloaded 91.9 Kb in 0.9 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/gitcreds_0.1.2.tar.gz&#39; ... ## OK [downloaded 93.3 Kb in 0.8 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/ini_0.3.1.tar.gz&#39; ... ## OK [downloaded 12.9 Kb in 0.8 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/whisker_0.4.1.tar.gz&#39; ... ## OK [downloaded 64.3 Kb in 0.8 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/ranger_0.14.1.tar.gz&#39; ... ## OK [downloaded 396 Kb in 1.2 secs] ## Retrieving &#39;https://packagemanager.rstudio.com/cran/__linux__/jammy/latest/src/contrib/RcppEigen_0.3.3.9.3.tar.gz&#39; ... ## OK [downloaded 1.5 Mb in 0.8 secs] ## Installing classInt [0.4-8] ... ## OK [installed binary] ## Moving classInt [0.4-8] into the cache ... ## OK [moved to cache in 7.9 milliseconds] ## Installing wk [0.7.1] ... ## OK [installed binary] ## Moving wk [0.7.1] into the cache ... ## OK [moved to cache in 8.1 milliseconds] ## Installing s2 [1.1.2] ... ## OK [installed binary] ## Moving s2 [1.1.2] into the cache ... ## OK [moved to cache in 7.3 milliseconds] ## Installing units [0.8-1] ... ## OK [installed binary] ## Moving units [0.8-1] into the cache ... ## OK [moved to cache in 9.9 milliseconds] ## Installing sf [1.0-9] ... ## OK [installed binary] ## Moving sf [1.0-9] into the cache ... ## OK [moved to cache in 10 milliseconds] ## Installing sp [1.6-0] ... ## OK [installed binary] ## Moving sp [1.6-0] into the cache ... ## OK [moved to cache in 8.6 milliseconds] ## Installing MODISTools [1.1.4] ... ## OK [installed binary] ## Moving MODISTools [1.1.4] into the cache ... ## OK [moved to cache in 7.9 milliseconds] ## Installing repr [1.1.6] ... ## OK [installed binary] ## Moving repr [1.1.6] into the cache ... ## OK [moved to cache in 7.2 milliseconds] ## Installing skimr [2.1.5] ... ## OK [installed binary] ## Moving skimr [2.1.5] into the cache ... ## OK [moved to cache in 7.7 milliseconds] ## Installing raster [3.6-14] ... ## OK [installed binary] ## Moving raster [3.6-14] into the cache ... ## OK [moved to cache in 9 milliseconds] ## Installing hwsdr [1.0] ... ## OK [built from source] ## Moving hwsdr [1.0] into the cache ... ## OK [moved to cache in 8.7 milliseconds] ## Installing credentials [1.3.2] ... ## OK [installed binary] ## Moving credentials [1.3.2] into the cache ... ## OK [moved to cache in 10 milliseconds] ## Installing zip [2.2.2] ... ## OK [installed binary] ## Moving zip [2.2.2] into the cache ... ## OK [moved to cache in 7.7 milliseconds] ## Installing gert [1.9.2] ... ## OK [installed binary] ## Moving gert [1.9.2] into the cache ... ## OK [moved to cache in 8.2 milliseconds] ## Installing gitcreds [0.1.2] ... ## OK [installed binary] ## Moving gitcreds [0.1.2] into the cache ... ## OK [moved to cache in 15 milliseconds] ## Installing ini [0.3.1] ... ## OK [installed binary] ## Moving ini [0.3.1] into the cache ... ## OK [moved to cache in 7 milliseconds] ## Installing gh [1.3.1] ... ## OK [installed binary] ## Moving gh [1.3.1] into the cache ... ## OK [moved to cache in 13 milliseconds] ## Installing whisker [0.4.1] ... ## OK [installed binary] ## Moving whisker [0.4.1] into the cache ... ## OK [moved to cache in 8.6 milliseconds] ## Installing usethis [2.1.6] ... ## OK [installed binary] ## Moving usethis [2.1.6] into the cache ... ## OK [moved to cache in 8.1 milliseconds] ## Installing RcppEigen [0.3.3.9.3] ... ## OK [installed binary] ## Moving RcppEigen [0.3.3.9.3] into the cache ... ## OK [moved to cache in 14 milliseconds] ## Installing ranger [0.14.1] ... ## OK [installed binary] ## Moving ranger [0.14.1] into the cache ... ## OK [moved to cache in 7.9 milliseconds] invisible(lapply(use_pkgs, require, character.only = TRUE)) 2.2.1.1 Other libraries and applications For this course, we will also need software that is not available as an R package. To work with other libraries and applications, you may need to install additional software on your computer. For example, to work with netcdf files in R, you would need to install the {ncdf4} library and the netCDF command-line tools: To install the {ncdf4} library, follow the same steps as above for installing an R library. To install the netCDF command-line tools, follow the instructions on the netCDF website. Once the {ncdf4} library and the netCDF command-line tools are installed, you can use them to work with .nc files in R. For example, you could use the nc_open() function from the \"ncdf4\" library to open a file. 2.2.2 Programming basics In this section, we will review the most basic programming elements (conditional statements, loops, functions…) for the R syntax. 2.2.2.1 Conditional statements In cases where we want certain statements to be executed or not, depending on a criterion, we can use conditional statements if, else if, and else. Conditionals are an essential feature of programming and available in all languages. The R syntax for conditional statements looks like this: if (temp &lt; 0.0){ is_frozen &lt;- TRUE } The evaluation of the criterion (here (temp &lt; 0.0)) has to return either TRUE or FALSE. Whenever the statement between parenthesis is true, the chunk of code between curly brackets is executed. Otherwise, nothing happens. if (temp &lt; 0.0){ is_frozen &lt;- TRUE } else { is_frozen &lt;- FALSE } You can also write a conditional that covers all possibilities, like the one above. When the temperature is below 0, the first chunk of code is executed. Whenever it is greater or equal that 0 (i.e. the condition returns FALSE) the second chunk of code is evaluated. You can also write more than two conditions, covering several cases. Conditionals are evaluated in order, so if the first condition is not true, it checks the second. If the second is false, it checks the third, and so on. The statements after else are evaluated when everything before was FALSE. 2.2.2.2 Loops Loops are another essential feature of programming. for and while loops exist in probably all programming languages. We introduce them here because they are a simple and powerful tool for solving many common tasks. for and while loops let us repeatedly execute the same set of commands, while changing an index or counter variable to take a sequence of different values. The following example calculates the sum of the first ten temperature values in df_small (from the previous chapter), by iteratively adding them together. temp_sum &lt;- 0 # initialize sum for (i in 1:10){ temp_sum &lt;- temp_sum + df_small$temp[i] } temp_sum Of course, this is equivalent to just using the sum() function. sum(df_small$temp[1:10]) Instead of directly telling R how many iterations it should do we can also define a condition. As long as the condition is TRUE, R will continue iterating. As soon as it is FALSE, R stops the loop. The following lines of code do the same operation as the for loop we just wrote. What is different? What is the same? i = 1 # initialize counter temp_sum &lt;- 0 # initialize sum while (i &lt;= 10){ temp_sum &lt;- temp_sum + df_small$temp[i] i = i+1 } temp_sum 2.2.2.3 Functions Often, analyses require many steps and your scripts may get excessively long. Over 2000 lines of code in one file are hard to digest. An important aspect of good programming is to avoid duplicating code. If the same sequence of multiple statements or functions are to be applied repeatedly to different objects, then it is usually advisable to bundle them into a new function and apply this single function to each object. This also has the advantage that if some requirement or variable name changes, it has to be edited only in one place. A further advantage of writing functions is that you can give the function an intuitively understandable name, so that your code reads like a sequence of orders given to a human. For example, the following code, converting temperature values provided in Fahrenheit to degrees Celsius, could be turned into a function. ## NOT ADVISABLE temp_soil &lt;- (temp_soil - 32) * 5 / 9 temp_air &lt;- (temp_air - 32) * 5 / 9 temp_leaf &lt;- (temp_leaf - 32) * 5 / 9 Functions are a set of instructions encapsulated within curly brackets ({}) that generate a desired outcome. Functions contain four main elements: They start with a name to describe their purpose, then they need arguments, which are a list of the objects being input, enclosed by curly brackets function(x){ ... } the code making up the body of the function, and lastly, within the body, a return statement indicting the output of the function. Below we define our own function convert_fahrenheit_to_celsius(): ## ADVISABLE convert_fahrenheit_to_celsius &lt;- function(temp_f){ # Convert values temp_c &lt;- (temp_f - 32) * 5 / 9 # Return statement temp_c } temp_soil &lt;- convert_fahrenheit_to_celsius(temp_soil) temp_air &lt;- convert_fahrenheit_to_celsius(temp_air) temp_leaf &lt;- convert_fahrenheit_to_celsius(temp_leaf) A good practice when writing a function is to document what the function does, the meaning and structure of every input (arguments) and the output (value) of the function. This helps you and others to reuse that function, without having to read and understand what the function does internally. For example, you can write it as a header in the body of the function, or as a header of the script where you define it. Furthermore, one should use the return() statement only for early returns (e.g. inside an if statement) and otherwise, R returns the result of the last evaluated expression. Note that, without the last line of the function’s body, our function wouldn’t return anything. convert_fahrenheit_to_celsius &lt;- function(temp_f){ # This function converts temperature values in Fahrenheit to Celsius # Arguments: # temp_f: numerical vector of temperature values in Fahrenheit # Value: # temp_c: numerical vector of temperatures in Celsius temp_c &lt;- (temp_f - 32) * 5 / 9 temp_c } Functions become increasingly important the more experienced one gets at coding. Using them minimises the amount of code being re-written, decreases accidental errors when retyping code and are key to keeping a clean workspace. Functions have their own environment, which means variables within the function are only ‘live’ or used when the function is running but are not saved to the global environment unless they are part of the output of the function. A good moment to think about using a function is when sections of code are being repeated again and again. Whenever possible, we should combine multiple processing steps that naturally belong together. Specifically, when the same sequence of steps must be applied to multiple datasets that have the same structure (variable names, etc.). Once such a function is created, we can apply it to the data in one go, instead of repeating the successive steps. Functions (particularly long ones) can be written to separate source files. These R scripts containing only function definitions can be saved in your ./R directory, to keep your workspace organized. Preferably, the file has the same name as the function. We can save the previous function in a script .R/convert_fahrenheit_to_celsius.R and load it later by running source(\".R/convert_fahrenheit_to_celsius\"). 2.2.3 Working with data frames In the first tutorial, we introduced data frames as an R object. Now, let’s get our hands on actual data for demonstrating how data is read, manipulated and written. As most of the code displayed in this book, the code chunks below are executable. You can try it out by opening the the book’s R project in RStudio. We are going to work with data from ecosystem flux measurements, taken by the eddy covariance technique, and provided as part of the FLUXNET2015 dataset (Pastorello2020?), which you can see here. The data we’re using below comes from a flux tower near Zürich (CH-Lae, located on the Laegern mountain between Regensberg and Baden and run by our colleagues at ETH). The data is stored as a Comma Separated Values file (.csv). This is a plain-text, and therefore a non-proprietary format. To follow the open science principles for data, distribute your data in a format that is non-proprietary and readable across platforms and applications. For example, avoid distributing your data as an Excel spreadsheat (.xlsx), or a Matlab data object (.mat), or an R data object (.RData, or .rds). 2.2.3.1 Reading data To import the data into the R environment, we use the function read_csv() from the {tidyverse} package. In other R code, you will also encounter the base R read.csv() function. However, read_csv() is much faster and reads data into a tidyverse-data frame (a tibble) which has some useful additional characteristics, on top of a common R data frame. To tell the function where the data is located, pass the data’s path as an argument. You can either use an absolute path, starting from C:/ on a Windows computer or ~/ on a Mac or Linux. Or, alternatively, you can provide a relative path, where ./ points to the present working directory and ../ is one level up, or ../../ is two levels up, etc. We recommend that you work with R projects and use relative paths, because the working directory is set to the root directory of the R project and relative paths will also work on another person’s computer, helping with reproducibility. df &lt;- readr::read_csv(&quot;./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv&quot;) print(df) ## # A tibble: 52,608 × 235 ## TIMEST…¹ TIMES…² TA_F_…³ TA_F_…⁴ TA_ERA TA_F TA_F_QC SW_IN…⁵ SW_IN…⁶ SW_IN…⁷ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.00e11 2.00e11 -9999 -9999 -2.22 -2.22 2 0 -9999 -9999 ## 2 2.00e11 2.00e11 -9999 -9999 -2.25 -2.25 2 0 -9999 -9999 ## 3 2.00e11 2.00e11 -9999 -9999 -2.28 -2.28 2 0 -9999 -9999 ## 4 2.00e11 2.00e11 -9999 -9999 -2.50 -2.50 2 0 -9999 -9999 ## 5 2.00e11 2.00e11 -9999 -9999 -2.72 -2.72 2 0 -9999 -9999 ## 6 2.00e11 2.00e11 -9999 -9999 -2.94 -2.94 2 0 -9999 -9999 ## 7 2.00e11 2.00e11 -9999 -9999 -3.17 -3.17 2 0 -9999 -9999 ## 8 2.00e11 2.00e11 -9999 -9999 -3.39 -3.39 2 0 -9999 -9999 ## 9 2.00e11 2.00e11 -9999 -9999 -3.61 -3.61 2 0 -9999 -9999 ## 10 2.00e11 2.00e11 -9999 -9999 -3.59 -3.59 2 0 -9999 -9999 ## # … with 52,598 more rows, 225 more variables: SW_IN_ERA &lt;dbl&gt;, SW_IN_F &lt;dbl&gt;, ## # SW_IN_F_QC &lt;dbl&gt;, LW_IN_F_MDS &lt;dbl&gt;, LW_IN_F_MDS_QC &lt;dbl&gt;, LW_IN_ERA &lt;dbl&gt;, ## # LW_IN_F &lt;dbl&gt;, LW_IN_F_QC &lt;dbl&gt;, LW_IN_JSB &lt;dbl&gt;, LW_IN_JSB_QC &lt;dbl&gt;, ## # LW_IN_JSB_ERA &lt;dbl&gt;, LW_IN_JSB_F &lt;dbl&gt;, LW_IN_JSB_F_QC &lt;dbl&gt;, ## # VPD_F_MDS &lt;dbl&gt;, VPD_F_MDS_QC &lt;dbl&gt;, VPD_ERA &lt;dbl&gt;, VPD_F &lt;dbl&gt;, ## # VPD_F_QC &lt;dbl&gt;, PA &lt;dbl&gt;, PA_ERA &lt;dbl&gt;, PA_F &lt;dbl&gt;, PA_F_QC &lt;dbl&gt;, P &lt;dbl&gt;, ## # P_ERA &lt;dbl&gt;, P_F &lt;dbl&gt;, P_F_QC &lt;dbl&gt;, WS &lt;dbl&gt;, WS_ERA &lt;dbl&gt;, WS_F &lt;dbl&gt;, … The file is automatically machine-readable because we have: Only one header row, containing the column (variable) names. Variables organised by columns, and observations by rows. Each column consists of a single data type (e.g., character, numeric, logical; see below for more info). Here, all columns are interpreted as numeric (&lt;dbl&gt;). One value per cell. No merged cells. In short, the data frame is tidy. To understand the sort of object we work with, i.e. the class, we can do: class(df) ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Fundamentally, df is a data.frame. In addition, it is also of some other classes (\"spec_tbl_df\", \"tbl_df\", \"tbl\") which gives it additional features. Other types of data inputs and how to read them will be covered in Chapter 5. 2.2.3.2 Understanding the data structure There are several base R functions to help you understand the structure of a data frame. Here is a non-exhaustive list of of them: Size dim() - Returns the dimensions of an object (here: number of rows and columns). nrow() - Returns the number of rows of an object. ncol() - Returns the number of columns of an object. Content head() - Returns the first 6 rows. tail() - Returns the last 6 rows. View() - Opens a window in the source panel in RStudio where you can look at the entire data set in the form of a table (It is not supported by the Jupyter environment). Names names() - Returns the column names (for data.frame objects it is synonymous to colnames()). rownames() - Returns the row names. Summary class() - Returns the classes of an object. str() - Returns the structure of an object and information about the class, length and content of each column. summary() - Returns generic statistics information, depending on the class of the object. For categorical variables it will show how common each class is, missing values, etc, and for numerical variables, the mean, quantiles, maximum and minimum values, etc. For example, the data frame df has 4018 rows and 334 columns: dim(df) ## [1] 52608 235 It is important to know the meaning of the column names and content. A description of standardized FLUXNET data variables is available here. A selection of available variables that we will use in subsequent chapters are: GPP (gC m\\(^{−2}\\) s\\(^{-1}\\)): Gross primary production WS (m s\\(^{-1}\\)): Horizontal wind speed USTAR (m s\\(^{-1}\\)): Friction velocity TA (\\(^{o}\\) C): Air temperature RH (%): Relative humidity (range 0–100%) PA (kPa): Atmospheric pressure G (W m\\(^{−2}\\)): Ground heat flux, not mandatory, but needed for the energy balance closure calculations NETRAD (W m\\(^{−2}\\)): Net radiation, not mandatory, but needed for the energy balance closure calculations SW_IN (W m\\(^{−2}\\)): Incoming shortwave radiation SW_IN_POT (W m\\(^−2\\)): Potential incoming shortwave radiation (top of atmosphere theoretical maximum radiation) PPFD_IN (\\(\\mu\\)mol photons m\\(^{−2}\\) s\\(^{-1}\\)): Incoming photosynthetic photon flux density P (mm): Precipitation total of each 30 or 60 minute period LW_IN (W m\\(^{−2}\\)): Incoming (down-welling) long-wave radiation SWC (%): Soil water content (volumetric), range 0–100% TS (\\(^{o}\\) C): Soil temperature CO2 (\\(\\mu\\)molCO2 mol\\(^{-1}\\)): Carbon dioxide (CO\\(_2\\)) mole fraction in moist air 2.2.3.3 Selecting data and entering the tidyverse df is a data frame. This is similar to a matrix and has two dimensions (rows and columns). If we want to extract specific data from it, we specify the indices, i.e. the “coordinates”, of the data. For two-dimensional objects (data frames, matrices), the first index refers to rows and the second to columns. For example, to refer to the element on the third row in the first column, we write: df[3,1] ## # A tibble: 1 × 1 ## TIMESTAMP_START ## &lt;dbl&gt; ## 1 200401010100 Reducing a data frame (tibble) to only the first columns can be done by: df[, 1] ## # A tibble: 52,608 × 1 ## TIMESTAMP_START ## &lt;dbl&gt; ## 1 200401010000 ## 2 200401010030 ## 3 200401010100 ## 4 200401010130 ## 5 200401010200 ## 6 200401010230 ## 7 200401010300 ## 8 200401010330 ## 9 200401010400 ## 10 200401010430 ## # … with 52,598 more rows The method of selecting parts of a data frame by index is quite flexible. For example, we may require the information in the third column for the first three rows. Putting a colon between two numbers, e.g. [1:3,], indicates we want to select the rows numbers starting at the first and ending with the second number. So here [1:3,] will give us rows one, two and three. df[1:3, 3] # reduces the data frame (tibble) to its first three rows and the 3rd column ## # A tibble: 3 × 1 ## TA_F_MDS ## &lt;dbl&gt; ## 1 -9999 ## 2 -9999 ## 3 -9999 To reduce the data frame (tibble) to several columns, the function c() is used. This outputs the data frame (tibble) reduced to the selected row or column numbers inside c(). df[, c(1,4,7)] ## # A tibble: 52,608 × 3 ## TIMESTAMP_START TA_F_MDS_QC TA_F_QC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 200401010000 -9999 2 ## 2 200401010030 -9999 2 ## 3 200401010100 -9999 2 ## 4 200401010130 -9999 2 ## 5 200401010200 -9999 2 ## 6 200401010230 -9999 2 ## 7 200401010300 -9999 2 ## 8 200401010330 -9999 2 ## 9 200401010400 -9999 2 ## 10 200401010430 -9999 2 ## # … with 52,598 more rows Another method is to select the columns by column names, i.e. giving as input a string vector with the name of each column we want to select (again, this is Base R notation). This is especially useful if the columns we want to select are not contiguous. For example: # Selecting data by name in base R df[, c(&quot;TIMESTAMP_START&quot;, &quot;TA_F_MDS&quot;, &quot;TA_F_MDS_QC&quot;)] ## # A tibble: 52,608 × 3 ## TIMESTAMP_START TA_F_MDS TA_F_MDS_QC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 200401010000 -9999 -9999 ## 2 200401010030 -9999 -9999 ## 3 200401010100 -9999 -9999 ## 4 200401010130 -9999 -9999 ## 5 200401010200 -9999 -9999 ## 6 200401010230 -9999 -9999 ## 7 200401010300 -9999 -9999 ## 8 200401010330 -9999 -9999 ## 9 200401010400 -9999 -9999 ## 10 200401010430 -9999 -9999 ## # … with 52,598 more rows In Chapter 3, we will use the tidyverse, which is a set of R packages designed for working with tidy data and writing code in a way that makes your workflow more clear and understandable. A code chunk which does the same as above, but is written for the tidyverse can read as follows. select(df, 1) # reduces the data frame (tibble) to its first column ## # A tibble: 52,608 × 1 ## TIMESTAMP_START ## &lt;dbl&gt; ## 1 200401010000 ## 2 200401010030 ## 3 200401010100 ## 4 200401010130 ## 5 200401010200 ## 6 200401010230 ## 7 200401010300 ## 8 200401010330 ## 9 200401010400 ## 10 200401010430 ## # … with 52,598 more rows select(df, TIMESTAMP_START, TA_F_MDS, TA_F_MDS_QC) # reduces the data frame to columns specified by names ## # A tibble: 52,608 × 3 ## TIMESTAMP_START TA_F_MDS TA_F_MDS_QC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 200401010000 -9999 -9999 ## 2 200401010030 -9999 -9999 ## 3 200401010100 -9999 -9999 ## 4 200401010130 -9999 -9999 ## 5 200401010200 -9999 -9999 ## 6 200401010230 -9999 -9999 ## 7 200401010300 -9999 -9999 ## 8 200401010330 -9999 -9999 ## 9 200401010400 -9999 -9999 ## 10 200401010430 -9999 -9999 ## # … with 52,598 more rows As a further shortcut in tidyverse, we can use the pipe %&gt;% operator. The data frame is still reduced to its first column: df %&gt;% select(1) ## # A tibble: 52,608 × 1 ## TIMESTAMP_START ## &lt;dbl&gt; ## 1 200401010000 ## 2 200401010030 ## 3 200401010100 ## 4 200401010130 ## 5 200401010200 ## 6 200401010230 ## 7 200401010300 ## 8 200401010330 ## 9 200401010400 ## 10 200401010430 ## # … with 52,598 more rows We pipe the object df into the select() function with argument 1. Note that the pipe operator %&gt;% can be used on any function. It tells the function to interpret what’s coming from the left of %&gt;% as its first argument. A recent new version of {base} R also includes a pipe operator |&gt; that you may encounter in code snippets online and in this course. For the remainder of the tutorial several variables will be required. The methods of data selection demonstrated above will be used below to get the desired variables. df_small &lt;- df %&gt;% select(TIMESTAMP_START, TA_F, PPFD_IN) Note: In the code above, an indentation was used to highlight which parts go together, which makes the code easy to understand. Indentations and line breaks have no meaning or function in R per se (unlike in other programming languages, e.g., Matlab, Python), but help to make the code easier to read. 2.2.3.4 Renaming TIMESTAMP_START, TA_F and PPFD_IN as variable names may be hard to remember and in this section you will have to type them a lot. Therefore we change their names to something more intelligible. df_small &lt;- df_small %&gt;% rename(time = TIMESTAMP_START, temp = TA_F, ppfd = PPFD_IN) 2.2.3.5 Writing data A data frame can be written to a CSV file by: write_csv(df_small, file = &quot;data/df_small.csv&quot;) Note that making a file publicly available as a .rds or .RData file violates the open science principles. It is not interoperable. Therefore, whenever possible, save your data in a format that is readable across platforms without requiring proprietary software. Hence use write_csv() whenever possible. We will encounter other non-proprietary formats that let you save and share more complex data structures in Chapter 5. 2.2.3.6 Intro to visualisation Visualising data is an integral part of any data science workflow. In this section, we introduce just the very basics. In Chapter 4, you will get introduced to additional methods for visualising data. Our data frame fluxes_subset contains three variables, one of which is time. In other words, we are dealing with a time series. Let’s look at the temporal course of temperature in the first 1440 time steps (corresponding to 30 days) as a line plot (type = \"l\"). plot(1:1440, df_small$temp[1:1440], type = &quot;l&quot;) Another useful way of looking, not at a temporal course, but rather at the distribution of your data, is to display a histogram. A histogram visualises the frequency or proportion of data that has a metric value that falls within a certain interval known as a ‘bin’. Below you will see the temperature on the x-axis split into these ‘bins’ ranging across 2°. The number of times a data point falls between say 2° to 4° is then tallied and displayed as the frequency on the y-axis. Here there are around 1500 temperature values between 2° and 4°. hist(df_small$temp, xlab = &quot;Temperature (°C)&quot;) Plots can be saved as files, as long as the file size does not get too large. It will write vector graphics as outputs, i.e. PDF. In base R, this can be done by: pdf(&quot;./figures/filename.pdf&quot;) hist(df_small$temp) 2.2.4 Where to find help The material covered in this course will give you a solid basis for your future projects. Even more so, it provides you with code examples that you can adapt to your own purposes. Naturally, you will face problems we did not cover in the course and you will need to learn more as you go. The good news is, you do not have to. Many people make their code available online and often others have faced similar problems. Modifying existing code might make it easier for you to get started. 2.2.4.1 Within R “I know the name of a function that might help solve the problem but I do not know how to use it.” Typing a ? in front of the function will open the documentation of the function, giving lots of information on the uses and options a function has. You have learned a few things about plots but you may not know how to make a boxplot: ?boxplot Running the above code will open the information on making boxplots in R. If you do know how a function works but need to be reminded of the arguments it takes, simply type: args(boxplot) “There must be a function that does task X but I do not know which one.” Typing ?? will call the function help.search(). Maybe you want to save a plot as a JPEG but you do not know how: ??jpeg Note that it only looks through your installed packages. 2.2.4.2 Online To search in the entire library of R go to the website rdocumentation.org or turn to a search engine of your choice. It will send you to the appropriate function documentation or a helpful forum where someone has already asked a similar question. Most of the time you will end up on stackoverflow.com, a forum where most questions have already been answered. 2.2.4.3 Error messages If you do not understand the error message, start by searching the web. Be aware, that this is not always useful as developers rely on the error catching provided by R. To be more specific add the name of the function and package you are using, to get a more detailed answer. 2.2.4.4 Asking for help If you cannot find a solution online, start by asking your friends and colleagues. Someone with more experience than you might be able and willing to help you. When asking for help it is important to think about how you state the problem. The key to receiving help is to make it as easy as possible to understand the issue your facing. Try to reduce what does not work to a simple example. Reproduce a problem with a simple data frame instead of one with thousands of rows. Generalize it in a way that people who do not do research in your field can understand the problem. If you are asking a question online in a forum include the output of sessionInfo() (it provides information about the R version, packages your using,…) and other information that can be helpful to understand the problem. Stackoverflow has its own guidelines on how to ask a good question, which you should follow. Here’s a great template you should use for R-specific question. If your question is well crafted and has not been answered before you can sometimes get an answer within 5 minutes. Finally, many packages have a mailing list or allow you to open a query on the code repository, where you can ask specific questions. The same is true for R itself. The R-Help mailing list is read by many people. However, the tone of such mailing lists can be pretty dry and unwelcoming to new users. Be sure to use the right terminology or else you might get an answer pointing out your misuse of language instead of your problem. Also, be sure your question is valid. Or else you won’t get an answer. 2.3 Exercises Use a for loop to compute the sum of all natural numbers from 1 to 100. Print the result to the screen. Repeat this exercise but use a while loop. Add up all numbers between 1 and 100 that are at the same time a multiple of 3 and a multiple of 7. Print the result to the screen in the form of: The sum is of multiples of 3 and 7 within 1-100 is: {your result}. Define a vector \\(\\vec{v}\\) of length 100. Define the vector so that \\(v_i = 6\\), for \\(i = 1 : 25\\) and \\(v_i = -20\\), for \\(i = 66 : 100\\). Linearly interpolate the remaining elements that are not defined. Plot the values of \\(\\vec{v}\\). A powerful capability of loops is that you can nest them! Let’s try this out. Define a dataframe with the code matrix(c(1,2,3,4,5,6), nrow = 2 ,ncol = 3), write an outer loop that iterates over the numbers of columns and an inner loop that iterates over the number of rows. Write the code so that the sum of each column is printed to the screen. Hints Useful functions (find out more with ?function_name: print(), paste(), seq(), plot(), ncol(), nrow(), read.table(), hear() Logical operators are covered in the previous tutorial. If you started an endlessly running loop, pres Esc on your keyboard or click the little red “stop” pictogram in the RStudio console. To find out whether any number is another number’s multiple, use conditional statements in combination with the modulo-operator %%. The modulo operator returns the remainder of a division, for example 8 %% 3 = 2. For the linear interpolation, find and sequentially add the increment that you need to to add to each previous value to get from the first undefined position to the last defined position. "],["datawrangling.html", "Chapter 3 Data wrangling 3.1 Learning objectives 3.2 Tutorial 3.3 Extra material 3.4 Exercises 3.5 Report Exercise", " Chapter 3 Data wrangling Chapter lead author: Benjamin Stocker 3.1 Learning objectives 3.2 Tutorial Exploratory data analysis - the transformation, visualization, and modelling of data - is the central part of any (geo-) data science workflow and typically takes up a majority of the time we spend on a research project. The transformation of data often turns out to be particularly (and often surprisingly) time-demanding. Therefore, it is key to master typical steps of data transformation, and to implement them in a transparent fashion and efficiently - both in terms of robustness against coding errors (“bugs”) and in terms of code execution speed. This chapter introduces typical transformation steps (the reduction, filtering, aggregation, and combination of data), applied to tabular data, and implemented using the R tidyverse “dialect” (see below). We refer to data wrangling here to encompass the steps for preparing the data set prior to modelling - including, the combination of variables from different data sources, the removal of bad data, and the aggregation of data to the desired resolution or granularity (e.g., averaging over all time steps in a day, or over all replicates in a sample). In contrast, pre-processing refers to the additional steps that are either required by the the specific machine learning algorithm used with the data (e.g., centering and scaling for K-Nearest Neighbors or Neural Networks), the gap-filling of variables, or the transformation of variables guided by the resulting improvement of the predictive power of the machine learning model. Pre-processing is part of the modelling workflow and includes all steps that apply transformations that use parameters derived from the data. We will introduce and discuss data pre-processing in Chapter 9. 3.2.1 Example data The example data used in this chapter are parallel time series of (gaseous) CO\\(_2\\) and water vapor exchange fluxes between the vegetation and the atmosphere, along with various meteorological variables measured in parallel. Quasi-continuous measurements of temporally changing gas exchange fluxes are measured with the eddy covariance technique which relies on the parallel quantification of vertical wind speeds and gas concentrations. The data is provided at half-hourly resolution for the site CH-Lae, located on the south slope of the Lägern mountain on the Swiss Plateau at 689 m a.s.l. in a mixed forest with a distinct seasonal course of active green leaves (a substantial portion of the trees in the measured forest are deciduous). The dataset is generated and formatted following standard protocols (FLUXNET2015). For more information of the variables in the dataset, see the FLUXNET2015 website and Pastorello et al., 2020 for a comprehensive documentation of variable definitions and methods. For our demonstrations, the following variables are the most relevant: TIMESTAMP_START: Hour and day of the start of the measurement period for which the respective row’s data is representative. Provided in a format of “YYYYMMDDhhmm”. TIMESTAMP_END: Hour and day of the end of the measurement period for which the respective row’s data is representative. Provided in a format of “YYYYMMDDhhmm”. TA_* (°C): Air temperature. SW_IN_* (W m\\(^{-2}\\)): Shortwave incoming radiation LW_IN_* (W m\\(^{-2}\\)): Longwave incoming radiation VPD_* (hPa): Vapor pressure deficit (the difference between actual and saturation water vapor pressure) PA_* (kPa): Atmospheric pressure P_* (mm): Precipitation WS_* (m \\(^{-1}\\)): Wind speed SWC_* (%): Volumetric soil water content GPP_* (\\(\\mu\\)mol CO\\(_2\\) m\\(^{-1}\\) s\\(^{-1}\\)): Gross primary production (the ecosystem-level gross CO\\(_2\\) uptake flux driven by photosynthesis) *_QC: Quality control information for the variable *. Important for us: NEE_*_QC is the quality control information for the net ecosystem CO\\(_2\\) exchange flux (NEE_*) and for GPP derived from the corresponding NEE estimate (GPP_*). 0 = measured, 1 = good quality gap-filled, 2 = medium, 3 = poor. Suffixes _* indicate that multiple estimates for respective variables are available and distinguished by different suffixes. For example, variables TA_* contain the same information, but are derived with slightly different assumptions and gap-filling techniques. The meanings of suffixes are described in Pastorello et al., 2020. 3.2.2 Tidyverse The tidyverse is a collection of R packages and functions that share a common design philosophy, enabling a particularly efficient implementation of transformation steps on tabular data. The most important data and function design principle of the tidyverse is that each function takes a data frame as its first argument and returns a data frame as its output. From this design principles, even the most convoluted code and implementation of data transformation steps fall into place and fast and error-free progression through exploratory data analysis is facilitated. Therefore, you will be introduced to the R tidyverse here and we heavily rely on this dialect of the R language throughout the remainder of this course. 3.2.3 Tabular data Tabular data is organised in rows and columns. Each column can be regarded as a vector of a certain type. Each row contains the same number of columns and each column contains the same type of values (for example numeric, or characters). Each row can be regarded as a separate instance of the same type, for example a record of simultaneously taken measurements, along with some attributes. For example, a data frame in R is tabular data. In Chapter 5, you will be introduced to other types of data. The most common format for tabular data is CSV (comma-separated-values), typically indicated by the file name suffix .csv. CSV is a text-based file format, readable across platforms and does not rely on proprietary software (as opposed to, for example, .xlsx). The first row in a CSV file typically specifies the name of the variable provided in the respective column. Let’s get started with working with our example data set and read it into R, as the variable hhdf. Note that the naming of variables can be important for keeping code legible. For example, it may indicate the type of the object it holds (df for data frame) and that the data is half-hourly (hh). hhdf &lt;- read_csv(&quot;./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv&quot;) ## Rows: 52608 Columns: 235 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (235): TIMESTAMP_START, TIMESTAMP_END, TA_F_MDS, TA_F_MDS_QC, TA_ERA, TA... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. hhdf ## # A tibble: 52,608 × 235 ## TIMEST…¹ TIMES…² TA_F_…³ TA_F_…⁴ TA_ERA TA_F TA_F_QC SW_IN…⁵ SW_IN…⁶ SW_IN…⁷ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.00e11 2.00e11 -9999 -9999 -2.22 -2.22 2 0 -9999 -9999 ## 2 2.00e11 2.00e11 -9999 -9999 -2.25 -2.25 2 0 -9999 -9999 ## 3 2.00e11 2.00e11 -9999 -9999 -2.28 -2.28 2 0 -9999 -9999 ## 4 2.00e11 2.00e11 -9999 -9999 -2.50 -2.50 2 0 -9999 -9999 ## 5 2.00e11 2.00e11 -9999 -9999 -2.72 -2.72 2 0 -9999 -9999 ## 6 2.00e11 2.00e11 -9999 -9999 -2.94 -2.94 2 0 -9999 -9999 ## 7 2.00e11 2.00e11 -9999 -9999 -3.17 -3.17 2 0 -9999 -9999 ## 8 2.00e11 2.00e11 -9999 -9999 -3.39 -3.39 2 0 -9999 -9999 ## 9 2.00e11 2.00e11 -9999 -9999 -3.61 -3.61 2 0 -9999 -9999 ## 10 2.00e11 2.00e11 -9999 -9999 -3.59 -3.59 2 0 -9999 -9999 ## # … with 52,598 more rows, 225 more variables: SW_IN_ERA &lt;dbl&gt;, SW_IN_F &lt;dbl&gt;, ## # SW_IN_F_QC &lt;dbl&gt;, LW_IN_F_MDS &lt;dbl&gt;, LW_IN_F_MDS_QC &lt;dbl&gt;, LW_IN_ERA &lt;dbl&gt;, ## # LW_IN_F &lt;dbl&gt;, LW_IN_F_QC &lt;dbl&gt;, LW_IN_JSB &lt;dbl&gt;, LW_IN_JSB_QC &lt;dbl&gt;, ## # LW_IN_JSB_ERA &lt;dbl&gt;, LW_IN_JSB_F &lt;dbl&gt;, LW_IN_JSB_F_QC &lt;dbl&gt;, ## # VPD_F_MDS &lt;dbl&gt;, VPD_F_MDS_QC &lt;dbl&gt;, VPD_ERA &lt;dbl&gt;, VPD_F &lt;dbl&gt;, ## # VPD_F_QC &lt;dbl&gt;, PA &lt;dbl&gt;, PA_ERA &lt;dbl&gt;, PA_F &lt;dbl&gt;, PA_F_QC &lt;dbl&gt;, P &lt;dbl&gt;, ## # P_ERA &lt;dbl&gt;, P_F &lt;dbl&gt;, P_F_QC &lt;dbl&gt;, WS &lt;dbl&gt;, WS_ERA &lt;dbl&gt;, WS_F &lt;dbl&gt;, … Since the file is properly formatted, with variable names given in the first line of the file, the function read_csv() identifies them correctly as column names and interprets values in each column as values of a consistent type. We used the function read_csv() from the {readr} package (part of tidyverse) here for reading the CSV since it is faster than the base-R read.csv() and generates a nicely readable output when printing the object as is done above. 3.2.4 Variable selection For our further data exploration, we will reduce the data frame we are working with and select a reduced set of variables. Reducing the dataset can have the advantage of speeding up further processing steps, especially when the data is large. For the further steps in this chapter we will now subset our original data. We select the following variants of variables described above, plus some additional variables (further information in Pastorello et al., 2020): All variables with names starting with TIMESTAMP) All meteorological variables derived following the “final gap-filled method”, as indicated with names ending with _F. GPP estimates that are based on the nighttime decomposition method, using the “most representative” of different gap-filling versions, after having applied the variable u-star filtering method (GPP_NT_VUT_REF) and the corresponding quality control information (NEE_VUT_REF_QC) Soil water measured at different depths (variables starting with SWC_F_MDS_) Do not use any radiation variables derived with the “JSBACH” algorithm (not with a name that contains the string JSB) Flag indicating whether a time step is at night (NIGHT) This is implemented by: hhdf &lt;- select( hhdf, starts_with(&quot;TIMESTAMP&quot;), ends_with(&quot;_F&quot;), GPP_NT_VUT_REF, NEE_VUT_REF_QC, starts_with(&quot;SWC_F_MDS_&quot;), -contains(&quot;JSB&quot;), NIGHT ) This reduces our dataset from 235 available variables to 59 variables. As you can see, select() is a powerful tool to apply multiple selection criteria on your data frame in one step. It takes many functions that make filtering the columns easier. For example, criteria can be formulated based on the variable names with starts_with(), ends_with, contains(), matches(), etc. Using these functions within select() can help if several column names start with the same characters or contain the same pattern and all need to be selected. If a minus (-) is added in front of a column name or one of the mentioned functions within select(), then R will not include the stated column(s). Note that the selection criteria are evaluated in the order we write them in the select() function call. You can find the complete reference for selecting variables here. 3.2.5 Time objects The automatic interpretation of the variables TIMESTAMP_START and TIMESTAMP_END by the function read_csv() is not optimal: class(hhdf$TIMESTAMP_START[[1]]) ## [1] &quot;numeric&quot; as.character(hhdf$TIMESTAMP_START[[1]]) ## [1] &quot;200401010000&quot; As we can see, it is considered by R as a numeric variable with 12 digits (“double-precision”, occupying 64 bits in computer memory). After printing the variable as a string, we can guess that the format is: YYYYMMDDhhmm. The {lubridate} (R-lubridate?) package is designed to facilitate processing date and time objects. Knowing the format of the timestamp variables in our dataset, we can use ymd_hm() to convert them to actual date-time objects. dates &lt;- ymd_hm(hhdf$TIMESTAMP_START) dates[1] ## [1] &quot;2004-01-01 UTC&quot; Working with such date-time objects facilitates typical operations on time series. For example, adding one day can be done by: nextday &lt;- dates + days(1) nextday[1] ## [1] &quot;2004-01-02 UTC&quot; The following returns the month of each date object: month(dates[1]) ## [1] 1 The number 1 stands for the month of the year, i.e., January. You can find more information on formatting dates and time within the {tidyverse} here, and a complete reference of the {lubridate} package is available here. 3.2.6 Variable (re-) definition Since read_csv() did not interpret the TIMESTAMP_* variables as desired, we may convert the entire column in the data frame into a date-time object. In base-R, we would do this by: hhdf$TIMESTAMP_START &lt;- ymd_hm(hhdf$TIMESTAMP_START) Modifying existing or creating new variables (columns) in a data frame is done in the tidyverse using the function mutate(). The equivalent statement is: hhdf &lt;- mutate(hhdf, TIMESTAMP_START = ymd_hm(TIMESTAMP_START)) Note that in the code chunk above, the function mutate() is from the tidyverse package {dplyr}. It takes a dataframe as its first argument (here hhdf) and returns a dataframe as its output. You will encounter an alternative, but equivalent, syntax in the following form: hhdf &lt;- hhdf |&gt; mutate(TIMESTAMP_START = ymd_hm(TIMESTAMP_START)) Here, the pipe operator |&gt; is used. It “pipes” the object evaluated on its left side into the function on its right side, where the object takes the place of (but is not spelled out as) the first argument of that function. Using the pipe operator can have the advantage of facilitating the separation, removal, inserting, or re-arranging of individual transformation steps. Arguably, it facilitates reading code, especially for complex data transformation workflows. Therefore, you will encounter the pipe operator frequently throughout the remainder of this course. Mutating both our timestamp variables could be written as mutate(TIMESTAMP_START = ymd_hm(TIMESTAMP_START), TIMESTAMP_END = ymd_hm(TIMESTAMP_END)). Sometimes, such multiple-variable mutate statements can get quite long. A handy short version of this can be implemented using across(): hhdf &lt;- hhdf |&gt; mutate(across(starts_with(&quot;TIMESTAMP_&quot;), ymd_hm)) We will encounter more ways to use mutate later in this tutorial. A complete reference to mutate() is available here. 3.2.7 Axes of variation Tabular data is two-dimensional (rows \\(\\times\\) columns), but not all two-dimensional data is tabular. For example, raster data is a two-dimensional array of data (a matrix) representing variables on an evenly spaced grid, for example pixels in remotely sensed imagery. For example the volcano data (provided as an example dataset in R) is a 2-dimensional array, each column contains the same variable, and no variable names are provided. volcano[1:5, 1:5] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 100 100 101 101 101 ## [2,] 101 101 102 102 102 ## [3,] 102 102 103 103 103 ## [4,] 103 103 104 104 104 ## [5,] 104 104 105 105 105 In the volcano dataset, rows and columns represent different geographic positions in latitude and longitude, respectively. The volcano data is not tabular data. Another typical example for non-tabular data are climate model outputs. They are typically given as arrays with more than two dimensions. Typically, this is longitude, latitude, and time, and sometimes a vertical dimension representing, for example, elevation. Such data is multi-dimensional and, as such, not tabular. Tabular data, although formatted in two dimensions by rows and columns, may represent data that varies along multiple axes. Most environmental data is structured, that is, values of “nearby” observations tend to be more similar than values of “distant” observations. Here, “nearby” and “distant” may refer to a spatial distance, but not necessarily so. Structure in data arises from similarity of the subjects generating the data (e.g., evapotranspiration over two croplands may be more similar than evapotranspiration over a forest), or from temporal proximity. In biological data, there may be a genetic structure arising from evolutionary relatedness (Roberts et al., 2016). Note also that temporal proximity is more complex than than being governed by a single dimension - time. In environmental data, time is often expressed through periodically varying conditions (the diurnal and seasonal cycles). It’s often critical to understand and account for the structure in data when analysing it and using it for model fitting. Challenges are posed when structure is not apparent or not known. Note also that some structures are hierarchical. For example, data may be structured by postal codes within cantons; or by hours within a day within a year). Biological data may be generated by species within genera within families. Data from experiments is typically structured as samples within treatments. Etc. You see, structure in data is rather the rule than the expception. Our example data contains values recorded at each half-hourly time interval over the course of eleven years (check by nrow(hhdf)/(2*24*365)). The data is recorded at a site, located in the temperate climate zone, where solar radiation and therefore also other meteorological variables and ecosystem fluxes vary substantially over the course of a day and over the course of a year. Although not explicitly separated, the date-time object thus encodes information along multiple axes of variation in the data. For example, over the course of one day (2*24 rows in our data), the shortwave incoming radiation SW_IN_F varies over a typical diurnal cycle: plot(hhdf[1:(2*24),]$TIMESTAMP_START, hhdf[1:(2*24),]$SW_IN_F, type = &quot;l&quot;) Over the course of an entire year, shortwave incoming radiation varies with the seasons, peaking in summer: plot(hhdf[1:(365*2*24),]$TIMESTAMP_START, hhdf[1:(365*2*24),]$SW_IN_F, type = &quot;l&quot;) All data frames have two dimensions, rows and columns. Our data frame is organised along half-hourly time steps in rows. As described above, these time steps belong to different days, months, and years, although these “axes of variation” are not reflected by the structure of the data frame and we do not have columns that indicate the day, month or year of each half-hourly time step. This would be redundant information since the date-time objects of columns TIMESTAMP_* contain this information. However, for certain applications, it may be useful to separate information regarding these axes of variation more explicitly. For example by: hhdf |&gt; mutate(year = year(TIMESTAMP_START), month = month(TIMESTAMP_START)) |&gt; select(TIMESTAMP_START, TIMESTAMP_END, year, month) # for displaying ## # A tibble: 52,608 × 4 ## TIMESTAMP_START TIMESTAMP_END year month ## &lt;dttm&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2004-01-01 00:00:00 2004-01-01 00:30:00 2004 1 ## 2 2004-01-01 00:30:00 2004-01-01 01:00:00 2004 1 ## 3 2004-01-01 01:00:00 2004-01-01 01:30:00 2004 1 ## 4 2004-01-01 01:30:00 2004-01-01 02:00:00 2004 1 ## 5 2004-01-01 02:00:00 2004-01-01 02:30:00 2004 1 ## 6 2004-01-01 02:30:00 2004-01-01 03:00:00 2004 1 ## 7 2004-01-01 03:00:00 2004-01-01 03:30:00 2004 1 ## 8 2004-01-01 03:30:00 2004-01-01 04:00:00 2004 1 ## 9 2004-01-01 04:00:00 2004-01-01 04:30:00 2004 1 ## 10 2004-01-01 04:30:00 2004-01-01 05:00:00 2004 1 ## # … with 52,598 more rows Note that we used mutate() here to create a new variable (column) in the data frame, as opposed to above where we overwrote an existing variable with the same function. 3.2.8 Tidy data Data comes in many forms and shapes. For example, Excel provides a playground for even the wildest layouts of information in some remotely tabular form and merged cells as we will see in the Exercises. A data frame imposes a relatively strict formatting in named columns of equal length. But even data frames can come in various shapes - even if the information they contain is the same. df1 ## # A tibble: 36 × 3 ## year month co2_concentration ## &lt;int&gt; &lt;ord&gt; &lt;dbl&gt; ## 1 1959 Jan 315. ## 2 1959 Feb 316. ## 3 1959 Mar 316. ## 4 1959 Apr 318. ## 5 1959 May 318. ## 6 1959 Jun 318 ## 7 1959 Jul 316. ## 8 1959 Aug 315. ## 9 1959 Sep 314. ## 10 1959 Oct 313. ## # … with 26 more rows df2 ## # A tibble: 3 × 13 ## year Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1959 315. 316. 316. 318. 318. 318 316. 315. 314. 313. 315. 315. ## 2 1960 316. 317. 317. 319. 320. 319. 318. 316. 314 314. 315. 316. ## 3 1961 317. 318. 318. 319. 320. 320. 318. 317. 315. 315. 316. 317. df3 ## # A tibble: 12 × 4 ## month `1959` `1960` `1961` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jan 315. 316. 317. ## 2 Feb 316. 317. 318. ## 3 Mar 316. 317. 318. ## 4 Apr 318. 319. 319. ## 5 May 318. 320. 320. ## 6 Jun 318 319. 320. ## 7 Jul 316. 318. 318. ## 8 Aug 315. 316. 317. ## 9 Sep 314. 314 315. ## 10 Oct 313. 314. 315. ## 11 Nov 315. 315. 316. ## 12 Dec 315. 316. 317. There are advantages for interoperability and ease of use when data frames come with consistent layouts, adhering to certain design principles. We have learned that in tabular data, each row contains the same number of columns and each column contains the same type of values (for example numeric, or characters). And that each row can be regarded as a separate instance of the same type, for example a record of simultaneously taken measurements, along with some attributes. Following these principles strictly leads to tidy data. In essence, quoting Wickham, data is tidy if: Each variable has its own column. Each observation has its own row. Each value has its own cell. The concept of tidy data can even be taken further by understanding a “value” as any object type, e.g. a list or a data frame. This leads to a list or data frame “nested” within a data frame. You will learn more about this below. The concept of tidy data can even be taken further by understanding a “value” as any object type, e.g. a list or a data frame. This leads to a list or data frame “nested” within a data frame. You will learn more about this below. 3.2.9 Aggregating data Aggregating data refers to collapsing a larger set of values into a smaller set of values that are derived from the larger set. For example, we can aggregate over all \\(N\\) rows in a data frame (\\(N\\times M\\)), calculating the sum for each of the \\(M\\) columns. This returns a data frame (\\(1 \\times M\\)) with the same number of columns as the initial data frame, but only one row. Often, aggregations are done not across all rows but for rows within \\(G\\) groups of rows. This yields a data frame (\\(G \\times M\\)) with the number of rows corresponding to the number of groups. Let’s say we want to calculate the mean of half-hourly shortwave radiation within each day. That is, to aggregate our half-hourly data to daily data by taking a mean. There are two pieces of information needed for an aggregation step: The factor (or “axis of variation”), here days, that groups a vector of values for collapsing it into a single value, and the function used for collapsing values, here, the mean() function. This function should take a vector as an argument and return a single value as an output. These two steps are implemented by the {dplyr} functions group_by() and summarise(). The entire aggregation workflow is implemented by the following code: ddf &lt;- hhdf |&gt; mutate(date = as_date(TIMESTAMP_START)) |&gt; # converts the ymd_hm-formatted date-time object to a date-only object (ymd) group_by(date) |&gt; summarise(SW_IN_F = mean(SW_IN_F)) The seasonal course can now be more clearly be visualized with the data aggregated to daily values. plot(ddf[1:365,]$date, ddf[1:365,]$SW_IN_F, type = &quot;l&quot;) We can also apply multiple aggregation functions to different variables simultaneously. In the example below, we aggregate half-hourly data to daily data by… taking the daily mean GPP counting the number of half-hourly data points by day counting the number of measured (not gap-filled) data points taking the mean shortwave radiation Finally, we calculate the fraction of measured underlying half-hourly data from which the aggregation is calculated and we save the daily data frame as a CSV file for later use. ddf &lt;- hhdf |&gt; mutate(date = as_date(TIMESTAMP_START)) |&gt; # converts time object to a date object group_by(date) |&gt; summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE), n_datapoints = n(), # counts the number of observations per day n_measured = sum(NEE_VUT_REF_QC == 0), # counts the number of actually measured data (excluding gap-filled and poor quality data) SW_IN_F = mean(SW_IN_F, na.rm = TRUE), # we will use this later .groups = &#39;drop&#39; # to un-group the resulting data frame ) |&gt; mutate(f_measured = n_measured / n_datapoints) # calculate the fraction of measured values over total observations write_csv(ddf, file = &quot;data/ddf.csv&quot;) ddf ## # A tibble: 1,096 × 6 ## date GPP_NT_VUT_REF n_datapoints n_measured SW_IN_F f_measured ## &lt;date&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2004-01-01 -0.0138 48 0 38.1 0 ## 2 2004-01-02 0.768 48 0 23.9 0 ## 3 2004-01-03 0.673 48 0 54.1 0 ## 4 2004-01-04 -0.322 48 0 41.7 0 ## 5 2004-01-05 0.841 48 0 17.4 0 ## 6 2004-01-06 1.22 48 0 40.5 0 ## 7 2004-01-07 0.215 48 0 31.6 0 ## 8 2004-01-08 1.11 48 0 58.4 0 ## 9 2004-01-09 1.44 48 0 11.9 0 ## 10 2004-01-10 0.364 48 0 27.6 0 ## # … with 1,086 more rows More info on how to group values using summarise functions here, or a summary on the inputs the function group_by() and summarise() take. Let’s Aggregating is related to nesting performed by the {tidyr} function nest(): hhdf |&gt; mutate(date = as_date(TIMESTAMP_START)) |&gt; group_by(date) |&gt; nest() ## # A tibble: 1,096 × 2 ## # Groups: date [1,096] ## date data ## &lt;date&gt; &lt;list&gt; ## 1 2004-01-01 &lt;tibble [48 × 20]&gt; ## 2 2004-01-02 &lt;tibble [48 × 20]&gt; ## 3 2004-01-03 &lt;tibble [48 × 20]&gt; ## 4 2004-01-04 &lt;tibble [48 × 20]&gt; ## 5 2004-01-05 &lt;tibble [48 × 20]&gt; ## 6 2004-01-06 &lt;tibble [48 × 20]&gt; ## 7 2004-01-07 &lt;tibble [48 × 20]&gt; ## 8 2004-01-08 &lt;tibble [48 × 20]&gt; ## 9 2004-01-09 &lt;tibble [48 × 20]&gt; ## 10 2004-01-10 &lt;tibble [48 × 20]&gt; ## # … with 1,086 more rows Here, the data frame has one row per date and therefore the same number of rows as the data frame ddf, but the data itself is not reduced by a summarising function. Instead, the data is kept at the half-hourly level, but it’s nested inside the new column data, which now contains a list of half-hourly data frames for each day. This is just a brief perspective of what nesting is about. More is explained in the Section 3.3 Extra material below. More comprehensive tutorials on nesting and functional programming are available in Altman, Behrman and Wickham (2021) or in Wickham &amp; Grolemund (2017), Chapter 21. 3.2.10 Data cleaning Data cleaning is often a time-consuming task and decisions taken during data cleaning may be critical for analyses and modelling. In the following, we distinguish between cleaning formats, the identification (and removal) of “bad” data, and the gap-filling of missing or removed data. An excellent resource for further reading is the Quartz Guide to Bad Data which provides an overview of how to deal with different types of bad data. 3.2.10.1 Cleaning formats As a general principle, we want to have machine readable data. Key for achieving machine-readability is that a cell should only contain one value of one type. Hence, for example, character strings should be kept in separate columns (as separate variables) from numeric data. Character strings can impose particular challenges for achieving machine-readability. Typically, they encode categorical or ordinal information, but are prone to spelling inconsistencies or errors that undermine the ordering or categorization. Here are typical examples for challenges working with character strings and lessons for avoiding problems: Often, character strings encode the units of a measurement, and entries are c(\"kg m-2\", \"kg/m2\", \"Kg / m2\", \"1000 g m-2\") . They are all equivalent, but “the machine” treats them as non-identical. To clean such data, one may compile a lookup-table to identify equivalent (but not identical) strings. Much better is to specify a consistent treatment of units before data collection. Even if the data is clean and contains a consistently spelled categorical variable in the form of a character string, R doesn’t necessarily treat it as categorical. For certain downstream steps of the workflow, it may be necessary to transform such a variable to one of type factor. For example, as entries of an unordered categorical variable, we have unique(df$gender) = c(\"female\", \"male\", \"non-binary\"). To treat them as categorical and not just mere character strings, we would have to do: df &lt;- df |&gt; mutate(gender = as.factor(gender)) Character strings may encode ordinal information. For example, entries specify quality control information and are one of c(\"good quality\", \"fair quality, \"poor quality\"). A challenge could be that the spelling is inconsistent (c(\"Good quality\", \"good quality\", …)). Using integers (positive natural numbers) instead of character strings avoids such challenges and enforces an order. The quality control variable NEE_VUT_REF_QC in our example dataset hhdf follows this approach: unique(hhdf$NEE_VUT_REF_QC) ## [1] 3 2 1 0 An entry like &gt;10 m is not a friend of a data scientist. Here, we have three pieces of information: &gt; as in “greater than”, 10, and m indicating the units. A machine-readable format would be obtained by creating separate columns for each piece of information. The &gt; should be avoided already at the stage of recording the data. Here, we may have to find a solution for encoding it in a machine readable manner (see Exercises). String manipulations are usually required for cleaning data. The section Strings below demonstrates some simple examples. Note that a majority of machine learning algorithms and other statistical model types require all data to be numeric. Methods exist to convert categorical data into numeric data, as we will learn later. We re-visit data cleaning in the form of data preprocessing as part of the modelling workflow in Chapter 9. 3.2.10.2 Bad data Data may be “bad” for different reasons, including sensor error, human error, a data point representing a different population, or unsuitable measurement conditions. In this sense, data is “bad” if it doesn’t represent what it is assumed to represent. Its presence in analyses and modelling may undermine the model skill or even lead to spurious results. A goal of data cleaning typically is to remove bad data. But how to detect them? And how safe is it to remove them? A diversity of processes may generate bad data and it is often not possible to formulate rules and criteria for their identification a priori. Therefore, an understanding of the data and the data generation processes is important for the identification and treatment of bad data. Often, such an understanding is gained by repeated exploratory data analysis cycles, involving the visualization, transformation, and analysis of the data. Ideally, information about the quality of the data is provided as part of the dataset. Also other meta-information (e.g., sensor type, human recording the data, environmental conditions during the data collection) may be valuable for data cleaning purposes. In our example dataset, the column with suffices _QC provide such information (see ‘Example data’ section above) and an example for their use in data cleaning is given further below. Bad data may come in the form of outliers, which are commonly defined based on their value with respect to the distribution of all values of the same variable in a dataset. Hence, their identification most commonly relies on quantifying their distance from the center of the variable’s empirical distribution. The default boxplot() plotting function in R (which we will learn about more in Chapter @ref(data_vis)) shows the median (bold line in the center), the upper and lower quartiles (corresponding to the 25% and the 75% quantiles, often referred to as \\(Q_1\\) and \\(Q_3\\) , given by the upper and lower edge of the box plot) and the range of \\(( Q_1 - 1.5 (Q_3 - Q_1), Q_3 + 1.5 (Q_3 - Q_1))\\). Any point outside this range is plotted by a circle and labeled an “outlier”. However, this definition is very restrictive and may lead to a false labeling of outliers, in particular if they are drawn from a distribution with a fat tail or from asymmetrical distributions. Outliers may also be identified via multivariate distributions. We will re-visit such methods later, in Chapter 8. For certain applications, outliers or anomalies may be the target of the investigation, not the noise in the data. This has spurred the field of anomaly detection which relies on machine learning algorithms for determining whether a value is anomalous, given a set of covariates. Sensor error may generate spurious values, identified, for example when a continuous variable attains the numerically identical value with a spuriously high frequency. XXX spurious? hhdf$GPP_NT_VUT_REF |&gt; table() |&gt; sort(decreasing = TRUE) |&gt; head() ## ## 5.18422 3.54996 1.3107 -5.57199 0.984756 2.49444 ## 32 22 19 18 17 17 Other processes may lead to spurious trends or drift in the data, for example caused by sensor degradation. Spurious step changes or change points in time series or in (multivariate) regressions may be related to the replacement or deplacement of the measuring device. Different methods and R libraries help identifying such cases (see for example this tutorial). Solutions have to be found for the remediation of such spurious patterns in the data on a case-by-case basis. 3.2.10.3 Handling missing data The question about when data is “bad” and whether to remove it is often critical. Such decisions are important to keep track of and should be reported as transparently as possible in publications. In reality, where the data generation process may start in the field with actual human beings writing notes in a lab book, and where the human collecting the data is often not the same as the human analyzing the data or writing the paper, it’s often more difficult to keep track of such decisions. As a general principle, it is advisable to design data records such that decisions made during the data collection process remain transparent throughout all stages of the workflow and that sufficient information be collected to enable later revisions of particularly critical decisions. In practice, this means that the removal of data and entire rows should be avoided and implemented only at the very last step if necessary (e.g., when passing the data into a model fitting function). Instead, information about whether data is bad or not should be kept in a separate, categorical, variable (a quality control variable, like *_QC variables in our example data hhdf). Data may be missing for several reasons. Some yield random patterns of missing data, others not. In the latter case, we can speak of informative missingness (Kuhn &amp; Johnson, 2003) and its information can be used for modelling. For categorical data, we may replace such data with \"none\" (instead of NA), while randomly missing data may be dropped altogether. Some machine learning algorithms (mainly tree-based methods, e.g., Random Forest) can handle missing values. However, when comparing the performance of alternative ML algorithms, they should be tested with the same data and removing missing data should be done beforehand. Most machine learning algorithms require missing values to be removed. That is, if any of the cells in one row has a missing value, the entire cell gets removed. This generally leads to a loss of information contained in the remaining variables that are not missing. Methods exist to impute missing values in order to avoid this information loss. However, the gain of data imputation has to be traded off against effects of associating the available variables with the imputed (knowingly wrong) values, and effects of data leakage have to be considered. Data imputation as part of the modelling process will be dealt with in Chapter 9. In our example dataset, some values of SWC_F_MDS_* are given as -9999. hhdf |&gt; select(TIMESTAMP_START, starts_with(&quot;SWC_F_MDS_&quot;)) |&gt; head() ## # A tibble: 6 × 9 ## TIMESTAMP_START SWC_F_MD…¹ SWC_F…² SWC_F…³ SWC_F…⁴ SWC_F…⁵ SWC_F…⁶ SWC_F…⁷ ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2004-01-01 00:00:00 -9999 -9999 -9999 -9999 -9999 -9999 -9999 ## 2 2004-01-01 00:30:00 -9999 -9999 -9999 -9999 -9999 -9999 -9999 ## 3 2004-01-01 01:00:00 -9999 -9999 -9999 -9999 -9999 -9999 -9999 ## 4 2004-01-01 01:30:00 -9999 -9999 -9999 -9999 -9999 -9999 -9999 ## 5 2004-01-01 02:00:00 -9999 -9999 -9999 -9999 -9999 -9999 -9999 ## 6 2004-01-01 02:30:00 -9999 -9999 -9999 -9999 -9999 -9999 -9999 ## # … with 1 more variable: SWC_F_MDS_4_QC &lt;dbl&gt;, and abbreviated variable names ## # ¹​SWC_F_MDS_1, ²​SWC_F_MDS_2, ³​SWC_F_MDS_3, ⁴​SWC_F_MDS_4, ⁵​SWC_F_MDS_1_QC, ## # ⁶​SWC_F_MDS_2_QC, ⁷​SWC_F_MDS_3_QC When reading the documentation of this specific dataset, we learn that -9999 is the code for missing data. The {dplyr} functions help us to clarify these missing values by mutating across all numeric variables and overwrite entries with NA if they hold a -9999. hhdf &lt;- hhdf |&gt; mutate(across(where(is.numeric), ~na_if(., -9999))) hhdf |&gt; select(TIMESTAMP_START, starts_with(&quot;SWC_F_MDS_&quot;)) |&gt; head() ## # A tibble: 6 × 9 ## TIMESTAMP_START SWC_F_MD…¹ SWC_F…² SWC_F…³ SWC_F…⁴ SWC_F…⁵ SWC_F…⁶ SWC_F…⁷ ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2004-01-01 00:00:00 NA NA NA NA NA NA NA ## 2 2004-01-01 00:30:00 NA NA NA NA NA NA NA ## 3 2004-01-01 01:00:00 NA NA NA NA NA NA NA ## 4 2004-01-01 01:30:00 NA NA NA NA NA NA NA ## 5 2004-01-01 02:00:00 NA NA NA NA NA NA NA ## 6 2004-01-01 02:30:00 NA NA NA NA NA NA NA ## # … with 1 more variable: SWC_F_MDS_4_QC &lt;dbl&gt;, and abbreviated variable names ## # ¹​SWC_F_MDS_1, ²​SWC_F_MDS_2, ³​SWC_F_MDS_3, ⁴​SWC_F_MDS_4, ⁵​SWC_F_MDS_1_QC, ## # ⁶​SWC_F_MDS_2_QC, ⁷​SWC_F_MDS_3_QC This lets us visualise the data and its gaps with vis_miss() from the {visdat} package. Visualising missing data can be informative for making decisions about dropping rows with missing data versus removing predictors from the analysis (which would imply too much data removal). visdat::vis_miss( hhdf |&gt; slice(1:10000), cluster = FALSE, warn_large_data = FALSE ) For many applications, we want to filter the data so that the values of particular variables satisfy certain conditions. The {dplyr} function used for such tasks is filter(). As argument, it takes the expressions that specify the criterion for filtering using logical operators (&gt;, &gt;=, &lt;, ==, !-, ..., see Chapter @ref(getting_started)). Multiple filtering criteria can be combined with logical (boolean) operators: &amp;: logical AND |: logical OR ! logical NOT For example, if we wanted only those rows in our data where NEE is based on measured or good quality gap-filled NEE data, we write: hhdf |&gt; filter(NEE_VUT_REF_QC == 0 | NEE_VUT_REF_QC == 1) For evaluating multiple OR operations simultaneously, we can write alternatively and equivalently: hhdf |&gt; filter(NEE_VUT_REF_QC %in% c(0,1)) Note that filter() removes entire rows. In some cases this is undesired and it is preferred to replace bad quality values with NA. It is important to note that specifying a value as missing is information itself. Dropping an entire row leads to the loss of this information. For cases where we do not want to drop entire rows when applying filter(), we can just replace certain values with NA. In our case, where we want to retain only data where NEE is based on actual measurements or good quality gap-filling, we can do this by: hhdf |&gt; mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC %in% c(0,1), GPP_NT_VUT_REF, NA)) If we decide to drop a row containing NA in any of the variables later during the workflow, we can do this, for example using the useful {tidyr} function drop_na(). hhdf |&gt; drop_na() An excellent source for a more comprehensive introduction to missing data handling is given in Kuhn &amp; Johnson. After having applied some data reduction and cleaning steps above, let’s save the data frame in the form of a CSV file for use in later chapters. write_csv(hhdf, file = &quot;data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN.csv&quot;) 3.2.11 Combining relational data Often, data is spread across multiple files and tables and needs to be combined for the planned analysis. In the simplest case, data frames have a corresponding set of columns and we can “stack” them along rows: df4 ## # A tibble: 6 × 4 ## month `1959` `1960` `1961` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jan 315. 316. 317. ## 2 Feb 316. 317. 318. ## 3 Mar 316. 317. 318. ## 4 Apr 318. 319. 319. ## 5 May 318. 320. 320. ## 6 Jun 318 319. 320. df5 ## # A tibble: 6 × 4 ## month `1959` `1960` `1961` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jul 316. 318. 318. ## 2 Aug 315. 316. 317. ## 3 Sep 314. 314 315. ## 4 Oct 313. 314. 315. ## 5 Nov 315. 315. 316. ## 6 Dec 315. 316. 317. bind_rows(df4, df5) ## # A tibble: 12 × 4 ## month `1959` `1960` `1961` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jan 315. 316. 317. ## 2 Feb 316. 317. 318. ## 3 Mar 316. 317. 318. ## 4 Apr 318. 319. 319. ## 5 May 318. 320. 320. ## 6 Jun 318 319. 320. ## 7 Jul 316. 318. 318. ## 8 Aug 315. 316. 317. ## 9 Sep 314. 314 315. ## 10 Oct 313. 314. 315. ## 11 Nov 315. 315. 316. ## 12 Dec 315. 316. 317. …, or data frames have a corresponding set of rows (and in the same order) and we can “stack” them along columns. df6 ## # A tibble: 12 × 3 ## month `1959` `1960` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jan 315. 316. ## 2 Feb 316. 317. ## 3 Mar 316. 317. ## 4 Apr 318. 319. ## 5 May 318. 320. ## 6 Jun 318 319. ## 7 Jul 316. 318. ## 8 Aug 315. 316. ## 9 Sep 314. 314 ## 10 Oct 313. 314. ## 11 Nov 315. 315. ## 12 Dec 315. 316. df7 ## # A tibble: 12 × 2 ## month `1961` ## &lt;ord&gt; &lt;dbl&gt; ## 1 Jan 317. ## 2 Feb 318. ## 3 Mar 318. ## 4 Apr 319. ## 5 May 320. ## 6 Jun 320. ## 7 Jul 318. ## 8 Aug 317. ## 9 Sep 315. ## 10 Oct 315. ## 11 Nov 316. ## 12 Dec 317. bind_cols(df6, df7) ## New names: ## • `month` -&gt; `month...1` ## • `month` -&gt; `month...4` ## # A tibble: 12 × 5 ## month...1 `1959` `1960` month...4 `1961` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;dbl&gt; ## 1 Jan 315. 316. Jan 317. ## 2 Feb 316. 317. Feb 318. ## 3 Mar 316. 317. Mar 318. ## 4 Apr 318. 319. Apr 319. ## 5 May 318. 320. May 320. ## 6 Jun 318 319. Jun 320. ## 7 Jul 316. 318. Jul 318. ## 8 Aug 315. 316. Aug 317. ## 9 Sep 314. 314 Sep 315. ## 10 Oct 313. 314. Oct 315. ## 11 Nov 315. 315. Nov 316. ## 12 Dec 315. 316. Dec 317. But beware! In particular the stacking along columns (bind_cols()) is very error-prone. Since a tidy data frame regards each row as an instance of associated measurements, the rows of the two data frames and their order must match exactly. Otherwise, an error is raised or (even worse) rows get associated when they shouldn’t be. In such cases, where information about a common set of observations is distributed across multiple data objects, we are dealing with relational data. The key for their combination (or “merging”) is a unique identification key - the column that is present in both data frames and which contains values along which the merging of the two data frames is performed. In our example from above, this is month, and we can use the dplyr function left_join(). df6 |&gt; slice(sample(1:n(), replace = FALSE)) |&gt; # re-shuffling rows left_join(df7, by = &quot;month&quot;) |&gt; arrange(month) # sort in ascending order ## # A tibble: 12 × 4 ## month `1959` `1960` `1961` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jan 315. 316. 317. ## 2 Feb 316. 317. 318. ## 3 Mar 316. 317. 318. ## 4 Apr 318. 319. 319. ## 5 May 318. 320. 320. ## 6 Jun 318 319. 320. ## 7 Jul 316. 318. 318. ## 8 Aug 315. 316. 317. ## 9 Sep 314. 314 315. ## 10 Oct 313. 314. 315. ## 11 Nov 315. 315. 316. ## 12 Dec 315. 316. 317. Note that here, we first re-shuffled (permuted) the rows of df6 for demonstration purposes, and arranged the output data frame again by month - an ordinal variable. left_join() is not compromised by the order of the rows, but instead relies on the unique identification key, specified by the argument by = \"month\", for associating (merging, joining) the two data frames. In some cases, multiple columns may act as the unique identification key in their combination (for example by = c(\"year\", \"month\")). Other variants of *_join() are available as described here. 3.3 Extra material 3.3.1 Functional programming I Above, we read a CSV table into R and applied several data transformation steps. In practice, we often have to apply the same data transformation steps repeatedly over a set of similar objects. This extra material section outlines an example workflow for demonstrating how to efficiently work with lists of similar objects - in particular, lists of data frames. Our aim is to read a set of files into R data frames and apply transformation steps to each data frame separately. Here, we will work with daily data, not half-hourly data. The daily data contains largely identical variables with consistent naming and units as in the half-hourly data (description above). Let’s start by creating a list of paths that point to the files with daily data. They are all located in the directory \"./data\" and share a certain string of characters in their file names \"_FLUXNET2015_FULLSET_DD_\". vec_files &lt;- list.files(&quot;./data&quot;, pattern = &quot;_FLUXNET2015_FULLSET_DD_&quot;, full.names = TRUE) print(vec_files) ## [1] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; ## [2] &quot;./data/FLX_CH-Lae_FLUXNET2015_FULLSET_DD_2004-2014_1-4.csv&quot; ## [3] &quot;./data/FLX_FI-Hyy_FLUXNET2015_FULLSET_DD_1996-2014_1-3.csv&quot; ## [4] &quot;./data/FLX_FR-Pue_FLUXNET2015_FULLSET_DD_2000-2014_2-3.csv&quot; vec_files is now a vector of three files paths as character strings. To read in the three files and combine the three data frames (list_df below) into a list of data frames, we could use a for loop: list_df &lt;- list() for (ifil in vec_files){ list_df[[ifil]] &lt;- read_csv(ifil) } Repeatedly applying a function (here read_csv()) over a list similar objects is facilitated by the map*() family of functions from the {purrr} package. An (almost) equivalent statement is: list_df &lt;- map(as.list(vec_files), ~read_csv(.)) Here, map() applies the function read_csv() to elements of a list. Hence, we first have to convert the vector vec_files to a list. A list is always the first argument within the map() function. Note two new symbols (~ and .). The ~ always goes before the function that is repeatedly applied (or “mapped”) to elements of the list. The . indicates where the elements of the list would go if spelled out (e.g., here, read_csv(.) would be read_csv(\"./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv\") for the first iteration). The output of map() is again a list. There are many variants of the function map() that each have a specific use. A complete reference for all {purrr} functions is available here. A useful and more extensive tutorial on purrr is available here. The above map() call does not return a named list as our for loop created. But we can give each element of the returned list of data frames different names by: names(list_df) &lt;- vec_files # this makes it a named list Next, we will apply a similar data cleaning procedure to this data set as we did above for half-hourly data. To do so, we “package” the individual cleaning steps into a function … # function definition clean_data_dd &lt;- function(df){ df &lt;- df |&gt; # select only the variables we are interested in select( TIMESTAMP, ends_with(&quot;_F&quot;), GPP_NT_VUT_REF, NEE_VUT_REF_QC, starts_with(&quot;SWC_F_MDS_&quot;), -contains(&quot;JSB&quot;)) |&gt; # convert to a nice date object mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) |&gt; # set all -9999 to NA mutate(across(where(is.numeric), ~na_if(., -9999))) return(df) } … and apply this function to each data frame within our list of data frames: list_df &lt;- map(list_df, ~clean_data_dd(.)) Having different data frames as elements of a list may be impractical. Since we read in similarly formatted files and selected always the same variables in each data frame, all elements of the list of data frames list_df share the same columns. This suggests that we can collapse our list of data frames and “stack” data frames along rows. As described above, this can be done using bind_rows() and we can automatically create a new column \"siteid\" in the stacked data frame that takes the name of the corresponding list element. ddf_allsites &lt;- bind_rows(list_df, .id = &quot;siteid&quot;) ddf_allsites ## # A tibble: 23,011 × 21 ## siteid TIMESTAMP TA_F SW_IN_F LW_IN_F VPD_F PA_F P_F WS_F GPP_N…¹ ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ./data/FLX… 1997-01-01 -4.57 77.4 223. 0.565 82.6 0.4 0.559 0.697 ## 2 ./data/FLX… 1997-01-02 -3.34 45.6 235. 0.978 82.9 0 1.11 1.04 ## 3 ./data/FLX… 1997-01-03 0.278 74.1 239. 2.24 82.4 0 2.03 -0.242 ## 4 ./data/FLX… 1997-01-04 -1.88 58.1 250. 1.38 81.7 1.8 1.92 0.247 ## 5 ./data/FLX… 1997-01-05 -4.96 80.8 248. 1.16 82.3 0 0.407 0.520 ## 6 ./data/FLX… 1997-01-06 -4.48 59.6 237. 0.838 82.7 0 0.466 0.0182 ## 7 ./data/FLX… 1997-01-07 -3.15 45.5 234. 1.33 82.9 0 1.03 0.0777 ## 8 ./data/FLX… 1997-01-08 -2.45 76.7 222. 1.87 82.7 0 1.95 -0.484 ## 9 ./data/FLX… 1997-01-09 -2.43 47.6 251. 1.44 82.2 0 0.785 -0.379 ## 10 ./data/FLX… 1997-01-10 -3.09 39.6 242. 0.776 82.8 0 1.25 -0.552 ## # … with 23,001 more rows, 11 more variables: NEE_VUT_REF_QC &lt;dbl&gt;, ## # SWC_F_MDS_1 &lt;dbl&gt;, SWC_F_MDS_2 &lt;dbl&gt;, SWC_F_MDS_3 &lt;dbl&gt;, ## # SWC_F_MDS_1_QC &lt;dbl&gt;, SWC_F_MDS_2_QC &lt;dbl&gt;, SWC_F_MDS_3_QC &lt;dbl&gt;, ## # SWC_F_MDS_4 &lt;dbl&gt;, SWC_F_MDS_4_QC &lt;dbl&gt;, SWC_F_MDS_5 &lt;dbl&gt;, ## # SWC_F_MDS_5_QC &lt;dbl&gt;, and abbreviated variable name ¹​GPP_NT_VUT_REF A visualisation of missing data indicates that soil water content data (SWC_F_MDS_*) are often missing. visdat::vis_miss( ddf_allsites |&gt; slice(1:10000), cluster = FALSE, warn_large_data = FALSE ) 3.3.2 Strings The column siteid currently contains strings specifying the full paths of the files that were read in earlier. The next task is to extract the site name from these strings. The file names follow a clear pattern (this also highlights why naming files wisely can often make life a lot simpler). ddf_allsites$siteid |&gt; head() ## [1] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; ## [2] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; ## [3] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; ## [4] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; ## [5] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; ## [6] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; The paths each start with the subdirectory where they are located (\"./data/\"), then \"FLX_\", followed by the site name (the first three entries of the table containing data from all sites are for the site \"CH-Dav\"), and then some more specifications, including the years that respective files’ data cover. The {stringr} package (R-stringr?) (part of tidyverse) offers a set of functions for working with strings. Wikham (XXX) provide a more comprehensive introduction to working with strings. Here, we would like to extract the six characters, starting at position 12. The function str_sub() does that job. vec_sites &lt;- str_sub(vec_files, start = 12, end = 17) head(vec_sites) ## [1] &quot;CH-Dav&quot; &quot;CH-Lae&quot; &quot;FI-Hyy&quot; &quot;FR-Pue&quot; We can use this function to mutate all values of column \"siteid\", overwriting it with just these six characters. ddf_allsites &lt;- ddf_allsites |&gt; mutate(siteid = str_sub(siteid, start = 12, end = 17)) ddf_allsites ## # A tibble: 23,011 × 21 ## siteid TIMESTAMP TA_F SW_IN_F LW_IN_F VPD_F PA_F P_F WS_F GPP_NT_VUT…¹ ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 CH-Dav 1997-01-01 -4.57 77.4 223. 0.565 82.6 0.4 0.559 0.697 ## 2 CH-Dav 1997-01-02 -3.34 45.6 235. 0.978 82.9 0 1.11 1.04 ## 3 CH-Dav 1997-01-03 0.278 74.1 239. 2.24 82.4 0 2.03 -0.242 ## 4 CH-Dav 1997-01-04 -1.88 58.1 250. 1.38 81.7 1.8 1.92 0.247 ## 5 CH-Dav 1997-01-05 -4.96 80.8 248. 1.16 82.3 0 0.407 0.520 ## 6 CH-Dav 1997-01-06 -4.48 59.6 237. 0.838 82.7 0 0.466 0.0182 ## 7 CH-Dav 1997-01-07 -3.15 45.5 234. 1.33 82.9 0 1.03 0.0777 ## 8 CH-Dav 1997-01-08 -2.45 76.7 222. 1.87 82.7 0 1.95 -0.484 ## 9 CH-Dav 1997-01-09 -2.43 47.6 251. 1.44 82.2 0 0.785 -0.379 ## 10 CH-Dav 1997-01-10 -3.09 39.6 242. 0.776 82.8 0 1.25 -0.552 ## # … with 23,001 more rows, 11 more variables: NEE_VUT_REF_QC &lt;dbl&gt;, ## # SWC_F_MDS_1 &lt;dbl&gt;, SWC_F_MDS_2 &lt;dbl&gt;, SWC_F_MDS_3 &lt;dbl&gt;, ## # SWC_F_MDS_1_QC &lt;dbl&gt;, SWC_F_MDS_2_QC &lt;dbl&gt;, SWC_F_MDS_3_QC &lt;dbl&gt;, ## # SWC_F_MDS_4 &lt;dbl&gt;, SWC_F_MDS_4_QC &lt;dbl&gt;, SWC_F_MDS_5 &lt;dbl&gt;, ## # SWC_F_MDS_5_QC &lt;dbl&gt;, and abbreviated variable name ¹​GPP_NT_VUT_REF 3.3.3 Functional programming II Functions can be applied to a list of objects of any type. Therefore, map() is a powerful approach to “iterating” over multiple instances of the same object type and can be used for all sorts of tasks. In the following, list elements are data frames of daily data and the function lm() fits a linear regression model of GPP versus shortwave radiation to each sites’ data. We’ll learn more about fitting statistical models in R in Chapter 8. list_linmod &lt;- map(list_df, ~lm(GPP_NT_VUT_REF ~ SW_IN_F, data = .)) Note how the . indicates where the elements of list_df go when evaluating the lm() function. This returns a list of linear model objects (the type of objects returned by the lm() function call). We can spin the functional programming concept further and apply (or map) the summary() function to the lm-model objects to get a list of useful statistics and metrics, and then further extract the element \"r.squared\" from that list as: list_linmod |&gt; map(summary) |&gt; # applyting a function map_dbl(&quot;r.squared&quot;) # extracting from a named list ## ./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv ## 0.4201802 ## ./data/FLX_CH-Lae_FLUXNET2015_FULLSET_DD_2004-2014_1-4.csv ## 0.5074248 ## ./data/FLX_FI-Hyy_FLUXNET2015_FULLSET_DD_1996-2014_1-3.csv ## 0.6415685 ## ./data/FLX_FR-Pue_FLUXNET2015_FULLSET_DD_2000-2014_2-3.csv ## 0.3772839 map_dbl() is a variant of the map() function that returns not a list, but a vector of numeric values of class “double” (hence, the name _dbl). Note further, that providing a character (\"r.squared\") as an argument instead of an (unquoted) function name, map() extracts the correspondingly named list element, instead of applying a function to a list element. When writing code for an analysis, it’s useful, if not essential, to understand the objects we’re working with, understand its type and shape, and make sense of the results of simple print &lt;object&gt; statements. Data frames are particularly handy as they provide an organisation of data that is particularly intuitive (variables along columns, observations along rows, values in cells). Here, we’re dealing with a list of linear model objects. Can such a list fit into the paradigm of tidy data frames? Yes, they can. Think of the linear model objects as ‘values’. Values don’t necessarily have to be scalars, but they can be of any type (class). tibble( siteid = vec_sites, linmod = list_linmod ) ## # A tibble: 4 × 2 ## siteid linmod ## &lt;chr&gt; &lt;named list&gt; ## 1 CH-Dav &lt;lm&gt; ## 2 CH-Lae &lt;lm&gt; ## 3 FI-Hyy &lt;lm&gt; ## 4 FR-Pue &lt;lm&gt; The fact that cells can contain any type of object offers a powerful concept. Instead of a linear model object as in the example above, each cell may even contain another data frame. In such a case, we say that the data frame is no longer flat, but nested. The following creates a nested data frame, where the column data is defined by the list of data frames read from files above (list_df). tibble( siteid = vec_sites, data = list_df ) ## # A tibble: 4 × 2 ## siteid data ## &lt;chr&gt; &lt;named list&gt; ## 1 CH-Dav &lt;tibble [6,574 × 16]&gt; ## 2 CH-Lae &lt;tibble [4,018 × 18]&gt; ## 3 FI-Hyy &lt;tibble [6,940 × 20]&gt; ## 4 FR-Pue &lt;tibble [5,479 × 10]&gt; We can achieve the same result by directly nesting the flat data frame holding all sites’ data (ddf_allsites). This is done by combining the group_by(), which we have encountered above when aggregating using summarise(), with the function nest() from the {tidyr} package. ddf_allsites |&gt; group_by(siteid) |&gt; nest() ## # A tibble: 4 × 2 ## # Groups: siteid [4] ## siteid data ## &lt;chr&gt; &lt;list&gt; ## 1 CH-Dav &lt;tibble [6,574 × 20]&gt; ## 2 CH-Lae &lt;tibble [4,018 × 20]&gt; ## 3 FI-Hyy &lt;tibble [6,940 × 20]&gt; ## 4 FR-Pue &lt;tibble [5,479 × 20]&gt; The function nest() names the nested data column automatically \"data\". This structure is very useful. For example, for applying functions over sites’ data frames separately (and not over the entire data frame). By combining map() and mutate(), we can fit linear models on each site’s data frame individually in one go. ddf_allsites |&gt; group_by(siteid) |&gt; nest() |&gt; mutate(linmod = map(data, ~lm(GPP_NT_VUT_REF ~ SW_IN_F, data = .))) This approach is extremely powerful and lets you stick to working with tidy data frames and use the rows-dimension flexibly. Here, rows are sites and no longer time steps, while the nested data frames in column \"data\" have time steps along their rows. The power of nesting is also to facilitate complex aggregation steps over a specified dimension (or axis of variation, here given by siteid), where the aggregating function is not limited to taking a vector as input and returning a scalar, as is the case for applications of summarise() (see above). Combining the steps described above into a single workflow, we have: ddf_allsites_nested &lt;- ddf_allsites |&gt; group_by(siteid) |&gt; nest() |&gt; mutate(linmod = map(data, ~lm(GPP_NT_VUT_REF ~ SW_IN_F, data = .))) |&gt; mutate(summ = map(linmod, ~summary(.))) |&gt; mutate(rsq = map_dbl(summ, &quot;r.squared&quot;)) |&gt; arrange(desc(rsq)) # to arrange output, with highest r-squared on top ddf_allsites_nested ## # A tibble: 4 × 5 ## # Groups: siteid [4] ## siteid data linmod summ rsq ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;dbl&gt; ## 1 FI-Hyy &lt;tibble [6,940 × 20]&gt; &lt;lm&gt; &lt;smmry.lm&gt; 0.642 ## 2 CH-Lae &lt;tibble [4,018 × 20]&gt; &lt;lm&gt; &lt;smmry.lm&gt; 0.507 ## 3 CH-Dav &lt;tibble [6,574 × 20]&gt; &lt;lm&gt; &lt;smmry.lm&gt; 0.420 ## 4 FR-Pue &lt;tibble [5,479 × 20]&gt; &lt;lm&gt; &lt;smmry.lm&gt; 0.377 This code is a demonstration of the power of tidy and nested data frames and for the clarity of the {tidyverse} syntax. Nesting is useful also for avoiding value duplication when joining relational data objects. Above, we nested time series data objects (where time steps and sites are both organised along rows) by sites and got a data frame where only sites are organised along rows, while time steps are nested inside the column \"data\". This now fits the structure of a relational data object (siteinfo_fluxnet2015) containing site-specific meta information (also with only sites along rows). base::load(&quot;data/siteinfo_fluxnet2015.rda&quot;) # loads siteinfo_fluxnet2015 Joining the nested data frame with site meta information results in a substantially smaller and much handier data frame compared to an alternative, where the site meta information is joined into the un-nested (daily) data frame, and therefore duplicated for each day within sites. ddf_allsites_nested_joined &lt;- siteinfo_fluxnet2015 |&gt; rename(siteid = sitename) |&gt; right_join(select(ddf_allsites_nested, -linmod, -summ, -rsq), by = &quot;siteid&quot;) ddf_allsites_joined &lt;- siteinfo_fluxnet2015 |&gt; rename(siteid = sitename) |&gt; right_join(ddf_allsites, by = &quot;siteid&quot;) print(paste(&quot;Flat and joined:&quot;, format(object.size(ddf_allsites_joined), units = &quot;auto&quot;, standard = &quot;SI&quot;))) ## [1] &quot;Flat and joined: 5.8 MB&quot; print(paste(&quot;Nested and joined:&quot;, format(object.size(ddf_allsites_nested_joined), units = &quot;auto&quot;, standard = &quot;SI&quot;))) ## [1] &quot;Nested and joined: 3.7 MB&quot; # save for later use write_rds(ddf_allsites_nested_joined, file = &quot;data/ddf_allsites_nested_joined.rds&quot;) 3.4 Exercises {dplyr} comes with a toy dataset dplyr::starwars (just type it into the console to see its content). Have a look at the dataset with View(). Play around with the dataset to get familiar with the {tidyverse} coding style. Use the functions dplyr::filter, dplyr::select, and dplyr::top_n to answer the following questions (hint: find the function names online or use the Help tab in RStudio): How many pale characters come from the planets Ryloth and Naboo? Who is the oldest of the tallest 5 characters? What is the name of the smallest character and their starship in “Return of the Jedi” Reuse the code in the tutorial to aggregate the hhdf dataset to the daily scale and calculate the following metrics for VPD_F using R functions: maximum value, minimum value, median, and standard deviation. The uncleaned dataset FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv holds half-hourly data that is sometimes not of high quality. Split your dataset into two - one with actually measured data (NEE_VUT_REF_QC == 0) and one with only gap-filled data. Compare the means of GPP_NT_VUT_REF of all three datasets and interpret your finding. Can you find a connect between the timepoint of an entry and its quality? What does the temporal pattern imply for the mean values that you calculated? 3.5 Report Exercise Great science is surely always easy to reproduce… right? Well, the truth is that not all journals require that authors upload their scripts and data in a coherent format and if you want to have a look at some data or reproduce findings, it is likely that you will have to go through data and code cleaning first. To exemplify this issue, conduct the following exercise: Download the .xlsx-file from the supplementary material of this paper: Groenigen, Kees Jan van, Xuan Qi, Craig W. Osenberg, Yiqi Luo, and Bruce A. Hungate. “Faster Decomposition Under Increased Atmospheric CO 2 Limits Soil Carbon Storage.” Science 344, no. 6183 (May 2, 2014): 508–9. https://doi.org/10.1126/science.1249534. Open the file and have a look - not quite tidy, right? Your task is to clean this file up so that it can easily be read into RStudio. Follow the principles that you learned in this tutorial and actually try to load the file into RStudio to see whether you’ve done a proper job. For your performance assessment, you have to upload your cleaned dataset to your report repository on GitHub and include a code-chunk in your report that loads the cleaned dataset and prints its first six rows. "],["datavis.html", "Chapter 4 Data visualisation 4.1 Learning objectives 4.2 Tutorial 4.3 Exercises 4.4 Report Exercises", " Chapter 4 Data visualisation Chapter lead author: Benjamin Stocker 4.1 Learning objectives 4.2 Tutorial Visualizations often take the center stage of publications and are often the main vehicles for transporting information in scientific publications and (ever more often) in the media. Visualizations communicate data and its patterns in visual form. Visualizing data is also an integral part of the exploratory data analysis cycle. Visually understanding the data guides its transformation and the identification of suitable models and analysis methods. The quality of a data visualization can be measured by its effectiveness of conveying information about the data and thus of answering a question with the data and telling a story. Different aspects determine this effectiveness, including the appropriateness of visualization elements, the intuitiveness of how information can be decoded from the visualization by the reader, the visual clarity and legibility (taking into account the vision and potential vision deficiencies of the reader), the visual appeal, etc. This tutorial introduces data visualization under the premise that not all aspects of data visualization are a matter of taste. There are appropriate and less appropriate ways of encoding data in visual form. This tutorial is inspired by the comprehensive and online available textbook Fundamentals of Data Visualization by Claus O. Wilke. Another excellent resource is the Chapter Data Visualisation in R for Data Science by Hadley Wickham 4.2.1 The grammar of graphics In Chapter 3, we learned about axes of variation in the data. For example, time is an axis of variation in our example data hhdf, or site identity and the date are axes of variation in our example data ddf. We have also learned that we can aggregate over axes of variation, and that we can often separate an axis of variation into a hierarchy of subordinate axes of variation (e.g., years, months, days, and a half-hourly time axis). In this chapter, we will be working mainly with the same half-hourly time series data of ecosystem-atmosphere fluxes and parallel measurements of meteorological variables - as in Chapters 1 and 3. For time series data, the entry point of the exploratory data analysis cycle may be a visualization of some variable of interest (here GPP_NT_VUT_REF) against time: hhdf &lt;- read_csv(&quot;data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN.csv&quot;) ## Rows: 52608 Columns: 20 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (18): TA_F, SW_IN_F, LW_IN_F, VPD_F, PA_F, P_F, WS_F, GPP_NT_VUT_REF, N... ## dttm (2): TIMESTAMP_START, TIMESTAMP_END ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. plot(hhdf$TIMESTAMP_START, hhdf$GPP_NT_VUT_REF, type = &quot;l&quot;) You may notice the spurious-looking values on the left, in the first third of year 2004. We’ll revisit this point later in this Chapter. From (Wilke): “All data visualizations map data values into quantifiable features of the resulting graphic. We refer to these features as aesthetics.” Applied to our example, the aesthetics are the x-axis and the y-axis of a cartesian coordinate system. TIMESTAMP_START is mapped onto the x-axis, GPP_NT_VUT_REF is mapped onto the y-axis, and their respective values specify the position of points in the cartesian coordinate system that are then connected with lines - making up the geometrical object that represents the data. Often, the aesthetic that is used to plot the target variable against corresponds to a known axis of variation in the data. The notion of mapping data onto aesthetics and using objects whose geometry is defined by the aesthetics gives rise to the grammar of graphics and to the {ggplot2} R package for data visualisation (which we will use throughout the remainder of this course). The equivalent {ggplot2} code that follows the philosophy of the grammar of graphics is: ggplot(data = hhdf, aes(x = TIMESTAMP_START, y = GPP_NT_VUT_REF)) + geom_line() The argument provided by the aes() statement specifies the aesthetics (x, and y) and which variables in data are mapped onto them. Once this is specified, we can use any suitable geometrical object that is defined by these aesthetics. Here, we used a line plot specified by + geom_line(). The data visualisation above is a dense plot and we cannot distinguish patterns because variations in GPP happen at time scales that are too narrow for displaying three years of half-hourly data in one plot. GPP varies throughout a day just as much as it varies throughout a season. To see this, we can focus on a narrower time span (selecting rows by index using dplyr::slice() in the code below). Visual clarity is also facilitated by an appropriate labeling (title and axes labels using labs()) and by a reduction of displayed elements to a minimum (therefore, the changing of the formatting theme by theme_classic()): # prepare plot data plot_data &lt;- hhdf |&gt; dplyr::slice(24000:25000) # plot figure ggplot( data = plot_data, aes(x = TIMESTAMP_START, y = GPP_NT_VUT_REF)) + geom_line() + labs(title = &quot;Gross primary productivity&quot;, subtitle = &quot;Site: CH-Lae&quot;, x = &quot;Time&quot;, y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() Find a complete reference to {ggplot2} here (R-ggplot2?). The grammar of graphics has found its way also into Python and you can use ggplot using the plotnine Python package (see here). 4.2.2 Every data has its representation In the above example, we mapped two continuous variables (TIMESTAMP_START and GPP_NT_VUT_REF) onto the aesthetics x, and y to visualize time series data. A line plot is an appropriate choice for such data as points are ordered along the time axis and can be connected by a line. Different “geometries” are suitable for visualizing different aspects of the data, and different variable types are suited to mapping onto different aesthetics. Common, available aesthetics are shown in Fig. 4.1 and can be allocated to variable types: Continuous variables: position, size, color (a color gradient), line width, etc. Categorical variables: shape, color (a discrete set of colors), line type, etc. Figure 4.1: Common aesthetics to display different variable types. Not only the different aesthetics, but also the type of geometry (the layers of the visualization added to a plot by + geom_*()) goes with certain types of variables and aspects of the data (but not with others). The sub-sections below provide a brief categorization of data visualization types. A more comprehensive overview is given by GGPlot2 Essentials for Great Data Visualization in R by Alboukadel Kassambara. 4.2.2.1 One value per category Probably the simplest case of data visualization is where a single value is shown across a categorical variable. This calls for a bar plot (geom_bar()). In the example below, we plot the mean GPP for within each month. The “custom plot” shown below is a demonstration for what you can do by combining different elements with {ggplot2}. Try to understand the command for creating the object gg2. Both examples are based on the data frame ddf which we created in Chapter 3. # read in demo daily data # as saved in the previous chapter ddf &lt;- read_csv(&quot;data/ddf.csv&quot;) ## Rows: 1096 Columns: 6 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (5): GPP_NT_VUT_REF, n_datapoints, n_measured, SW_IN_F, f_measured ## date (1): date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Bar plot plot_data &lt;- ddf |&gt; dplyr::mutate(month = month(date, label = TRUE)) |&gt; dplyr::group_by(month) |&gt; dplyr::summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF)) # Bar plot gg1 &lt;- ggplot( data = plot_data, aes(x = month, y = GPP_NT_VUT_REF)) + geom_bar(stat = &quot;identity&quot;) + theme_classic() + labs(title = &quot;Bar plot&quot;, x = &quot;Month&quot;, y = expression(paste(&quot;Mean GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) # Custom plot gg2 &lt;- ggplot( data = plot_data, aes(x = month, y = GPP_NT_VUT_REF)) + geom_segment(aes(x = month, xend = month, y = 0, yend = GPP_NT_VUT_REF), size = 3, color = &quot;grey40&quot;) + geom_point(aes(x = month, y = GPP_NT_VUT_REF), size = 8, color = &quot;grey40&quot;) + geom_text(aes(x = month, y = GPP_NT_VUT_REF, label = format(GPP_NT_VUT_REF, digits = 2)), size = 3, color = &quot;white&quot;) + theme_classic() + labs(title = &quot;Custom plot&quot;, x = &quot;Month&quot;, y = expression(paste(&quot;Mean GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + scale_y_continuous(limits = c(0, 8.75), expand = c(0, 0)) + coord_flip() ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. # combine plots cowplot::plot_grid(gg1, gg2) Above, we created two objects, gg1 and gg2, that contain the instructions for creating the plots. To combine multiple sub-plots within panels of a single plot, we used cowplot::plot_grid() from the {cowplot} library. Note also the stat = \"identity\" specification within the geom_bar() function call. This is required when the bar height is specified by a single value within each category (month in the example above). To visualize not a value per se but the count of values within categories, use stat = \"count\" to get the equivalent result as when aggregating by taking the number of observations within categories explicitly using the function dplyr::summarise(). This equivalency is demonstrated below. # subset plot data and count occurences plot_data_counted &lt;- hhdf |&gt; dplyr::filter(NEE_VUT_REF_QC == 0) |&gt; dplyr::group_by(NIGHT) |&gt; dplyr::summarise(count = n()) # separate aggregation gg1 &lt;-ggplot( data = plot_data_counted, aes(x = NIGHT, y = count)) + geom_bar(stat = &quot;identity&quot;) + labs(subtitle = &quot;Count via &#39;summarise&#39; and &#39;stat = identiy&#39;&quot;) + theme_classic() # prepare data (not summarizing counts) plot_data_uncounted &lt;- hhdf |&gt; dplyr::filter(NEE_VUT_REF_QC == 0) # implicit aggregation by &#39;stat&#39; gg2 &lt;- ggplot( data = plot_data_uncounted, aes(x = NIGHT)) + geom_bar(stat = &quot;count&quot;) + labs(subtitle = &quot;Count directly via &#39;stat = count&#39;&quot;) + theme_classic() # combine plots cowplot::plot_grid(gg1, gg2) 4.2.2.2 Distribution of one variable Examining the distribution of a variable is often the first step of exploratory data analysis. A histogram displays the distribution of numerical data by mapping the frequency (or count) of values within discrete bins (equally spaced ranges along the full range values of a given variable) onto the “height” of a bar, and the range of values within bins onto the position of the bar. In other words, it shows the count of how many points of a certain variable (below GPP_NT_VUT_REF) fall into a discrete set of bins. When normalizing (scaling) the “bars” of the histogram to unity, we get a density histogram. To specify the y-axis position of the upper end of the histogram bar as the density, use y = ..density.. in the aes() call. To show counts, use y = ..count... ggplot( data = hhdf, aes(x = GPP_NT_VUT_REF, y = ..density..) ) + geom_histogram(fill = &quot;grey70&quot;, color = &quot;black&quot;) + geom_density(color = &quot;red&quot;) + # we can overlay multiple plot layers! labs(title = &quot;Histogram and density&quot;, x = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Note that the red line plotted by geom_density() on top of the density histogram visualises the density distribution in continuous (not discrete, or binned) form. Note also that both “geoms” share the same aesthetics with aes() specified in the ggplot() function call. 4.2.2.3 Distributions within categories To visualize distributions of a single continuous variable within categories, perhaps the most common visualization type is the box plot. As described in Chapter 3, it shows the median (bold line in the center), the upper and lower quartiles, corresponding to the 25% and the 75% quantiles, often referred to as \\(Q_1\\) and \\(Q_3\\) , and given by the upper and lower edge of the box plot. The lines extending from the box edges visualize the range of \\(( Q_1 - 1.5 (Q_3 - Q_1)\\) to \\(Q_3 + 1.5 (Q_3 - Q_1)\\). Any point outside this range is plotted by a point. The box plot is rather reductionist in showing the data (the vector of all values is reduced to the median, \\(Q_1\\) , \\(Q_3\\), and outlying points) and may yield a distorted picture of the data distribution and does not reflect information about the data volume. For this reason, several journals are now requiring individual data points or at least the number of data points to be shown in addition to each box. Below, points are added by geom_jitter() , where points are “jittered”, that is, randomly spread out along the x-axis. Violin plots are a hybrid of a density plot and a box plot. The form of their edge is given by the density distribution of the points they represent. # prepare plot data set.seed(1985) # for random number reproducibility in sample_n() and jitter plot_data &lt;- hhdf |&gt; sample_n(300) |&gt; mutate(Night = ifelse(NIGHT == 1, TRUE, FALSE)) # Boxplot gg1 &lt;-ggplot( data = plot_data, aes(x = Night, y = VPD_F)) + geom_boxplot(fill = &quot;grey70&quot;) + labs(title = &quot;Box plot&quot;) + labs(y = &quot;VPD (hPa)&quot;) + theme_classic() # Box plot + jittered points gg2 &lt;-ggplot( data = plot_data, aes(x = Night, y = VPD_F)) + geom_boxplot(fill = &quot;grey70&quot;, outlier.shape = NA) + geom_jitter(width = 0.2, alpha = 0.3) + labs(title = &quot;Boxplot + jittered points&quot;) + labs(y = &quot;VPD (hPa)&quot;) + theme_classic() # Violin plot gg3 &lt;- ggplot( data = plot_data, aes(x = Night, y = VPD_F)) + geom_violin(fill = &quot;grey70&quot;) + labs(title = &quot;Violin plot&quot;) + labs(y = &quot;VPD (hPa)&quot;) + theme_classic() # combine plots cowplot::plot_grid(gg1, gg2, gg3, ncol = 3) 4.2.2.4 Regression of two continuous variables Scatter plots visualize how two variables co-vary. The position of each point in a scatter plot is given by the simultaneously recorded value of two variables, provided in two columns along the same row in a data frame, and mapped onto two dimensions in a cartesian coordinate system. We can also say that two variables are regressed against each other. In the figure below, we start with a simple scatter plot (a), regressing GPP against shortwave radiation. A visualization is supposed to tell a story with data. The positive and largely linear relationship between shortwave radiation and GPP is expected from theory (Monteith, 1972) and our process understanding of the dominant controls on photosynthesis - it’s mainly solar (shortwave) radiation. The linear regression line, added by geom_smooth(method = \"lm\") in (a), indicates that relationship. # prepare plot data plot_data &lt;- hhdf |&gt; sample_n(1000) # a gg1 &lt;- ggplot( data = plot_data, aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(size = 0.75) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() # b gg2 &lt;- ggplot( data = plot_data, aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = NIGHT)) + geom_point(size = 0.75) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() # c gg3 &lt;- ggplot( data = plot_data, aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = as.factor(NIGHT))) + geom_point(size = 0.75) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() # d gg4 &lt;- ggplot( data = plot_data, aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = TA_F)) + geom_point(size = 0.75) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() + scale_color_viridis_c() # combine plots cowplot::plot_grid(gg1, gg2, gg3, gg4, ncol = 2, labels = &quot;auto&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Are there additional variables that modify the relationship between solar radiation and GPP? To visually investigate this, we can map additional variables in our data set onto additional aesthetics. For example, at night, photosynthesis ceases as shown in (b). Here, the variable NIGHT was mapped onto the aesthetic color of same geometry (geom_point()). By default, ggplot() used a continuous color scale, as indicated by the color key on the right. It did so although NIGHT is a categorical (a binary) variable because in the data frame, NIGHT is stored as a numeric value (as can be checked by class(hhdf$NIGHT)). To avoid this, and automatically trigger the use of a color scheme that is suitable for categorical variables, we specify aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = as.factor(NIGHT)) in (c). (d) is an example for using a continuous color scheme, as explained in the next section. 4.2.2.5 Use of colors The above example demonstrates that color schemes have to be chosen depending on the nature of the data. Mapping a continuous variable onto the aesthetics color requires a continuous color scheme to be applied, categorical data requires discrete color schemes. More distinctions should be considered: Continuous variables should be distinguished further if they span a range that includes zero or not. If so, diverging color schemes should be used, where zero appears neutral (e.g., white). If zero is not contained within the range of values in the data, diverging color schemes should be avoided. Continuous or ordinal variables may be cyclic in nature. For example, hours in a day are cyclic, although there are twelve discrete numbers. The time 00:00 is nearer to 23:59 than it is from, for example, 01:00. The cyclical, or periodical nature of the data should be reflected in the choice of a color scheme where the edges of the range are more similar to one another than they are to the center of the range (see example below XXX) Multisequential color schemes reflect that there is a natural distinction between two parts of the range of continuous values (see example below XXX). Choices of colors and their combination is far from trivial. Colors in color schemes (or “scales”) should be: Distinguishable for people with color vision deficiency Distinguishable when printed in black and white Evenly spaced in the color space Intuitively encoding the information in the data (for example, blue-red for cold-hot) Visually appealing In (d), we mapped temperature, a continuous variable, onto the color aesthetic of the points and chose the continuous {viridis} color scale by specifying + scale_color_viridis_c(). The viridis scales have become popular for their respect of the points listed above. For further reading, several excellent resources exist that theorize and guide the use of color in data visualization. Excellent sources are: Fabio Crameri’s Scientific colour maps, Crameri (2018) and its R package {scico} (on CRAN). Paul Tol’s Notes, available for example in the {khroma} R package (on CRAN). 4.2.2.6 Regression within categories In the sub-plot (d) above, we may observe a pattern: GPP recorded at low temperatures (dark colored points) tend to be located in the lower range of the cloud of points. We may formulate a hypothesis from this observation, guiding further data analysis and modelling. This illustrates how data visualization is an integral part of any (geo-) data science workflow. Since the relationship between incoming solar radiation and ecosystem photosynthesis is strongly affected by how much of this light is actually absorbed by leaves, and because the amount of green foliage varies strongly throughout a year (the site CH-Lae from which the data is recorded is located in a mixed forest), the slope of the regression between solar radiation and GPP should change between months. Hence, let’s consider months as the categories to be used for separating the data and analyzing the bi-variate relationships separately within. Below, two alternatives are presented. Either the data is separated into a grid of sub-plots, or the data is separated by colors within the same plot panel. Separation by color # prepare data plot_data &lt;-ddf |&gt; dplyr::mutate(month = month(date, label = TRUE)) ggplot( data = plot_data, aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = month)) + geom_point(alpha = 0.5) + geom_smooth(formula = y ~ x + 0, method = &quot;lm&quot;, se = FALSE) + labs(x = expression(paste(&quot;PPFD (&quot;, mu, &quot;mol m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)) ) + theme_classic() + scico::scale_color_scico_d(palette = &quot;romaO&quot;) Note three aspects here. First, the color-mapping is specified within aes() in the ggplot() function call and then adopted for all subsequent additions of geoms. Hence, also the geom_smooth() thus takes the color information, and not by a “hard-coded” specification of color = inside the geom_smooth() call as done in Fig. XXX. Second, we specified a formula for the linear regression “smooting curve” to force the lines through the origin (y ~ x + 0). This is motivated by our a priori understanding of the process generating the data: when solar radiation is zero, photosynthesis (and hence GPP) should be zero. Third, we chose a color palette that reflects the cyclic (or periodic) nature of the categories (months). January is closer to December than it is to April. Therefore, their respective colors should also be closer in the color space. An appropriate palette for this is \"romaO\" from the {scico} package. Separation into sub-plots Yet another “mapping” is available with facet_wrap(). It separates the visualisation into different sub-plots, each showing only the part of the data that falls into the respective category, separated by facet_wrap(). Note, this mapping is not dealt with the same way as other aesthetics - not with specifying it with aes()), but with adding the facet_wrap() with a + to the ggplot() object. The variable by which facet_wrap() separates the plot has to be specified as an argument with a preceeding ~. Here, this is ~month. ggplot( data = plot_data, # reusing the previously subset data (see above) aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(alpha = 0.4) + geom_smooth(formula = y ~ x + 0, method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + labs(x = expression(paste(&quot;PPFD (&quot;, mu, &quot;mol m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)) ) + facet_wrap(~month) You may object here that a linear regression is not a good model for our data. Instead, the relationship looks saturating, as indicated for example by the data in August. But we’ll get to modelling in later chapters. Nevertheless, the two visualizations above confirm our suspicion that the light-GPP relationship varies between months - a demonstration for why data visualization is an integral part of the scientific process. 4.2.2.7 Time series A time series plot can be regarded as a special case of a regression of two variables. In this case, one variable is regressed against time. A defining aspect of time is that there is a natural order in time steps. Therefore, it makes sense to visualize temporal data using lines that connect the points using geom_line(). The example below shows the time series of daily GPP in three years. ggplot( data = ddf, aes(x = date, y = GPP_NT_VUT_REF)) + geom_line() + labs(title = &quot;Line plot&quot;, x = &quot;Time&quot;, y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() In the line plot above, we see a spurious-looking part of the time series in the first third of year 2004. Is this bad data that should be removed? Also, in winter of 2005/2005, some daily GPP values appear as high as a typical summer level of GPP. Is this bad data? Remember, that in Chapter 3, we aggregated the half-hourly hhdf data frame to a daily data frame ddf from which data is visualized above. The aggregation kept a record of the fraction fraction of actually measured (not gap-filled) half-hourly data points per day (f_measured). This yields a “data quality axis”. Is there a pattern between f_measured and the presumably bad data? Discerning such patterns is often only possible with a suitable visualization. What is suitable here? A solution is to “map” f_measured to the color axis. When adding such an additional mapping to visualisation dimensions (“aesthetics”), we have to specify it using aes(). This only affects the points and the color of points, while the lines and points and their position in x-y space is shared. Hence, we write aes(x = date, y = GPP_NT_VUT_REF) in the ggplot() function call (indicating that all subsequent additions of geom_ layers share this x-y mapping); while aes(color = f_measured) is specified only in the geom_point() layer. ggplot( data = ddf, aes(x = date, y = GPP_NT_VUT_REF)) + geom_line() + geom_point(aes(color = f_measured), size = 0.9) + labs(x = &quot;Time&quot;, y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + scale_color_viridis_c(direction = -1) + # inverse color scale is more intuitive here theme_classic() We observe the presumably bad data appear in yellow, and are therefore indeed characterised with a particularly low fraction of actually measured data from which their values are derived. This is an insight we would never have reached by just looking at the naked values in our data frames. Data visualizations are essential for guiding analyses and data processing throughout all steps. Having learned this, we now have a justification for applying further data filtering criteria. 4.2.2.8 Periodic data The seasons are an important axis of variation in our data. Hence our data are periodic - with a periodicity of 365 days in the ddf dataset and with both 12 hours and 365 days in the hhdf dataset. A polar coordinate system, instead of a cartesian system, lends itself to displaying periodic data. A polar coordinate system reflects the fact that, for example, January 1st is closer to December 31st, although they are located on the extreme end of a linear spectrum of days in a year. In a polar coordinate system, the x-axis spans the angle (360\\(^\\circ\\), like a clock), while the y-axis spans the radius (distance from the center). This is specified by changing the coordinate system of the ggplot object by + coord_polar(). Below, we first aggregate the data to get a mean seasonal cycle from ddf (a, b), and to get a mean diurnal (daily) cycle from June data in hhdf (c, d). To get the mean seasonal cycle, we first determine the day-of-year (counting from 1 for January first to 365 for December 31st) using the {lubridate} function yday(). # prepare plot data plot_data &lt;- ddf |&gt; mutate(doy = yday(date)) |&gt; group_by(doy) |&gt; summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF)) # seasonal cycle, cartesian gg1 &lt;- ggplot( data = plot_data, aes(doy, GPP_NT_VUT_REF)) + geom_line() # seasonal cycle, polar gg2 &lt;- ggplot( data = plot_data, aes(doy, GPP_NT_VUT_REF)) + geom_line() + coord_polar() # prepare plot data (diurnal step) plot_data_diurnal &lt;- hhdf |&gt; mutate(month = month(TIMESTAMP_START)) |&gt; filter(month == 6) |&gt; # taking only June data mutate(hour = hour(TIMESTAMP_START)) |&gt; group_by(hour) |&gt; summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF)) # diurnal cycle, cartesian gg3 &lt;-ggplot( data = plot_data_diurnal, aes(hour, GPP_NT_VUT_REF)) + geom_line() # diurnal cycle, polar gg4 &lt;-ggplot( data = plot_data_diurnal, aes(hour, GPP_NT_VUT_REF)) + geom_line() + coord_polar() # combine plots cowplot::plot_grid(gg1, gg2, gg3, gg4, ncol = 2, labels = &quot;auto&quot;) 4.2.2.9 Density along two continuous variables Scatter plots can appear “overcrowded” when points are plotted on top of each other and potentially important information is lost in the visualization. ggplot( data = hhdf, aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point() + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() To avoid obscuring important details about the data, we may want to visualise the density of points. We want to plot how many points fall within bins of a certain range values in GPP and shortwave radiation, or, in other words, within grid cells in the GPP-radiation space. We can visualize the data, for example, with a raster plot that measures the density using stat_density_2d() or with a binning into hexagonal cells using the simple geom_hex() layer. # density raster ggplot( data = ddf, aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + stat_density_2d( geom = &quot;raster&quot;, # the geometric object to display the data aes(fill = after_stat(density)), # using `density`, a variable calculated by the stat contour = FALSE ) + scale_fill_viridis_c() + labs(x = expression(paste(&quot;PPFD (&quot;, mu, &quot;mol m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)) ) + theme_classic() + scale_x_continuous(expand = c(0, 0)) + # avoid gap between plotting ara and axis scale_y_continuous(expand = c(0, 0)) # density hexagonal bins ggplot( data = ddf, aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_hex() + scale_fill_viridis_c() + labs(x = expression(paste(&quot;PPFD (&quot;, mu, &quot;mol m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)) ) + theme_classic() + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) An alternative solution to “overplotting” points is described in this blog post. 4.2.2.10 Raster data In the figure above, the density of points in a grid of equally spaced bins along two axes, one for each variable, was shown. Often, data is organised along a grid of equally spaced bins by nature - think a matrix or raster data. Examples of such data are climate model outputs (which often span more than two dimensions), remote sensing images (again, just one “layer” of an image or one “band”), or images in general. In these cases, two (often spatial) axes span the space of a cartesian coordinate system and the value within each pixel is mapped onto the color aesthetic. The base-R function image() can be used to visualize such spatial data as images. image(volcano) {ggplot2} forces us to the data frame paradigm and therefore doesn’t lend itself naturally to raster data. We can, however, convert raster data into a data frame in a separate step. # example from https://github.com/thomasp85/scico df_volcano &lt;- tibble( x = rep(seq(ncol(volcano)), each = nrow(volcano)), y = rep(seq(nrow(volcano)), ncol(volcano)), height = c(volcano) - 140 # subtract 140 for example below ) ggplot( data = df_volcano, aes(x = x, y = y, fill = height)) + geom_raster() + scico::scale_fill_scico(palette = &#39;bukavu&#39;, midpoint = 0) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) In this example, we used the multisequential \"bukavu\" color scale from the {scico} package to reflect values above 0 (e.g., elevation above the sea level) and below 0 (ocean depth). 4.2.2.11 Geospatial data Raster data often reflects values in geographic space. Also points and polygons may be located in geographic space. This opens the door to geospatial data visualisation (a.k.a. creating maps) which does not form part of this course. Interested readers can find more content on geospatial data visualisation here: Timo Grossenbacher’s blog on geographic data journalism (link) Koen Hufken’s blog containing various worked example XXX 4.3 Exercises Identifying Outliers: Based on this and previous tutorials, read the half-hourly dataset for site CH-Lae, drop entries with -9999 and aggregate GPP_NT_VUT_REF and PPFD_IN to daily means. Fit a linear relationship between GPP_NT_VUT_REF and PPFD_IN, using lm(). The function returns a list of objects which you can access using the $ syntax. Access the fitted residuals and use them in boxplot.stats() to get box plot statistics to determine outliers. The output of boxplot.stats() is a list with the object out, a named vector where the names are the row number of the residuals in your original dataframe and the values are the entries for that row. Use out_boxplot$out |&gt; names() |&gt; as.integer() to get all row numbers. Create a new variable in the daily-aggregated dataset, which defines whether a datapoint is an outlier or not. Visualising diurnal and seasonal cycles: Using the half-hourly dataset for site CH-Lae (after dropping -9999 entries), visualise how GPP (GPP_NT_VUT_REF) varies on two time scales: diurnal (within-day at hourly time scale) and seasonal. To implement this, follow the following steps: Summarise half-hourly data for each hour and day across multiple years to get a mean seasonality with a mean diurnal cycle for each day of the year. To deal with date-time objects, use the {lubridate} package. Use the functions lubridate::yday() and lubridate::hour() to extract the day of year and hour of day across entries. See ?yday to get more hints. Create a raster plot (geom_raster()), mapping the hour of the day to the x-axis, the day of the year to the y-axis, and the magnitude of GPP_NT_VUT_REF to color (use the fill argument). Make this figure ready for publication by adding nice labels and choosing a good color scale. 4.4 Report Exercises Running mean of monthly CO2 concentrations: Read the famous monthly Mauna Loa CO2 record from file co2_monthly_maunaloa.txt. Plot the data with x-values from the column co2_avg and y-values from the column co2 avg. Plot it as a line graph. Define a function to compute a 12-month running mean of the monthly CO2 time series. Arguments to your function may be a vector of the original (monthly) data and a parameter defining the number of elements over which the mean is to be taken (in this case 12). Plot the running mean on top of the monthly data. Use a different color for the running mean graph. Data exploration: Tell a story about the airquality data (data is directly available in R, just type airquality into the console). Apply what you have learned in this Chapter. Explore the data and find suitable ways to display variables. Detect, visualise, and hypothesise patterns that seem interesting to you. "],["datavariety.html", "Chapter 5 Data variety 5.1 Learning objectives 5.2 Tutorial 5.3 Exercises", " Chapter 5 Data variety Chapter lead author: Koen Hufkens 5.1 Learning objectives As a scientist you will encounter variety of data (formats). In this section you will learn some of the most common formats around, their structure, and the advantages and disadvantages of using a particular data format. Only singular files are considered in this section, and databases are not covered although some files (formats) might have a mixed use. However, more and more data moves toward a cloud server based model where data is queried from an online database using an Application Programming Interface (API). Although the explicit use of databases is not covered, you will learn basic API usage to query data which is not represented as a file. In this chapter you will learn: how to recognize data/file formats understand data/file format limitations manipulation wise content wise how to read and or write data in a particular file format how to query an API and store it locally 5.2 Tutorial 5.2.1 Files and file formats 5.2.1.1 File extensions In order to manipulate data and make some distinctions on what a data file might contain, files carry a particular file format extension. These file extensions denote the intended content and use of a particular file. For example a file ending in .txt suggests that it contains text. A file extension allows you, or a computer, to assess the content of a file without opening the file. File extensions are therefore an important tool in assessing what data you are dealing with, and what tools you will need to manipulated (read / write) the data. NOTE: It is important to note that file extensions can be changed. In some cases the file extension does not represent the content and or use case of the data contained within the file. TIP: If a file doesn’t read it is always wise to check the first couple of lines to verify if the data has a structure which corresponds to the file extension. # On a linux/macos system you can use the terminal command (using # the language bash) to show the first couple lines of a file head your_file # alternatively you can show the last few lines # of a file using tail your_file 5.2.1.2 Human readable data One of the most important distinctions in data formats falls along the line of it being human readable or not. Human readable data is, as the term specifies, made up of normal text characters. Human readable text has the advantage that it is easily read, and or edited using conventional text processors. This convenience comes at the cost of the files not being compressed in any way, and file sizes can become unsustainable. However, for many applications where file sizes are limited (&lt;50MB), human readable formats are the preferred option. Most human readable data falls in two broad categories, tabulated data and structured data. Tabulated data Often, human readable formats provide data in tabulated form using a consistent delimiter. This delimiter is a character separating columns of a table. column_one, column_two, column_three 1, 2, 3 1, 2, 3 1, 2, 3 Common delimiters in this context are the comma (,), as shown in the above example. A file with this particular format often carries the comma-separated values file extension (*.csv). Other delimiters are the tabulation (tab) character. Files with tab delimited values have the *.tsv format. TIP: File extensions aren’t always a true indication of the delimiter used. For example, .txt files often contain comma or tab separated data. If reading a file using a particular delimiter fails it is best to check the first few lines of a file. Structured data Tabulated delimited data is row and column oriented and therefore doesn’t allow complex structured content, e.g. tables within tables. This issue is sidestepped by for example the JSON format. The JSON format in particular uses attribute-value pairs to store data, and is therefore more flexible in terms of accommodating varying data structures. Below you see an example of details describing a person, with entries being fo varying length and data content. { &quot;firstName&quot;: &quot;John&quot;, &quot;lastName&quot;: &quot;Smith&quot;, &quot;isAlive&quot;: true, &quot;age&quot;: 27, &quot;address&quot;: { &quot;streetAddress&quot;: &quot;21 2nd Street&quot;, &quot;city&quot;: &quot;New York&quot;, &quot;state&quot;: &quot;NY&quot;, &quot;postalCode&quot;: &quot;10021-3100&quot; }, &quot;phoneNumbers&quot;: [ { &quot;type&quot;: &quot;home&quot;, &quot;number&quot;: &quot;212 555-1234&quot; }, { &quot;type&quot;: &quot;office&quot;, &quot;number&quot;: &quot;646 555-4567&quot; } ], &quot;children&quot;: [ &quot;Catherine&quot;, &quot;Thomas&quot;, &quot;Trevor&quot; ], &quot;spouse&quot;: null } NOTE: despite being human readable, a JSON file is considerably harder to read than a comma separated file. Editing such a file is therefore more prone to errors if not automated. Other human readable structured data formats include the eXtensible Markup Language (XML), which is commonly used in web infrastructure. XML is used for storing, transmitting, and reconstructing arbitrary data but uses (text) markup instead of attribute-value pairs. &lt;note&gt; &lt;to&gt;Tove&lt;/to&gt; &lt;from&gt;Jani&lt;/from&gt; &lt;heading&gt;Reminder&lt;/heading&gt; &lt;body&gt;Don&#39;t forget me this weekend!&lt;/body&gt; &lt;/note&gt; 5.2.1.3 Writing and reading human readable files in R There are a number of ways to read human readable formats into an R work environment. Here the basic approaches are listed, in particular reading CSV and JSON data. Large volumes of data are still available as CSV files or similar. Understanding how to read in such data into a programming environment is key. In this context the read.table() function is a general purpose tool to read in text data. Depending on the format, additional meta-data or comments, certain parameters need to be specified. Its counterpart is a function to write human readable data to file, called - you guessed it - write.table(). Again parameters are required for maximum control over how things are written to file, by default though data are separated by a single empty space ” “, not a comma. Below you find and example in which a file is written to a temporary location, and read in again using the above mentioned functions. # create a data frame with demo data df &lt;- data.frame( col_1 = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), col_2 = c(&quot;d&quot;, &quot;e&quot;, &quot;f&quot;), col_3 = c(1,2,3) ) # write table as CSV to disk write.table( x = df, file = file.path(tempdir(), &quot;your_file.csv&quot;), sep = &quot;,&quot;, row.names = FALSE ) # Read a CSV file df &lt;- read.table( file.path(tempdir(), &quot;your_file.csv&quot;), header = TRUE, sep = &quot;,&quot; ) # help files of both functions can be accessed by # typing ?write.table or ?read.table in the R console In this example a data frame is generated with three columns. This file is then written to a temporary file in the temporary file directory tempdir(). Here, tempdir() returns the location of the temporary R directory, which you can use to store intermediate files. We use the file.path() function to combine the path (tempdir()) with the filename (your_file.csv). Using file.path() is good practice as directory structures are denoted differently between operating systems e.g., using a backslash (\\) on Windows vs. a slash (/) on Unix based systems (Linux/macOS). The file.path() function ensures that the correct directory separator is used. Note that in this command we have to manually set the separator (sep = \",\") and if a header is there (header = TRUE). Depending on the content of a file you will have to alter these parameters. Additional parameters of the read.table() function allow you to specify comment characters, skip empty lines, etc. Similar to this simple CSV file we can generate and read JSON files. For this we do need an additional library, as default R install does not provide this capability. However, the rest of the example follows the above workflow. # we&#39;ll re-use the data frame as generated for the CSV # example, so walk through the above example if you # skipped ahead # install the required package install.packages(&quot;jsonlite&quot;) # load the library library(&quot;jsonlite&quot;) # write the file to a temporary location jsonlite::write_json( x = df, path = file.path(tempdir(), &quot;your_json_file.json&quot;) ) # read the freshly generated json file df_json &lt;- jsonlite::read_json( file.path(tempdir(), &quot;your_json_file.json&quot;), simplifyVector = TRUE ) # check if the two data sets # are identical (they should be) identical(df, df_json) Note that the reading and writing JSON data is easier, as the structure of the data (e.g., field separators) are more strictly defined. While reading the data we use the simplifyVector argument to return a data frame rather than a nested list. This works as our data has a tabulated structure, this might not always be the case. Finally we compare the original data with the data read in using identical(). TIP: In calling the external library we use the :: notation. Although by loading the library with library() makes all jsonlite functions available, the explicit referencing of the origin of the function makes debugging often easier. 5.2.1.4 Binary data All digital data which is not represented as text characters can be considered binary data. Binary data can vary in its content from an executable, which runs a program, to the digital representation of an image (jpeg images). However, in all cases the data is represented as bytes (made of eight bits) and not text characters. One of the advantages of binary data is that it is an efficient representation of data, saving space. This comes at the cost of requiring a dedicated software, other than a text editor, to manipulate the data. For example, digital images in a binary format require image manipulation software. More so than human readable data, the file format (extension) determines how to treat the data. Knowing common data formats and their use cases is therefore key. 5.2.1.5 Common file formats Environmental sciences have particular file formats which dominate the field. Some of these file formats relate to the content of the data, some of these formats are legacy formats due to the history of the field itself. Here we will list some of the most common formats you will encounter. File format (extension) Format description Use case R Library *.csv comma separated tabulated data General purpose flat files with row and column oriented data base R *.txt tabulated data with various delimiters General purpose flat files with row and column oriented data base R *.json structured human readable data General purpose data format. Often used in web application. Has geospatial extensions (geojson). jsonlite *.nc NetCDF data array data Array-oriented data (matrices with &gt; 2 dimensions). Commonly used to store climate data or model outputs. Alternative to HDF data. ncdf4, terra, raster *.hdf HDF array data Array-oriented data (matrices with &gt; 2 dimensions). Commonly used to store climate data or model outputs. hdf *.tiff, *.geotiff Geotiff multi-dimensional raster data (see below) Layered (3D) raster (image) data. Commonly used to represent spatial (raster) data. terra, raster *.shp Shapefile of vector data (see below) Common vector based geospatial data. Used to describe data which can be captured by location/shape and attribute values. sp, sf 5.2.2 Meta-data Meta-data, or data which is associated with the main data file is key to understanding the content and context of a data file (or the data set to which the file belongs). In some cases you will find this data only as a general description referencing the file(s) itself. In other cases, meta-data is included in the file itself. For example, many tabular CSV data files contain a header specifying the content of each column, and at times a couple of lines of data specifying the content of the file itself - or context within which the data should be considered. # This is meta-data associated with the tabulated CSV file # for which the data is listed below. # # In addition to some meta-data, the first row of the data # contains the column header data column_one, column_two, column_three 1, 2, 3 1, 2, 3 1, 2, 3 In the case of binary files it will not be possible to read the meta-data as plain text. In this case, specific commands can be used to read the meta-data included in a file. The example below shows how you would list the meta-data of a GeoTiff file using the bash. # list geospatial data for a geotiff file gdalinfo your_geotiff.tiff TIP: Always keep track of your meta-data by including it, if possible, in the file itself. If this is not possible, meta data is often provided in a file called README. Meta-data is key in making science reproducible and guaranteeing consistency between projects. Key meta-data to retain are: the source of your data (URL, manuscript, DOI) the date when the data was downloaded manipulations on the data before using the data in a final workflow Meta-data of data read into R can be accessed by plotting the object itself, i.e. calling the object on the command line. If it is a simple table the first lines of the table will be shown. When it is a more complex object the meta-data will be output as a formatted statement. You can also use the str or summary commands to summarize data and meta-data. For in depth discussions on meta-data and data wrangling look at Chapter 3. 5.2.3 Spatial data representation Environmental data often has an explicit spatial and temporal component. For example, climate data is often represented as 2D maps which vary over time. This spatial data requires an additional level of understanding of commonly used data formats and structures. In general, we can distinguish two important data models when dealing with spatial data, the raster and vector data model. Both data have their typical file formats (see above) and particular use cases. The definition of these formats, optimization of storage and math/logic on such data are the topic of Geographic Information System (GIS) science and beyond the scope of this course. We refer to other elective GIS courses for a greater understanding of these details. However, a basic understanding of both raster and vector data is provided here. 5.2.3.1 Raster data model The basic raster model represents geographic (2D) continuous data as a two-dimensional array, where each position has a geographic (x, y) coordinate, a cell size (or resolution) and a given extent. Using this definition any image adheres to the raster model. However, in most geographic applications, coordinates are referenced and correspond to a geographic position, e.g., a particular latitude longitude. Often, the model is expanded with a time dimension, stacking various two-dimensional arrays into a three-dimensional array. The raster data model is common for all data sources which use either imaging sensors, such as satellites or unmanned aerial vehicles (UAVs), or model based output which operates on a fixed grid, such as climate and weather models. Additional meta-data stores both the geographic reference system, the time components as well as other data which might be helpful to end users. Within the environmental sciences, NetCDF and GeoTiff are common raster data file formats. 5.2.3.2 Vector data model The vector data model, in contrast to the raster data model, describes (unbound) features using a geometry (location, shape) using coordinates and linked feature attributes. Geometries can be points, lines, polygons, or even volumes. Vector data does not have a defined resolution, making them scale independent. This makes the vector data model ideal for discrete features such as roads or building outlines. Conversely, vector data is poorly suited for continuous data. Conversions between the vector and raster model are possible, but limitations apply. For example, when converting vector data to raster data a resolution needs to be specified, as you lose scale independence of the vector format. Conversions from raster to vector are similarly limited by the original resolution of the raster data. # read in an example shape file # a common vector format 5.2.4 Online data sources The sections above assume that you have inherited some data from someone, or have data files on disk (in a particular format). Yet, most of the time, gathering data is the first step in any analysis. Depending on where data is hosted you can simply download data through your web browser or use the internal download.file() R function to grab data. But, many of the data described in previous sections are today warehoused in large cloud facilities. These data (and their underlying data formats) are stored in large databases and displayed through various applications. For example, Google Maps displays remote sensing (satellite) raster image data in addition to street level vector based labels. These services allow you to access the underlying (original) data using an API, hence programmatically using code. Mastering the use of these services has become key in gathering research data. 5.2.4.1 Direct downloads Before diving into a description of APIs we remind you that some file reading functions in R are web aware, and can not only read local files but also remote ones (i.e., URLs). Getting ahead of ourselves a bit (see tutorials below), the example code shows you how to read the content of a URL directly into your R environment. Although using this functionality isn’t equivalent to using an API, the concept is the same. I.e., you load a remote data source. # define a URL with data of interest # in this case annual mean CO2 levels at Mauna Loa url &lt;- &quot;https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_annmean_mlo.csv&quot; # read in the data directly from URL df &lt;- read.table( url, header = TRUE, sep = &quot;,&quot; ) 5.2.4.2 APIs Web based Application Programming Interfaces (APIs) offer a way to specify the scope of the returned data, and ultimately, the processing which goes on behind the scene in response to a (data) query. APIs are a way to, in a limited way, control a remote server to execute a certain (data) action. In most (RESTful) APIs, such query takes the form of an HTTP URL via an URL-encoded scheme using an API endpoint (or base URL). To reduce some of the complexity of APIs, it is common that a wrapper is written around an API in the language of choice (e.g., R, python). These dedicated API libraries make it easier to access data and limit coding overhead, as more concisely written. Dedicated API libraries As an example of a dedicated library we use the {MODISTools} R package which queries remote sensing data generated by the MODIS remote sensing (satellite) mission from the Oak Ridge National Laboratories data archive. # install the MODISTools package install.packages(&quot;MODISTools&quot;) # load the library library(&quot;MODISTools&quot;) # list all available products products &lt;- MODISTools::mt_products() # print the first few lines # of available products print(head(products)) # download a demo dataset # download data subset &lt;- MODISTools::mt_subset( product = &quot;MOD11A2&quot;, lat = 40, lon = -110, band = &quot;LST_Day_1km&quot;, start = &quot;2004-01-01&quot;, end = &quot;2004-02-01&quot;, km_lr = 1, km_ab = 1, internal = TRUE ) # print the dowloaded data print(head(subset)) A detailed description of all functions of the MODISTools R package is beyond the scope of this course. However, the listed command show you what a dedicated API package does. It is a shortcut to functional elements of an API. For example mt_products() allows you to quickly list all products without any knowledge of an API URL. Although more complex, as requiring parameters, the mt_subset() routine allows you to query remote sensing data for a single location (specified with a latitude lat and longitude lon), and a given date range (e.g. start, end parameters), a physical extent (in km left-right and above-below). GET Depending on your data source, you will either need to rely on a dedicated R package to query the API or study the API documentation. The general scheme for using an API follows the use of the GET() command of the {httr} R library. You define a query using API parameters, as a named list, and then use a GET() statement to download the data from the endpoint (url). # formulate a named list query to pass to httr query &lt;- list( &quot;argument&quot; = &quot;2&quot;, &quot;another_argument&quot; = &quot;3&quot; ) # create url string (varies per product / param) url &lt;- &quot;https://your.service.endpoint.com&quot; # download data using the # API endpoint and query data status &lt;- httr::GET( url = url, query = query, httr::write_disk( path = &quot;/where/to/store/data/filename.ext&quot;, overwrite = TRUE ) ) Below, we provide an example of using the GET command to download data from the Regridded Harmonized World Soil Database (v1.2) as hosted on the Oak Ridge National Laboratory computer infrastructure. In this case we download a subset of a global map of topsoil sand content (T_SAND). # set API URL endpoint # for the total sand content url &lt;- &quot;https://thredds.daac.ornl.gov/thredds/ncss/ornldaac/1247/T_SAND.nc4&quot; # formulate query to pass to httr query &lt;- list( &quot;var&quot; = &quot;T_SAND&quot;, &quot;south&quot; = 32, &quot;west&quot; = -81, &quot;east&quot; = -80, &quot;north&quot; = 34, &quot;disableProjSubset&quot; = &quot;on&quot;, &quot;horizStride&quot;= 1, &quot;accept&quot;=&quot;netcdf4&quot; ) # download data using the # API endpoint and query data status &lt;- httr::GET( url = url, query = query, httr::write_disk( path = file.path(tempdir(), &quot;T_SAND.nc&quot;), overwrite = TRUE ) ) # to visualize the data # we need to load the {terra} # library library(&quot;terra&quot;) r &lt;- terra::rast(file.path(tempdir(), &quot;T_SAND.nc&quot;)) terra::plot(r) Authentication Depending on the API, authentication using a user name and a key or password is required. Then, the template should be slightly altered to accommodate for these requirements. Note that instead of the GET() command we use POST() as we need to post some authentication data before we can get the data in return. # an authenticated API query status &lt;- httr::POST( url = url, httr::authenticate(user, key), httr::add_headers(&quot;Accept&quot; = &quot;application/json&quot;, &quot;Content-Type&quot; = &quot;application/json&quot;), body = query, encode = &quot;json&quot; ) 5.3 Exercises 5.3.1 Files and file formats Reading and writing human readable files While not leaving your R session, download and open the files at the following locations: https://raw.githubusercontent.com/geco-bern/agds/main/data/demo_1.csv https://raw.githubusercontent.com/geco-bern/agds/main/data/demo_2.csv https://raw.githubusercontent.com/geco-bern/agds/main/data/demo_3.csv Once loaded into your R environment, combine and save all data as a temporary CSV file. Read in the new temporary CSV file, and save it as a JSON file in your current working directory. # Your solution script Reading and writing binary files Download and open the following file: https://raw.githubusercontent.com/geco-bern/agds/main/data/some_data.nc What file format are we dealing with? What library would you use to read this kind of data? What does this file contain? Write this file to disk in a different geospatial format you desire (use the R documentation of the library used to read the file and the chapter information). # Your solution script Download and open the following file: https://raw.githubusercontent.com/geco-bern/agds/main/data/some_data.tiff Does this data seem familiar, and how can you tell? What are your conclusions? # Your solution script 5.3.2 API use 5.3.3 GET Download the HWSD total sand content data for the extent of Switzerland following the tutorial example. Visualize the data as a simple map. Download the HWSD topsoil silt content for the extent of Switzerland. 5.3.4 Dedicated library Use the {hwsdr} library (a dedicated package for the API) to download the same data. How does this compare to the previous code written? List how many data products there are on the ORNL MODIS data repository. Download the MODIS land cover map for the canton of Bern. "],["openscience.html", "Chapter 6 Open science practices 6.1 Learning objectives 6.2 Tutorial 6.3 Exercises", " Chapter 6 Open science practices Chapter lead author: Koen Hufkens 6.1 Learning objectives In this chapter you will learn the reasons for practicing open science, and some of the basic methodological techniques that we can use to facilitate an open science workflow. In this chapter you will learn how to: structure a project manage a project workflow capture a session or machine state use dynamic reporting ensure data/project retention 6.2 Tutorial The scientific method relies on repeated testing of a hypothesis. When dealing with data and formal analysis one can reduce this problem to the question: could an independent scientist attain the same results given the described methodology, the data and code. Although this seems trivial, this issue has vexed the scientific community. These days, many scientific publications are based on complex analyses with often large data sets. More so, methods in publications often are insufficiently detailed to really capture the scope of an analysis. Even from this technical point of view, the reproducibility crisis or the inability to reproduce experimental results, is a complex problem. This is further compounded by social aspects and incentives. The last decades scientific research has seen a steady increase in speed due to the digitization of many fields and the commodification of science. Although digitization has opened up new research possibilities it also had less desired outcomes such as the reproducibility crisis, overall decreasing research quality, or outright fraud. Historically research (output) has been confined to academic journals with a limited reach into the public (civil) domain. Digitization made both the academic output visible, as well as the issues that stem from it (e.g. fraud). In many ways the reproducibility crisis as digital tools have made this position untenable. Open and reproducible science is in part a counter movement to make scientific research (output) accessible to the larger public, increase research transparency and countering accusations of fraud and limiting disinformation (to some extent). Open science therefore aims to be as open as possible about the whole scientific process, and as closed as desirable (e.g. privacy or security reasons). It is important to acknowledge that there is a spectrum of reproducible data and code workflows which depends on the state or source of the data and the output of the code (or analysis). Within the context of this course and the discussion of open science we focus solely on the practical aspects for reproducible science, i.e. ensuring that given the same data and code the results will be similar. Figure 6.1: The reproducibility matrix by The Turing Way The basics of open science coding and data practices rely on a number of simple concepts. The below sections describe a selection of the most important ones. Sticking to these principles and/or tools will increase the reproducibility of your work greatly. 6.2.1 Project structure Reproducible science relies on a number of key components. Data and code management and the tracking of required meta-data is the first step in an open science workflow. Although current computers make it easy to “find” your data and are largely file location agnostic this is not the case in many research environments. Here files need a precise, structured, location. This structure allows you to determine both the function and or order of a workflow. It is good practice to have a consistent project structure within and between projects. This allows you to find most project components regardless of when you return to a particular project. Structuring a project in one folder also makes projects portable. All parts reside in one location making it easy to create a github project from this location, or just copy the project to a new drive. An example data structure for raw data processing is given below and we provided an R project template to work from and adjust on the lab website. A full description on using the template is provided in the next section (??). data-raw/ ├─ raw_data_product/ ├─ 00_download_raw_data.R ├─ 01_process_raw_data.R 6.2.2 Managing workflows Although some code is agnostic to the order of execution many projects are effectively workflows, where the output of one routine is required for the successful execution of the next routine. In order to make sure that a future you, or a collaborator, understands in which order things should be executed it is best to number scripts / code properly. This is the most basic approach to managing workflows. In the example below all statistics code is stored in the statistics folder in an overall analysis folder (which also includes code for figures). All statistical analysis are numbered, to ensure that the output of a first analysis is available to the following one. analysis/ ├─ statistics/ │ ├─ 00_randomforest_model.R │ ├─ 01_randomforest_tuning.R ├─ figures/ │ ├─ global_model_results_map.R │ ├─ complex_process_visualization.R 6.2.2.1 Automating and visualizing workflows with targets To sidestep some of the manual management in R you can use dedicated pipeline tool like the {targets} package in R. The targets package learns how your pipeline fits together, skips tasks that are already up to date, runs only the necessary computation. Given the highly controlled environment {targets} can also visualize the (progress of) your workflow. Figure 6.2: A targets visualized workflow by rOpenSci Due to the added complexity of the targets package we won’t include extensive examples of such a workflow but refer to the excellent documentation of the package for simple examples. https://books.ropensci.org/targets/walkthrough.html 6.2.3 Capturing your session state Often code depends on various components, packages or libraries. These libraries and all software come in specific versions, which might or might not alter the behaviour of the cod which relies on them. If you want to ensure full reproducibility, especially across several years, you will need to capture the state of the system and libraries with which you ran the original analysis. In R the {renv} package serves this purpose and will provide an index of all the packages used in your project as well as their version.For a particular project it will create a local library of packages with a static version. These static packages will not be updated over time, and therefore ensure consistent results. This makes your analysis, isolated, portable and reproducible. The analogue in python would be the virtual environments, or venv program. When setting up your project you can run: # Initiate a {renv} environment renv::init() To initiate your static R environment. Whenever you want to save the state of your project (and its packages) you can call: # Save the current state of the environment / project renv::snapshot() To save any changes made to your environment. All data will be saved in a project description file called a lock file (i.e. renv.lock). It is advised to update the state of your project regularly, and in particular before closing a project. When you move your project to a new system, or share a project on github with collaborators, you can revert to the original state of the analysis by calling: # On a new system, or when inheriting a project # from a collaborator you can use a lock file # to restore the session/project state using renv::restore() NOTE: as mentioned in the {renv} documentation. “For development and collaboration, the .Rprofile, renv.lock and renv/activate.R files should be committed to your version control system. But the renv/library directory should normally be ignored. Note that renv::init() will attempt to write the requisite ignore statements to the project .gitignore.” We refer to 6.1 for details on github and its use. 6.2.4 Capturing a system state Although R projects and the use of targets make your workflow consistent the package versions used between various systems (e.g. your home computer, a cluster at the university etc. might vary). To address issues with changes in the versions of package you can use the {renv} package which manages package version (environments) for you. When tasks are even more complex and include components outside of R you can use Docker to provide containerization of an operating system and the included ancillary application. The {rocker} project provides access to some of these features within the context of reproducible R environments. Using these tools you can therefore emulate the state of a machine independent of the machine on which the docker file is run. These days Machine Learning applications are often deployed as docker sessions to limit the complexity of installing required software components. The application of docker based installs is outside the scope of the current course, but feel free to explore these resources as they are widespread in data science. 6.2.5 Readable reporting using Rmarkdown Within Rstudio you can use Rmarkdown dynamic documents to combine both text and code. Rmarkdown is ideal for reporting i.e., writing your final document presenting your analysis results. A Rmarkdown documents consists of a header document properties, such as how it should be rendered (as an html page, a docx file or a pdf), and the actual content. Below you see the header file of an Rmarkdown document that should be rendered as an html page. --- title: My R Markdown Report author: You output: html_document --- The remainder of the document shows a code chunk outlined by ``` quotes and the chunk arguments in {} brackets. Inline operations, the evaluation of code, in text is also possible by using single quotes around a variable or code. ```r x &lt;- 5 # radius of a circle ``` For a circle with the radius 5, its area is 78.5398163. The document can be rendered by calling rmarkdown::render() on the command line or hitting the “Knit” button in the RStudio IDE. Depending on your settings a html file, pdf or docx file will be generated in your current directory (and or displayed in the IDE viewer). # render the document on the command line rmarkdown::render() 6.2.5.1 Referencing and finding files In R projects all files can be referenced relative to the top most path of the project. When opening your_project.Rproj in RStudio you can load data in the console as such read.table(\"data/some_data.csv\"), specifying a “soft” relative path for some_data.csv. project/ ├─ your_project.Rproj ├─ statistics/ │ ├─ your_dynamic_document.Rmd ├─ data/ │ ├─ some_data.csv Rmarkdown files are rendered relative to the file path where to document resides. This means that data which resides in data can’t be accessed by statistics/your_dynamic_document.Rmd even when using an R project and soft relative paths (which work for scripts and functions). As such trying to render the your_dynamic_document.Rmd will fail as the file some_data.csv will not be found. --- title: Your dynamic document author: You output: html_document --- data &lt;- read.table(&#39;data/some_data.csv&#39;) We need to explicitly take the project’s base path into the fold using the {here} package. The here package gathers the absolute path of files inside an R project. As such, here::here(\"data/some_data.csv\") will return the full path of the data (e.g. /your_computer/project/data/some_data.csv). This absolute path will be a valid path for the read.table() function, making the Rmarkdown file render correctly. The correct Rmarkdown code to read the data therefore reads: --- title: Your dynamic document author: You output: html_document --- data &lt;- read.table(here::here(&#39;data/some_data.csv&#39;)) But why not use absolute paths to begin with? Portability! When I would run your *.Rmd file with an absolute path on my computer it would not render as the file some_data.csv would then be located at: /my_computer/project/data/some_data.csv 6.2.5.2 Limitations The file referencing issue and the common use of Rmarkdown as a one size fits all solution, containing all aspects from data cleaning to reporting, makes Rmarkdown files not portable or reproducible. The one size fits all approach to Rmarkdown also encourages bad project management practices. As illustrated above, if no session management tools such as the package {here} are used this automatically causes files to pile up in the top most level of a project, undoing most efforts to physically structure data and code as highlighted in 6.2.1. At the heart of this discussion are not only practical considerations but also the fact that R markdown documents mix two cognitive tasks, writing text content (i.e. reporting) and writing code. Switching between these two modes comes with undue overhead. If you code, you should not be writing prose, and vise versa. If your R markdown file contains more code than it does text, it should be considered an R script or function (with comments or documentation). Conversely, if your markdown file contains more text than code it probably is easier to collaborate on a true word processing file (or cloud based solution). The use case where the notebooks might serve some importance is true reporting of general statistics. R markdown files have their function for reporting concise results, once generated (through functions or analysis scripts) but should be generally be avoided to develop code &amp; ideas as it encourages bad project management practices. 6.2.6 Data retention Coding practices and documenting all moving parts in a coding workflow is only one practical aspect of open science. An additional component is long term data and code retention and versioning. For code online collaboration tools, such as github, gitlab or codeberg, provide a way to provide access to code. However, these tools should only be considered collaborative aids not a place to store code into perpetuity. Furthermore, these services mostly have a limit to how much data can realistically be stored in a repository (mostly ~2GB). For small projects data can be included in the repository itself, for larger projects this won’t be possible. To ensure long term storage of code and data, outside of commercial for profit services, it is best to rely on for example Zenodo. Zenodo is an effort by the European commission, but accessible to all, to facilitate archiving of science projects of all nature (code and data) up to 50GB. In addition, Zenodo provides a citable digital object identifier or DOI. This allows data and code, even if not formally published in a journal, to be cited. Other noteworthy open science storage options include Dryad and the Center for Open Science. 6.3 Exercises 6.3.1 External data You inherit a project folder which contains the following files. ~/project/ ├─ survey.xlsx ├─ xls conversion.csv ├─ xls conversion (copy 1).csv ├─ Model-test_1.R ├─ Model-test-final.R ├─ Plots.R ├─ Figure 1.png ├─ test.png ├─ Rplot01.png ├─ Report.Rmd ├─ Report.html ├─ my_functions.R What are your steps to make this project more reproducible? Write down how and why you would organize your project. 6.3.2 A new project What are the basic steps to create a reproducible workflow from a file management perspective? Create your own R project using these principles and provide details the on steps involved and why they matter. The project should be a reproducible workflow: to download and plot a MODIS land cover map for Belgium using skills you learned in 5 contain a function to count the occurrences of land cover classes in the map as a formal function using skills you learned in 3 create a plot of the land cover map 4 contain a dynamic report describing your answers to the above questions regarding how to structure a reproducible workflow 6.3.3 Tracking the state of your project Track the packages you use in the project you created using {renv} Install any additional library and update the state of your project create a simple {targets} project using the above workflow make changes to the API download routine rerun the targets project "],["codemgmt.html", "Chapter 7 Code management 7.1 Learning objectives 7.2 Tutorial 7.3 Exercises 7.4 Report Exercises", " Chapter 7 Code management Chapter lead author: Koen Hufkens 7.1 Learning objectives In this chapter you will learn how to manage your code with common version control tools, i.e. git. You will learn how to: create a git project (new or from a template) track changes in your code project collaborate with others ensure reproducibility of your project by openly sharing your work and progress. 7.2 Tutorial Code management is a key of managing any data science project, especially when collaborating. Proper code management limits mistakes such as code loss and increases efficiency by structuring projects. In this chapter, we will discuss the management of code in both the location sense, where things are kept, and tracking temporal changes over time using a version control system. Current version control of code is dominated by the software tool git. However, version control has a long history and can be found not only in code development practices.For example, whenever you use track changes in a text document you apply a form of version control i.e., you track changes in your text over time and selectively accepted changes. In this respect git, as a tool for version control of code, does not differ much from track changes of a text document, but follows a manual modify –&gt; staged –&gt; commit workflow. Figure 7.1: The git workflow- by Paola Corrales and Elio Campitelli Git allows for the collaboration of multiple people on the same code, while being consistent in how changes are implemented. Built upon git are cloud based platforms such as github, gitlab and codeberg which make these collaborative decisions and operations even easier. Figure 7.2: The github remote workflow- by Paola Corrales and Elio Campitelli In this chapter, you will learn how to use git and github to manage your project and collaborate on code. NOTE: Coding style, and documentation practices of the code itself have been covered previously in the Chapter Programming primers. Although the tutorial below focuses on github the jargon and operations are transferable to other platforms such as gitlab and codeberg. 7.2.1 Git and local version control Git allows for the tracking of changes in code (or any file) within a git project. A git project is defined by the topmost directory in which a git project is created. For example the following project is not tracked for changes using git. project/ ├─ YOUR_PROJECT.Rproj You can start tracking a project by initiating a local git repository using the following code in R. We’ll use the {usethis} package to make some of the setup a project easier. # initiate a github repository usethis::use_git() This will create a github repository in your project. It will also create a .gitignore file which specifies which files NOT to track (even if asked to). In addition it will make an first commit. 7.2.1.1 git add Before we can track anything we need to tell git which files to track. We therefore have to add them to an index of tracked files. You can either do this on the command line using: git add your_file.csv Or using the RStudio git panel. In this panel you will see all un-tracked files or directories highlighted with a yellow question mark. Figure 7.3: Unstaged files in a git enabled R project You select the file tick boxes to the left to stage all files for inclusion into the git repository. Once staged, the next step will be to finally commit these staged files to be included in git tracking. Figure 7.4: Staged files in a git enabled R project 7.2.1.2 git commit To store any changes to the files which were staged we need to commit these changes. We therefore hit the commit button, this will pop up a new window. Figure 7.5: Entering a commit message Each commit needs a brief message describing what you have included in the staged files, or the commit message, as shown in the panel on the right. You need to provide this small message before pressing the commit button once more. This will let git track the changes to these files. A message will be shown if the commit is successful. Figure 7.6: A completed commit With this you will track all files locally. Any new changes to a file will need to be committed to the git repository once more. So, unlike cloud services such as Dropbox, your files are not automatically tracked, this is a manual step. As with normal documents you are advised to save (commit) your changes to your project frequently. More so, if you create a new file you will need to add it before you can commit it. You can commit changes of staged files using the command line as well using the following command. git commit -m &quot;A message&quot;&quot; 7.2.2 Remote version control Local files limit the degree in which you can collaborate with people. This is where remote cloud based git solutions such as github, gitlab and codeberg come in. They provide a cloud based git repository which you can associate with your local project (see figure above). To create a remote project and successfully associate it with an R project we first have to specify some details, such as the username and email you used is singing up for github. To not leave your R session you can use the {usethis} package for this. # Configure your project library(usethis) usethis::use_git_config(user.name = &quot;Jane Doe&quot;, user.email = &quot;jane@example.org&quot;) For security reasons the use of your github password is not allowed in remote actions. You therefore need to generate a personal access token (PAT) which can be restricted in time and functionality. To proceed first generate a github PAT using these instructions. To create a new project on github hit the “+” sign top left on the github main page (once logged in), and select the “new repository” from the dropdown menu. Figure 7.7: Create a new github repository A new interface will open up in which you should not use any template, but specify your own project name and brief description. Make sure your project is public, and all other settings are kept as is before you hit the “Create repository” button. Figure 7.8: Set the github project name Note the URL that is generated for your project, you will need it when creating a new RStudio project which is linked to github. Figure 7.9: A repository link you need during the R project wizard Next we’ll setup an R project which is associated with the repository. Use: File &gt; New Project &gt; Version Control &gt; Git. Figure 7.10: Linking the github project to a new git enabled R project In the “repository URL” paste the URL of your new GitHub repository, in the example above this would be https://github.com/khufkens/YOUR_PROJECT.git. select a location where to store the project, select the “Open in new session” option and click “Create Project”. A window will pop up, asking for your github username and a password. This password is not your github login password but the PAT described above. After entering your credential, RStudio creates a *.Proj file as well as a .gitignore file (7.2.1.1). You can add both files as you would otherwise (see 7.2.1.1), and these files are tracked locally. 7.2.2.1 git push Once a remote git service has been configured you can push your local git repository to this remote repository, i.e. syncing both. You can use both the push buttons in the RStudio panel for this or the command linen using git push. At the end of a day or a session it is always advised to push your changes to your remote repository to store any changes. Figure 7.11: Remote git workflow - by Paola Corrales and Elio Campitelli NOTE: Syncing between github and your local repository is a manual task. If not performed the repository will not be synced. To retain all your changes sync both repositories often! 7.2.2.2 git pull and merge conflicts Git pull compares your local git repository with the remote one and implements more recent changes if there are any. Note, that if you make changes on both sides, i.e. your remote and local repository, at the same time you will generate a merge conflict. A merge conflict states that remote and local changes can’t be reconciled without supervised intervention on your part. Changes will be made to your local repository, but the files will include the below syntax for highlighting conflicting differences. &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Adding some content to mess with it later Append this text to initial commit ======= Changing the contents of text file from new branch &gt;&gt;&gt;&gt;&gt;&gt;&gt; new_branch_for_merge_conflict You will need to remove the &lt;&lt;&lt;, === and &gt;&gt;&gt; brackets and retain the changes you want to resolve the conflict before committing the changes again. Content which is currently on file is separated by === to that which provides a conflict and to be merged. 7.2.2.3 git clone You can create a new copy of your project on github using the git clone command. For example, on the command line, you can use: # create a local copy of the remote github repository git clone https://github.com/khufkens/YOUR_PROJECT.git to create a local copy of your remote repositories. You can then start working on this repository by using the modify -&gt; staged -&gt; commit -&gt; push workflow. 7.2.2.4 git fork and pull request You can also create a copy of any public github project from within your github account by creating a fork. You can create a fork of a project by pressing the fork button top right on any public github project page. The number of forks of a project is displayed next to the button, in case of the {rpmodel} package there are 24 forks of this project. Figure 7.12: Creating a fork of an existing project (rpmodel) You can give the forked project a new name and description if so desired. Figure 7.13: Github fork settings A fork allows you to experiment with the code stored in the original project without affecting it. However, the relation to the original project is maintained. If you want to contribute changes to the original project you can do so with a pull request. NOTE: to make changes to a fork project you will first have to clone it to your local system! See workflow above. In a forked project, go to the “Pull requests” tab and press the green “New pull request” button. You will then have to provide description of the changes you made. This information will be forwarded to the original owner of the project, who can accept these changes and accept the pull request and “pull” in the changes. Figure 7.14: Creating a new github pull request 7.2.3 Location based code management - github templates Both code (and data) management require you to be conscientious about where you store your code (and data). Structuring your projects using the same template will allow you to understand where all pieces of an an analysis are stored. This has been mentioned in the 6 chapter. In our R project template we provide a project structure for both data and code which removes the mental overhead out of structuring data projects. This project structure sorts code, data and reporting in a consistent way. You can use the template in combination with a github based version (??) control approach to manage your projects. Simply create a new project from this template and clone the project to your local computer. Any changes to the project can be tracked by the workflows described above. To use the template create a new repository on github, as you otherwise would using the big green button. If you are in the project on github you can hit the green button top right (Use this template). Figure 7.15: Use a github project as a template Otherwise you can select the repository from the template dropdown menu, select geco-bern/R-project-template. Figure 7.16: Using a new template based repository Proceed as usual by naming your repository. However, be careful to select the correct owner of the project if you have multiple identities. Rename the default .Proj file. Figure 7.17: Assigning a new template based repository You can now clone the project to your local computer and continue to populate it with code and data. 7.3 Exercises 7.3.1 Location based code management Create a new R project using the git R project template. Make some changes to the README.md Put a small data set in the appropriate directory Make sure that both local and remote repositories (projects) are synced 7.4 Report Exercises 7.4.1 Github This is a team exercise, so team up with someone else in the classroom! You will learn about how to collaborate online using git and Github. Note that this is part of your final performance assessment! You will hand in a link to your GitHub account and we will check your repositories’ commit histories to see whether this pair-coding exercise was done correctly. Therefore, follow these steps precisely: Person 1 - Create a new repository (can be the same as you created following the tutorial but should no be the same as the one where you hand in your report) Person 2 - Fork the github project that Person 1 created in Step 1. Person 2 - Create a new file in this project Person 2 - Commit and push these changes to this project. Create a pull request to the original project of Person 1. Person 1 - Review the pull request from Person 2. Provide some comments, accept the pull request, letting it the new code by Person 2 be integrated into the project. Person 1 - Add a new file to your own project, and update the github project. Person 2 - Sync your forked project to integrate the changes made by Person 1 into your own repository. Voluntary: Can you force a merge conflict, for example by editing the same file at once, and resolve? To complete the exercise, reverse rolls between Person 1 and Person 2! "],["regressionclassification.html", "Chapter 8 Regression and classification 8.1 Learning objectives 8.2 Tutorial 8.3 Report Exercise", " Chapter 8 Regression and classification Chapter lead author: Pepa Aran 8.1 Learning objectives After completing this tutorial, you will be able to: Understand the basics of regression and classification models Fit linear and logistic regression models in R Choose and calculate relevant model performance metrics Evaluate and compare regression models Detect data outliers Select best predictive variables Contents of this Chapter are inspired and partly adopted by the excellent book Hands-On Machine Learning in R by Boehmke &amp; Greenwell. 8.2 Tutorial 8.2.1 Types of models Models try to explain relationships between variables through a mathematical formulation, particularly to predict a given target variable using other explanatory variables, also called predictors. Generally, we say that the target variable \\(Y\\) is a function (denoted \\(f\\)) of a set of explanatory variables \\(X_1, X_2, \\dots, X_p\\) and some model parameters \\(\\beta\\). Models can be represented as: \\[Y \\sim f(X_1, X_2, \\dots, X_p, \\beta)\\] This is a very general notation and depending on the structure of these components, we get to different modelling approaches. The first distinction comes from the type of target variable. Whenever \\(Y\\) is a continuous variable, we are facing a regression problem. If \\(Y\\) is categorical, we talk about classification. Regression Classification Target variable Continuous Categorical Common models Linear regression, polynomial regression, KNN, tree-based regression Logistic regression, KNN, SVM, tree classifiers Metrics RMSE, \\(R^2\\), adjusted \\(R^2\\), AIC, BIC Accuracy, precision, AUC, F1 8.2.2 Regression In this section, we will introduce the most basic regression model: linear regression. We’ll explain how to fit the model with R, how to include categorical predictors and polynomial terms. Finally, several performance metrics for regression models are presented. 8.2.2.1 Linear regression Theory Let’s start with the simplest model: linear regression. You probably have studied linear regression from a statistical perspective, here we will take a data-fitting approach. For example, we can try to explain the relationship between GPP and short wave radiation, like in the Chapter 4. The figure below shows a cloud of data points, and a straight line predicting GPP based on observed shortwave radiation values. # read and format data from Ch 3 hhdf &lt;- readr::read_csv(&quot;./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN.csv&quot;) set.seed(2023) gg1 &lt;- hhdf |&gt; sample_n(2000) |&gt; # to reduce the dataset ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(size = 0.75, alpha=0.4) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() segment_points &lt;- data.frame(x0 = 332, y0 = 3.65, y_regr = 8.77) gg1 + geom_segment(aes(x = x0, y = y0, xend = x0, yend = y_regr), data = segment_points, color = &quot;blue&quot;, lwd = 1.2, alpha = 0.8) We want to find the best straight line that approximates a cloud of data points. For this, we assume a linear relationship between a single explanatory variable \\(X\\) and our target \\(Y\\): \\[ Y_i \\sim \\beta_0 + \\beta_1 X_i, \\;\\;\\; i = 1, 2, ...n \\;, \\] where \\(Y_i\\) is the i-th observation of the target variable, and \\(X_i\\) is the i-th value of the (single) predictor variable. \\(n\\) is the number of observations we have and \\(\\beta_0\\) and \\(\\beta_1\\) are constant coefficients (model parameters). We call \\(\\beta_0\\) the intercept and \\(\\beta_1\\) the slope of the regression line. Generally, \\(\\hat{Y}\\) denotes the model prediction. Fitting a linear regression is finding the values for \\(\\beta_0\\) and \\(\\beta_1\\) such that, on average over all points, the distance between the line at \\(X_i\\), that is \\(\\beta_0 + \\beta_1 X_i\\) (blue segment in the plot above), and the observed value \\(Y_i\\), is as small as possible. Mathematically, this is minimizing the sum of the square errors, that is: \\[ \\min_{\\beta_0, \\beta_1} \\sum_i (Y_i - \\beta_0 - \\beta_1 X_i)^2 . \\] This linear model can be used to make predictions on new data, which are obtained by \\(\\hat{Y}_\\text{new} = \\beta_0 + \\beta_1 X_\\text{new}\\). When the new data comes from the same distribution as the data used to fit the regression line, this should be a good prediction. It’s not hard to imagine that the univariate linear regression can be generalized to a multivariate linear regression, where we assume that the target variable is a linear combination of \\(p\\) predictor variables: \\[Y \\sim \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\; ... \\; + \\beta_p X_p \\;.\\] Note that here, \\(X_1, \\dots, X_p\\) and \\(Y\\) are vectors of length corresponding to the number of observations in our data set (\\(n\\) - as above). Analogously, calibrating the \\(p+1\\) coefficients \\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_p\\) is to minimize the sum of square errors \\(\\min_{\\beta} \\sum_i (Y_i - \\hat{Y}_i)^2\\). While the regression is a line in two-dimensional space for the univariate case, it is a plane in three-dimensional space for bi-variate regression, and hyperplanes in higher dimensions. Implementation in R To fit a univariate linear regression model in R, we can use the lm() function. Already in Chapter 3, we created linear models by doing: # numerical variables only, remove NA df &lt;- hhdf %&gt;% dplyr::select(-starts_with(&quot;TIMESTAMP&quot;)) %&gt;% tidyr::drop_na() # fit univariate linear regression linmod1 &lt;- lm(GPP_NT_VUT_REF ~ SW_IN_F, data = df) Here, GPP_NT_VUT_REF is \\(Y\\), and SW_IN_F is \\(X\\). We can include multiple predictors for a multivariate regression, for example as: # fit multivariate linear regression linmod2 &lt;- lm(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = df) or all available features in our data set (all columns other than GPP_NT_VUT_REF in df) as: linmod3 &lt;- lm(GPP_NT_VUT_REF ~ ., data = df) linmod* is now a model object of class \"lm\". It is a list containing the following components: ls(linmod1) ## [1] &quot;assign&quot; &quot;call&quot; &quot;coefficients&quot; &quot;df.residual&quot; ## [5] &quot;effects&quot; &quot;fitted.values&quot; &quot;model&quot; &quot;qr&quot; ## [9] &quot;rank&quot; &quot;residuals&quot; &quot;terms&quot; &quot;xlevels&quot; Enter ?lm in the console for a complete documentation of these components and other details of the linear model implementation. R offers a set of generic functions that work with this type of object. The following returns a human-readable report of the fit. Here the residuals are the difference between the observed target values and the predicted values. summary(linmod1) ## ## Call: ## lm(formula = GPP_NT_VUT_REF ~ SW_IN_F, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -38.699 -2.092 -0.406 1.893 35.153 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.8732273 0.0285896 30.54 &lt;2e-16 *** ## SW_IN_F 0.0255041 0.0001129 225.82 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.007 on 41299 degrees of freedom ## Multiple R-squared: 0.5525, Adjusted R-squared: 0.5525 ## F-statistic: 5.099e+04 on 1 and 41299 DF, p-value: &lt; 2.2e-16 We can also extract coefficients \\(\\beta\\) with coef(linmod1) ## (Intercept) SW_IN_F ## 0.87322728 0.02550413 and the residual sum of squares (which we wanted to minimize) with sum(residuals(linmod1)^2) ## [1] 1035309 Although summary() provides a nice, human-readable output, you may find it unpractical to work with. A set of relevant statistical quantities are returned in a tidy format using tidy() from the broom package: broom::tidy(linmod1) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.873 0.0286 30.5 1.25e-202 ## 2 SW_IN_F 0.0255 0.000113 226. 0 Model advantages and concerns An advantage of linear regression is that the coefficients provide information that is straight-forward to interpret. We’ve seen above, that GPP_NT_VUT_REF increases by 0.0255 for a unit increase in SW_IN_F. Of course, the units of the coefficients depend on the units of GPP_NT_VUT_REF and SW_IN_F. This has the advantage that the data does not need to be normalised. That is, a linear regression model with the same predictive skills can be found, irrespective of whether GPP_NT_VUT_REF is given in g C m\\(^{-2}\\)s\\(^{-1}\\) or in kg C m\\(^{-2}\\)s\\(^{-1}\\). Another advantage of linear regression is that it’s much less prone to overfit than other algorithms. You’ll learn more about the concept of overfitting in Chapter 9. Not being prone to overfitting can also be a disadvantage: linear regression models are often not flexible enough to be effectively fit to the data. They are also not able to capture non-linearities in the observed relationship and, as we’ll see later in this chapter, it exhibits a poorer performance than more complex models (e.g. polynomial regression) also on the validation data set. A further limitation is that least squares regression requires \\(n&gt;p\\). In words: the number of observations must be greater than the number of predictors. If this is not given, one can resort to step-wise forward regression, where predictors are sequentially added based on which predictor adds the most additional information at each step. When multiple predictors are linearly correlated, then linear regression cannot discern individual effects and individual predictors may appear statistically insignificant when they would be significant if covarying predictors were not included in the model. Such instability can get propagated to predictions. Again, stepwise regression can be used to remedy this problem. However, when one predictor covaries with multiple other predictors, this may not work. For many applications in Geography and Environmental Sciences, we deal with limited numbers of predictors. We can use our own knowledge to examine potentially problematic covariations and make an informed pre-selection rather than throwing all predictors we can possibly think of at our models. Such a pre-selection can be guided by the model performance on a validation data set (more on that below). An alternative strategy is to use dimension reduction methods. Principal Component regression reduces the data to capture only the complementary axes along which our data varies and therefore collapses covarying predictors into a single one that represents their common axis of variation. Partial Least Squares regression works similarly but modifies the principal components so that they are maximally correlated to the target variable. You can read more on their implementation in R here. 8.2.2.2 Regression on categorical variables In the regression within categories section of Chapter 4, we saw that when we separate the data into sub-plots, hidden patterns emerge. This information is very relevant for modeling, because it can be included in our regression model. It is crucial to spend enough time exploring the data before you start modeling, because it helps to understand the fit and output of the model, but also to create models that capture the relationships between variables better. # create month category df_cat &lt;- hhdf |&gt; mutate(MONTH = lubridate::month(TIMESTAMP_START)) |&gt; tidyr::drop_na() |&gt; dplyr::select(MONTH, GPP_NT_VUT_REF, SW_IN_F) So far, we have only used continuous variables as explanatory variables in a linear regression. It is also possible to use categorical variables. To do this in R, such variables cannot be of class numeric, otherwise the lm() function treats them as continuous variables. For example, although the variable NIGHT is categorical with values 0 and 1, the model linmod3 treats it as a number. We must make sure that categorical variables have class character or, even better, factor. # fix class of categorical variables df_cat &lt;- df_cat |&gt; mutate(MONTH = as.factor(MONTH)) Now we can fit the linear model again: linmod_cat &lt;- lm(GPP_NT_VUT_REF ~ MONTH + SW_IN_F, data = df_cat) summary(linmod_cat) ## ## Call: ## lm(formula = GPP_NT_VUT_REF ~ MONTH + SW_IN_F, data = df_cat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36.212 -2.346 -0.223 2.200 34.416 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.6146109 0.0893693 18.067 &lt; 2e-16 *** ## MONTH2 -1.8105447 0.1294675 -13.985 &lt; 2e-16 *** ## MONTH3 -2.8800172 0.1264177 -22.782 &lt; 2e-16 *** ## MONTH4 -2.5667281 0.1278097 -20.082 &lt; 2e-16 *** ## MONTH5 -0.0288745 0.1273491 -0.227 0.820631 ## MONTH6 0.4614556 0.1298069 3.555 0.000378 *** ## MONTH7 0.1697514 0.1283830 1.322 0.186100 ## MONTH8 1.2942463 0.1231252 10.512 &lt; 2e-16 *** ## MONTH9 0.5140562 0.1165474 4.411 1.03e-05 *** ## MONTH10 -0.4807082 0.1152536 -4.171 3.04e-05 *** ## MONTH11 -1.3370277 0.1159059 -11.535 &lt; 2e-16 *** ## MONTH12 -1.2634451 0.1151530 -10.972 &lt; 2e-16 *** ## SW_IN_F 0.0246420 0.0001169 210.810 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.865 on 41288 degrees of freedom ## Multiple R-squared: 0.5776, Adjusted R-squared: 0.5775 ## F-statistic: 4704 on 12 and 41288 DF, p-value: &lt; 2.2e-16 In the fit summary, you can observe that, there are MONTH2 to MONTH12 parameters. MONTH is a factor which can take 12 different values: 1 to 12. lm() uses one of the factor level as the reference, in this case 1, and fits an intercept for the other categories. The result is a set of parallel regression lines, one for each different month. df_cat |&gt; mutate(MONTH_NAME = lubridate::month(as.integer(MONTH), label = TRUE)) |&gt; ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(alpha = 0.2) + geom_smooth(formula = y ~ x + 0, method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + labs(x = &quot;SW&quot;, y = &quot;GPP&quot;) + facet_wrap(~MONTH_NAME) + theme_classic() In the grid image, we can observe that GPP does not increase with SW at the same rate every month. For example, the increase in GPP is less steep in February than in September. To model this, we should consider a variable slope parameter for each month or category. In R, this is implemented by including an interaction term MONTH:SW_IN_F in the regression formula, like this: linmod_inter &lt;- lm(GPP_NT_VUT_REF ~ MONTH + SW_IN_F + MONTH:SW_IN_F, data = df_cat) # equivalently: lm(GPP_NT_VUT_REF ~ MONTH * SW_IN_F, data = df_cat) summary(linmod_inter) ## ## Call: ## lm(formula = GPP_NT_VUT_REF ~ MONTH + SW_IN_F + MONTH:SW_IN_F, ## data = df_cat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -28.891 -2.113 -0.420 1.892 34.029 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.0449603 0.0944991 21.640 &lt; 2e-16 *** ## MONTH2 -1.5386938 0.1369424 -11.236 &lt; 2e-16 *** ## MONTH3 -1.5249304 0.1365863 -11.165 &lt; 2e-16 *** ## MONTH4 -1.0050639 0.1396023 -7.199 6.15e-13 *** ## MONTH5 -0.4502367 0.1412720 -3.187 0.00144 ** ## MONTH6 -1.2559057 0.1474257 -8.519 &lt; 2e-16 *** ## MONTH7 -0.8440097 0.1446838 -5.833 5.47e-09 *** ## MONTH8 -0.2188300 0.1346734 -1.625 0.10419 ## MONTH9 -1.3407190 0.1269387 -10.562 &lt; 2e-16 *** ## MONTH10 -0.9991456 0.1235627 -8.086 6.32e-16 *** ## MONTH11 -1.2124373 0.1230946 -9.850 &lt; 2e-16 *** ## MONTH12 -1.0724209 0.1210819 -8.857 &lt; 2e-16 *** ## SW_IN_F 0.0158600 0.0008758 18.110 &lt; 2e-16 *** ## MONTH2:SW_IN_F -0.0030373 0.0011518 -2.637 0.00837 ** ## MONTH3:SW_IN_F -0.0058229 0.0009713 -5.995 2.05e-09 *** ## MONTH4:SW_IN_F -0.0038333 0.0009469 -4.048 5.17e-05 *** ## MONTH5:SW_IN_F 0.0087370 0.0009305 9.389 &lt; 2e-16 *** ## MONTH6:SW_IN_F 0.0135219 0.0009172 14.743 &lt; 2e-16 *** ## MONTH7:SW_IN_F 0.0110791 0.0009182 12.066 &lt; 2e-16 *** ## MONTH8:SW_IN_F 0.0151014 0.0009317 16.209 &lt; 2e-16 *** ## MONTH9:SW_IN_F 0.0180496 0.0009297 19.415 &lt; 2e-16 *** ## MONTH10:SW_IN_F 0.0097277 0.0009761 9.966 &lt; 2e-16 *** ## MONTH11:SW_IN_F -0.0011415 0.0010932 -1.044 0.29640 ## MONTH12:SW_IN_F -0.0099745 0.0012972 -7.689 1.52e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.593 on 41277 degrees of freedom ## Multiple R-squared: 0.6237, Adjusted R-squared: 0.6234 ## F-statistic: 2974 on 23 and 41277 DF, p-value: &lt; 2.2e-16 8.2.2.3 Polynomial regression Furthermore, the relationships between variables may be non-linear. In the previous example, we see that the increase in GPP saturates as shortwave radiation grows, which suggests that the true relationship could be represented by a curve. There are many regression methods that fit this kind of relationship, like polynomial regression, LOESS (local polynomial regression fitting), etc. Let’s fit a simple quadratic regression model, just for the month of August. For this we use the poly() function which constructs orthogonal polynomials of a given degree: quadmod &lt;- lm(GPP_NT_VUT_REF ~ poly(SW_IN_F, 2), data = df_cat |&gt; filter(MONTH == 8)) summary(quadmod) ## ## Call: ## lm(formula = GPP_NT_VUT_REF ~ poly(SW_IN_F, 2), data = filter(df_cat, ## MONTH == 8)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.367 -2.055 -0.253 1.801 32.375 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.13084 0.07944 89.77 &lt;2e-16 *** ## poly(SW_IN_F, 2)1 447.25113 4.61907 96.83 &lt;2e-16 *** ## poly(SW_IN_F, 2)2 -151.08797 4.61907 -32.71 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.619 on 3378 degrees of freedom ## Multiple R-squared: 0.7556, Adjusted R-squared: 0.7555 ## F-statistic: 5223 on 2 and 3378 DF, p-value: &lt; 2.2e-16 In the following plot, you can see how the model fit for GPP in August improves as we consider higher degree polynomials: df_cat |&gt; filter(MONTH == 8) |&gt; ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(alpha = 0.4) + geom_smooth(formula = y ~ x, method = &quot;lm&quot;, aes(color = &quot;lm&quot;), se = FALSE) + geom_smooth(formula = y ~ poly(x, 2), method = &quot;lm&quot;, aes(color = &quot;poly2&quot;), se = FALSE) + geom_smooth(formula = y ~ poly(x, 3), method = &quot;lm&quot;, aes(color = &quot;poly3&quot;), se = FALSE) + geom_smooth(formula = y ~ poly(x, 4), method = &quot;lm&quot;, aes(color = &quot;poly4&quot;), se = FALSE) + labs(x = &quot;SW&quot;, y = &quot;GPP&quot;, color = &quot;Regression&quot;) + theme_classic() 8.2.2.4 Metrics for regression evaluation Metrics measure the quality of fit between predicted and observed values, are essential to model training (where the metric defines the loss function, see Chapter 10), and inform model evaluation. Different metrics measure different aspects of the model-data agreement. In other words, a single metric never captures all aspects and patterns of the model-data agreement. Therefore, a visual inspection of the model fit is always a good start of the model evaluation. To get an intuitive understanding of the different abilities of different metrics, compare the scatterplots in Fig. 8.1 and how different aspects of the model-data agreement are measured by different metrics. The observed target values \\(y\\) are plotted against the predicted values \\(\\hat{y}\\) from a regression model, and the dashed line represents the ideal scenario: our predictions matching the data perfectly. Definitions of the metrics displayed and other metrics are given below. Figure 8.1: Correlation plots between observed and fitted target values. Common metrics used for evaluating regression fits are: MSE The mean squared error is defined, as its name suggests, as: \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{Y_i})^2 \\]It measures the magnitude of the errors, and is minimized to fit a linear regression or, as we will see in Chapter 9, during model training when used as a loss function. Note that since it scales with the square of the errors, the MSE is sensitive to large errors in single points, including outliers. RMSE The root mean squared error is, as its name suggests, the root of the MSE: \\[ \\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{Y_i})^2} \\]Like the MSE, the RMSE also measures the magnitude of the errors and is minimized during model training. By taking the square root of mean square errors, the RMSE is in the same units as the data \\(Y\\) and is less sensitive to outliers than the MSE. MAE The mean absolute error is similarly defined: \\[ \\text{MAE} = \\frac{1}{n} \\sum_{i = 1}^{n} |Y_i - \\hat{Y_i}| \\] Measuring the discrepancies between predictions and observations using absolute errors, instead of squared errors, gives less importance to errors of large magnitude and more importance to small errors than the MSE would. Hence, this measures is more stable in the presence of outliers. \\(R^2\\) - coefficient of determination describes the proportion of variation in \\(y\\) that is captured by modelled values \\(\\hat{y_i}\\). It tells us how much better our fitted values \\(\\hat{y_i}\\) are than just taking the average of the target \\(\\bar{y}\\) as predictions. In this case, the goal is to maximize the metric, thus trying the explain as much variation as possible. In contrast to the MSE and RMSE, \\(R^2\\) measures consistency, or correlation, or goodness of fit. It is defined as: \\[ R^2 = 1 - \\frac{\\sum_i (\\hat{Y}_i - Y_i)^2}{\\sum_i (Y_i - \\bar{Y})^2}\\\\ \\] When the regression model is fitted by minimizing the MSE, the \\(R^2\\) takes values between 0 and 1. A perfect fit is quantified by \\(R^2 = 1\\). There is no generally valid threshold of \\(R^2\\) for a model to be considered “good”. It depends on the application and the nature of the data and the data-generating process. Note that the above equation can also be written as \\(R^2 = 1 - \\text{MSE}/var(Y)\\). \\(r\\) - Pearson’s correlation The linear association between two variables (here \\(y\\) and \\(\\hat{y}\\)) is measured by the Pearson’s correlation coefficient \\(r\\). \\[ r = \\frac{\\sum_i (Y_i - \\bar{Y}) (\\hat{Y_i} - \\bar{\\hat{Y}}) }{\\sqrt{ \\sum_i(Y_i-\\bar{Y})^2 \\; (\\hat{Y_i}-\\bar{\\hat{Y}})^2 } } \\] The correlation calculated between the target \\(Y\\) and a predictor \\(X\\) can tell us about the predictive power of \\(X\\) in a linear regression model (the higher the correlation, the more powerful). We can also compute the correlation between the target \\(Y\\) and the predicted values \\(\\hat{Y}\\) by a model (multivariate, or even not linear) to assess the adequacy of the model chosen. See Figure 8.1 as an example. It is noteworthy to mention that correlation is location and scale invariant, hence it will not detect model deviations like the ones in the middle row plots. The squared value of the Pearson’s r is often reported as “\\(R^2\\)” but doesn’t correspond to the definition of the coefficient of determination given above. However, the square of the Pearson’s r is closely related to the coefficient of determination \\(R^2\\). For a linear regression, fitted minimizing the MSE, they are identical (see proof here). In subsequent chapters, we will use “\\(R^2\\)” to refer to the square of the Pearson’s r between the observed \\(Y\\) and predicted \\(\\hat{Y}\\) values. Note the implementations in R.The \\(R^2\\) reported by the generic summary() function corresponds to the base-R function cor()^2 , to yardstick::rsq(), and to the definition of the square of the Pearson’s \\(r\\) given above. The yardstick::rsq_trad() returns the coefficient of determination as traditionally defined and is not equal to the values above, unless computed on the predicted values \\(\\hat{y}\\). Sometimes the Person’s version is computed between \\(y\\) and \\(x\\), and it leads to the same number due to its “location and scale invariant” property. Nevertheless, this is conceptually wrong, as we should look at the predictions, not the predictors: We are not predicting \\(y\\) by just giving the values of \\(x\\) instead. Hence, especially when using {yardstick} functions, make sure you compute the values on \\(\\hat{y}\\). When we have several predictors, it’s already clear that we should compare \\(y\\) to \\(\\hat{y}\\) instead of \\(y\\) to each predictor separately. # generate correlated random data set.seed(1982) df &lt;- tibble(x = rnorm(100)) |&gt; mutate(y = x + rnorm(100)) |&gt; mutate(y_fitted = lm(y ~ x)$fitted.values) # implementations using Pearson&#39;s correlation summary(lm(y ~ x, data = df))$r.squared ## [1] 0.6186521 cor(df$y, df$x)^2 # remember: location and scale invariant ## [1] 0.6186521 yardstick::rsq(df, y, x) |&gt; pull(.estimate) ## [1] 0.6186521 (sum((df$x - mean(df$x))*(df$y - mean(df$y))))^2/ (sum((df$y - mean(df$y))^2)*sum((df$x - mean(df$x))^2)) ## [1] 0.6186521 # implementations using coefficient of determination definition 1 - sum((df$x - df$y)^2) / sum((df$y - mean(df$y))^2) # should be \\hat{y}, not x ## [1] 0.5993324 yardstick::rsq_trad(df, y, x) %&gt;% pull(.estimate) # incorrect ## [1] 0.5993324 yardstick::rsq_trad(df, y, y_fitted) %&gt;% pull(.estimate) # correct ## [1] 0.6186521 An “\\(R^2\\)” is commonly reported when evaluating the agreement between observed and predicted values of a given model. When the correlation between two different variables in a sample is quantified, \\(r\\) is commonly used to reflect also whether the correlation is positive or negative (\\(r\\) can attain positive or negative values in the interval \\([-1, 1]\\)). The coefficient of determination can return negative values when comparing observed and predicted values for uninformative estimates (worse than just using the average of \\(Y\\)) and is thus not actually bound between 0 and 1. Therefore, be careful with the interpreration of “\\(R^2\\)” and think on which variables it was computed and with which method. Bias The bias is simply the mean error: \\[ \\text{Bias} = \\frac{1}{n} \\sum_i^n{(\\hat{Y}_i - Y_i)} \\] Slope The slope refers to the slope of the linear regression line between predicted and observed values. It is returned as the second element of the vector returned by coef(lm(..)): coef(lm(y ~ y_fitted, data = df))[2] ## y_fitted ## 1 8.2.2.5 Metrics for regression model comparison In general, the aim of regression modelling is to find a model that best explains the data - but not the random errors in the data. More complex models tend to overfit more than simpler models. The implication of overfitting is that the model fits the data used for model fitting well, but doesn’t fit well when evaluating the predictions of the same model to new data (data not used for model fitting). In such a case, the model’s generalisability is poor. We’ll learn more about overfitting and generalisability in the context of supervised machine learning in later chapters. Often, simpler models generalise better than more complex model. The challenge is to strike a balance between complexity and generalisability. But how to find the “sweet spot” of this trade-off? In this context it should be noted that the \\(R^2\\) always increases when predictors are added to a model. Therefore, the \\(R^2\\) is not a suitable metric for comparing models that differ with respect to their number of predictors - a factor controlling model complexity. Cross-validation can be regarded as the “gold-standard” for measuring model generalisability if the data is plentiful. It will be introduced in the context of supervised machine learning in Chapter 10. However, when the data size is small, cross validation estimates may not be robust. Without resorting to cross validation, the effect of spuriously improving the evaluation metric by adding uninformative predictors can also be mitigated by penalizing the number of predictors \\(p\\). Different metrics are available: Adjusted \\(R^2\\) The adjusted \\(R^2\\) discounts values of \\(R^2\\) by the number of predictors. It is defined as \\[ {R}^2_{adj} = 1 - (1-R^2) \\; \\frac{n-1}{n-p-1} \\;, \\] where \\(n\\) (as before) is the number of observations, \\(p\\) the number of parameters and \\(R^2\\) the usual coefficient of determination. Same as for \\(R^2\\), the goal is to maximize \\(R^2_{adj}\\). AIC The Akaike’s Information Criterion is defined in terms of log-likelihood (covered in Quantitative Methoden) but for linear regression it can be written as: \\[ \\text{AIC} = n \\log \\Big(\\frac{\\text{SSE}}{n}\\Big) + 2(p+2) \\] where \\(n\\) is the number of observations used for estimation, \\(p\\) is the number of explanatory variables in the model and SSE is the sum of squared errors (SSE\\(= \\sum_i (Y_i-\\hat{Y_i})^2\\)). Also in this case we have to minimize it and the model with the minimum value of the AIC is often the best model for forecasting. Since it penalizes having many parameters, it will favor less complex models. AIC\\(_c\\) For small values of \\(n\\) the AIC tends to select too many predictors. A bias-corrected version of the AIC is defined as: \\[ \\text{AIC}_c = \\text{AIC} + \\frac{2(p + 2)(p + 3)}{n-p-3} \\] Also AIC\\(_c\\) is minimized for an optimal predictive model. BIC The Schwarz’s Bayesian Information Criterion is defined as \\[ \\text{BIC} = n \\log \\Big(\\frac{\\text{SSE}}{n}\\Big) + (p+2) \\log(n) \\] Also for BIC, the goal is to minimize it. This metric has the feature that if there is a true underlying model, the BIC will select that model given enough data. The BIC tends to select a model with fewer predictors than AIC. Implementation in R Let’s calculate the metrics introduced above for a few of the fitted regression models. Some of these metrics, like \\(R^2\\) and \\(R^2_{adj}\\) are given by the summary() function. Alternatively, the {yardstick} package provides implementations for a few of these metrics, which we compute below: compute_regr_metrics &lt;- function(mod){ p &lt;- length(mod$coefficients) n &lt;- length(mod$residuals) tibble( mse = mean(mod$residuals^2), R2 = summary(mod)$r.squared, R2_adj = summary(mod)$adj.r.squared, AIC = extractAIC(mod)[2], AIC_adj = extractAIC(mod)[2] + 2*(p+2)*(p+3)/(n-p-3), BIC = BIC(mod) # this implementation is based on log-likelihood ) } list_metrics &lt;- purrr::map( list(linmod1, linmod2, linmod_cat, quadmod), ~compute_regr_metrics(.)) names(list_metrics) &lt;- c(&quot;Linear model&quot;, &quot;Linear model 2&quot;, &quot;Linear + categories&quot;, &quot;Quadratic model&quot;) bind_rows(list_metrics, .id = &quot;type&quot;) ## # A tibble: 4 × 7 ## type mse R2 R2_adj AIC AIC_adj BIC ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Linear model 25.1 0.553 0.553 133058. 133058. 250293. ## 2 Linear model 2 24.8 0.558 0.558 132590. 132590. 249842. ## 3 Linear + categories 23.7 0.578 0.577 130700. 130700. 248030. ## 4 Quadratic model 21.3 0.756 0.755 10350. 10350. 19972. 8.2.3 Classification Classification models predict a categorical target variable. Note that predictors may be continuous. We will introduce a classification problem with a binary target, since it’s straightforward to generalize to categorical variables with more than two classes. As an example, let’s observe the CO2 dataset from the {datasets} package, which contains data from an experiment on the cold tolerance of a grass species. We will try to classify the origin of each plant (categorical variable Type with values Quebec or Mississippi) depending on the carbon dioxide uptake rate of the plant (continuous variable uptake measured in \\(\\mu\\)mol m\\(^{-2}\\)s\\(^{-1}\\)). More information on the dataset can be obtained by typing ?datasets::CO2 in the console. gg2 &lt;- datasets::CO2 |&gt; ggplot(aes(x = uptake, y = Type, color = Type)) + geom_point(size = 0.75, alpha = 0.8) + theme_classic() + theme(legend.position = &quot;none&quot;) gg2 At first sight, it’s easy to see that the carbon uptake is lower for the Mississippi type. Note that other predictors can be included in the model, but we’ll focus on a single predictor. Using this example, we’ll cover logistic regression, its implementation in R and metrics for classification. 8.2.3.1 Logistic regression Theory A classification problem is a bit more difficult to write mathematically than a regression problem. Before, the mathematical representation of GPP_NT_VUT_REF ~ SW_IN_F was GPP_NT_VUT_REF\\(\\;=\\; \\beta_0 + \\beta_1\\)SW_IN_F. With the classification model Type ~ uptake, we cannot just write Type\\(\\;=\\; \\beta_0 + \\beta_1\\)uptake because Type is not a number. Hence, the categorical variable must be encoded, in this case 0 represents Quebec and 1 represents Mississippi. The next issue is that a linear model makes continuous predictions in the entire real numbers space \\((-\\inf, \\inf)\\), but we want the predictions to be either 0 or 1. We can transform these values to be in the interval \\([0,1]\\) with a link function. For a binary response, it’s common to use a logit link function: \\[\\text{logit}(z) = \\frac{\\exp(z)}{1+\\exp(z)}.\\] curve(exp(x)/(1+exp(x)), -5, 5, ylab = &quot;logit(x)&quot;) Combining a linear model (with any type of predictors, like for regression) and a logit link function, we arrive to a logistic regression model: \\[f(X, \\beta) = \\text{logit}(\\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p) = \\frac{\\exp(\\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p)}{1 + \\exp(\\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p)}.\\] This predicted value can be understood as the probability of belonging to class 1 (in our example, Mississippi). A classification rule is defined such that an observation \\(X_{new}\\) with a predicted probability of belonging to class 1 higher than a given threshold \\(\\tau\\) (i.e. \\(f(X_{new}, \\beta) &gt; \\tau\\)) will be classified as 1; and if the predicted probability is smaller than the threshold, it will be classified as 0. A logistic regression model results in a linear classification rule. This means that the \\(p\\)-dimensional space will be divided in two by a hyperplane, and the points falling in each side of the hyperplane will be classified as 1 or 0. In the example above with carbon uptake as predictor, the classification boundary would be a point dividing the real line. If we include a second predictor, we would obtain a line diviting the 2-dimensional plane in two. Furthermore, to fit a logistic regression model means to calculate the maximum likelihood estimator of \\(\\beta\\) with an iterative algorithm. We will learn more about iterative model fitting, i.e. parameter optimization, in the context of supervised machine learning (Chapter 10). Implementation in R First, let’s see how the target variable is encoded. R directly loads the dataframe with Type as a factor and Quebec as the reference level. R factors work such that each factor level (here Quebec and Mississippi) corresponds to an integer value (its position given by levels(), here 1 and 2 respectively). We can fit a logistic model in R with this encoding. str(datasets::CO2) ## Classes &#39;nfnGroupedData&#39;, &#39;nfGroupedData&#39;, &#39;groupedData&#39; and &#39;data.frame&#39;: 84 obs. of 5 variables: ## $ Plant : Ord.factor w/ 12 levels &quot;Qn1&quot;&lt;&quot;Qn2&quot;&lt;&quot;Qn3&quot;&lt;..: 1 1 1 1 1 1 1 2 2 2 ... ## $ Type : Factor w/ 2 levels &quot;Quebec&quot;,&quot;Mississippi&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Treatment: Factor w/ 2 levels &quot;nonchilled&quot;,&quot;chilled&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ conc : num 95 175 250 350 500 675 1000 95 175 250 ... ## $ uptake : num 16 30.4 34.8 37.2 35.3 39.2 39.7 13.6 27.3 37.1 ... ## - attr(*, &quot;formula&quot;)=Class &#39;formula&#39; language uptake ~ conc | Plant ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; ## - attr(*, &quot;outer&quot;)=Class &#39;formula&#39; language ~Treatment * Type ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; ## - attr(*, &quot;labels&quot;)=List of 2 ## ..$ x: chr &quot;Ambient carbon dioxide concentration&quot; ## ..$ y: chr &quot;CO2 uptake rate&quot; ## - attr(*, &quot;units&quot;)=List of 2 ## ..$ x: chr &quot;(uL/L)&quot; ## ..$ y: chr &quot;(umol/m^2 s)&quot; levels(datasets::CO2$Type) ## [1] &quot;Quebec&quot; &quot;Mississippi&quot; To fit a logistic regression in R we can use the glm() function, which fits a generalized linear model, indicating that our target variable is binary and the link function is a logit function. Let’s see the model output: logmod &lt;- glm(Type ~ uptake, family = binomial(link = logit), data = datasets::CO2) summary(logmod) ## ## Call: ## glm(formula = Type ~ uptake, family = binomial(link = logit), ## data = datasets::CO2) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.29454 -0.66966 -0.02006 0.74112 1.68565 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.87192 0.87273 4.437 9.14e-06 *** ## uptake -0.14130 0.02992 -4.723 2.32e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 116.449 on 83 degrees of freedom ## Residual deviance: 83.673 on 82 degrees of freedom ## AIC: 87.673 ## ## Number of Fisher Scoring iterations: 4 This fitted model results in a linear classification boundary (discontinued line) that splits the predictor variables space in two. Where that line falls depends on the choice of threshold, in this case \\(\\tau=0.5\\) (see where the grey logistic regression line meets the dashed threshold line). You can see it plotted below: beta &lt;- coef(logmod) # reuse previous plot with classification line gg2.1 &lt;- datasets::CO2 |&gt; ggplot(aes(x = uptake, y = as.numeric(Type)-1, color = Type)) + geom_point(size = 0.75, alpha = 0.8) + ylab(&quot;&quot;) + theme_classic() + stat_smooth(method = &quot;glm&quot;, color = &quot;grey&quot;, se = FALSE, method.args = list(family=binomial)) + geom_vline(xintercept = -beta[1]/beta[2], lty = 2) gg2.1 ## `geom_smooth()` using formula = &#39;y ~ x&#39; Most blue points fall to one side of the dashed classification line and most red points to the other side; this is what we wanted. The points that are in the wrong side of the line are misclassified by the logistic regression model, we’re trying to minimize that. Note that, just like for linear regression, a logistic regression model allows to use categorical explanatory variables and polynomial transformations of the predictors to achieve better-fitting classification models. Model advantages and concerns One advantage of logistic regression is simplicity. It’s part of the generalized linear regression family of models and the concept of a link function used to build such a model can also be used for various types of response variables (not only binary, but also count data…). You can find more details in this Wikipedia article. Furthermore, logistic regression allows for an interesting interpretation of its model parameters: odds and log-odds. Odds represent how much likely it is to find one class versus the other (e.g. class 1 is twice as likely as class 0 whenever we have probabilities \\(66\\%\\) vs \\(33\\%\\)). The odds are defined as the probability of \\(Y\\) belonging to class 1 divided by the probabiity of belonging to class 0, and relates to the model parameters as \\[\\frac{P(Y_i=1)}{P(Y_i=0)} = \\exp(\\beta_0+\\beta_1 X_i).\\] So the log-odds are \\[\\log\\left(\\frac{P(Y_i=1)}{P(Y_i=0)}\\right) = \\beta_0+\\beta_1 X_i.\\] Increases in the values of the predictors affect the odds multiplicatively and the log-odds linearly. It is easy to extend a logistic regression model to more than two classes by fitting models iteratively. For example, first you classify class 1 against classes 2 and 3; then another logistic regression classifies class 2 against 3. Nevertheless, logistic regression relies on statistical assumptions to fit the parameters and interpret the fitted parameters. So whenever these assumptions are not met, one must be careful with the conclusions drawn. Other machine learning methods, that will be covered in Chapters 9 and 10, can also be used for classification tasks. These offer more flexibility than logistic regression (are not necessarily linear) and don’t need to satisfy strict statistical assumptions. 8.2.3.2 Metrics for classification Measuring the quality of a classification model is based on counting how many observations were correctly classified, rather than the distance between the values predicted by a regression and the true observed values. These can be represented in a confusion matrix: \\(Y = 1\\) \\(Y = 0\\) \\(\\hat{Y} = 1\\) True positives (TP) False positives (FP) \\(\\hat{Y} = 0\\) False negatives (FN) True negatives (TN) In a confusion matrix, correctly classified observations are on the diagonal and off-diagonal values correspond to different types of errors. Some of these error types are more relevant for certain applications. Imagine that you want to classify whether the water of a river is safe to drink based on measurements of certain particles or chemicals in the water (Y=1 means safe, Y=0 means unsafe). It’s much worse to tag as “safe” a polluted river than to tag as “unsafe” a potable water source, one must be conservative. In this case, we would prioritize avoiding false positives and wouldn’t care so much about false negatives. The following metrics are widely used and highlight different aspects of our modeling goals. Accuracy is simply the proportion of outputs that were correctly classified: \\[ \\text{Accuracy}=\\frac{\\text{TP} + \\text{TN}}{N},\\] where \\(N\\) is the number of observations. This is a very common metric for training ML models and treats both classes as equally important. It’s naturally extended to multi-class classification and usually compared to the value \\(\\frac{1}{C}\\) where \\(C\\) is the number of classes. Classification models are usually compared to randomness: How much better is our model compared to throwing a coin for classification? At random, we would assign each class \\(50\\%\\) of the time. So if we assume that both classes are as likely to appear, that is, they are balanced, the accuracy of a random guess would be around \\(0.5\\). Hence, we want the accuracy to be “better than random”. If there are \\(C\\) different classes and the observations are balanced, we want the accuracy to be above \\(1/C\\). A challenge is posed by imbalanced classes. For a dataset where \\(90\\%\\) of the observations are from class 1 and \\(10\\%\\) from class 0, always predicting 1 would lead to a accuracy of \\(0.9\\). This value may sound good, but that model is not informative because it doesn’t use any information from predictors. Therefore, be careful when working with imbalanced classes and interpreting your results. Precision measures how often our “positive” predictions are correct: \\[\\text{precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}.\\] The true positive rate (TPR), also called Recall or sensitivity measures the proportion of real “positives” (\\(Y = 1\\)) we are able to capture: \\[ \\text{TPR} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}.\\] The false positive rate (FPR) is defined by \\[\\text{FPR} = \\frac{\\text{FP}}{\\text{FP}+\\text{TN}}.\\] and is related to another metric called specificity by \\(\\text{FPR} = 1 - \\text{specificity}\\). Receiver operating characteristic (ROC) curve: To evaluate the performance of a binary classification model, it’s common to plot the ROC curve, where the TPR is plotted against the FPR, for varying values of the threshold \\(\\tau\\) used in the classification rule. When we decrease the threshold, we get more positive values (more observations are classified as 1), increasing both the true positive and false positive rate. The following image describes clearly how to interpret a ROC curve plot: ROC curves and how they compare, from Wikipedia. AUC: The “area under the curve” is defined as the area left below the ROC curve. For a random classifier we would have AUC=0.5 and for the perfect classifier, AUC=1. It’s good to try to increase the AUC and it’s used often as a reporting metric. Nevertheless, a visual inspection of the ROC curve can say even more. F1: The F1 score is a more sophisticated metric, defined as the harmonic mean of precision and sensitivity, or in terms of the confusion matrix values: \\[ F1= 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} = \\frac{2 \\text{TP}}{2 \\text{TP} + \\text{FP} + \\text{FN}}. \\] This metric provides good results for both balanced and imbalanced datasets and takes into account both the model’s ability to capture positive cases (recall) and be correct with the cases it does capture (precision). It takes values between 0 and 1, with 1 being the best and values of 0.5 and below being bad. These metrics can be used to compare the quality of different classifiers but also to understand the behaviour of a single classifier from different perspectives. This was an introduction of the most basic classification metrics. For a more information on the topic, check out this book chapter. Implementation in R Let’s take a look at the previous metrics for the logistic regression model we fitted before. The confusionMatrix() function from the {caret} library provides most of the statistics introduced above. # Make classification predictions Y &lt;- logmod$data$Type x &lt;- as.factor(round(logmod$fitted.values)) # Use 0.5 as threshold # Change class names levels(Y) &lt;- levels(x) &lt;- c(&quot;Quebec&quot;, &quot;Mississippi&quot;) # plot confusion matrix conf_matrix &lt;- caret::confusionMatrix(data = x, reference = Y) conf_matrix ## Confusion Matrix and Statistics ## ## Reference ## Prediction Quebec Mississippi ## Quebec 32 13 ## Mississippi 10 29 ## ## Accuracy : 0.7262 ## 95% CI : (0.618, 0.8179) ## No Information Rate : 0.5 ## P-Value [Acc &gt; NIR] : 2.039e-05 ## ## Kappa : 0.4524 ## ## Mcnemar&#39;s Test P-Value : 0.6767 ## ## Sensitivity : 0.7619 ## Specificity : 0.6905 ## Pos Pred Value : 0.7111 ## Neg Pred Value : 0.7436 ## Prevalence : 0.5000 ## Detection Rate : 0.3810 ## Detection Prevalence : 0.5357 ## Balanced Accuracy : 0.7262 ## ## &#39;Positive&#39; Class : Quebec ## Now we can visualize the confusion matrix as a mosaic plot. This is quite helpful when we work with many classes. mosaicplot(conf_matrix$table, main = &quot;Confusion matrix&quot;) 8.2.4 Model evaluation Model evaluation refers to several techniques that help you understand how the model performs, whether this behavior is what you expect and how you can improve it. You can use metrics and plots to get an overview of the weaknesses of your model. This section covers model comparison, variable selection and outlier detection, and more concepts related to model evaluation (overfitting, data pre-processing, cross-validation…) are explained in the remaining chapters. Concepts will be explained using regression as an example, but are directly translated to classification problems. 8.2.4.1 Model comparison Be systematic with your model comparisons. Three key ideas in model selection are: Comparisons should be hierarchical: compare a model to another that “contains it”, i.e. compare GPP_NT_VUT_REF ~ SW_IN_F to GPP_NT_VUT_REF ~ SW_IN_F + LW_IN_F, not GPP_NT_VUT_REF ~ SW_IN_F to GPP_NT_VUT_REF ~ NIGHT + TA_F. Complexity must be increased slowly: add one variable at a time, not three variables all at once. This helps avoid collinearity in the predictors. Choose the most appropriate metric: if possible, a metric that accounts for model complexity and represents the goal of your analysis (e.g., recall for a classification where you don’t want to miss any positives). If you’re considering different model approaches for the same task, you should first fit the best possible model for each approach, and then compare those optimized models to each other. For example, fit the best linear regression with your available data, the best KNN non-parametric regression model and a random forest; then compare those three final models and choose the one that answers your research question the best. One must be careful not to keep training or improving models until they fit the data perfectly, but maintain the models’ ability to generalize to newly available data. Chapter 9 introduces the concept of overfitting, which is central to data science. Think of model interpretation and generalization when comparing them, not only of performance. Simple models can be more valuable than very complex ones because they tell a better story about the data (e.g., by having few very good predictors rather than thousands of mediocre ones, from which we cannot learn the underlying relationships). 8.2.4.2 Variable selection Let’s think of variable selection in the context of linear regression. A brute force approach to variable selection would be: Fit a linear regression for each combination of available predictors, calculate a metric (e.g., AIC) and choose the best one (lowest AIC). The problem is, if you have 8 predictors, you would fit 40320 different regression models. This can be very computationally expensive. Instead, take a hierarchical, or “greedy”, approach, starting with an empty model (just an intercept) and adding one variable at a time. This is called stepwise (forward) regression. The algorithm goes as follows: First, you fit all regression models with just one variable and compute the \\(R^2\\). Then, select the one predictor leading to a model with the greatest \\(R^2\\) (best fitting model) and compute the AIC (or BIC). In the next step, compare remaining predictors to be added as a second variable in the model and calculating their \\(R^2\\). Choose as second predictor the one leading to the best \\(R^2\\). Then, compute the AIC. If the AIC (which accounts for model fit and complexity) is worse, that is, bigger, stop and keep the univariate linear model. If the AIC is better, that is, smaller, add the second variable and repeat the previous steps to include a third variable. The method finishes once you cannot reduce the AIC anymore, or when you run out of variables. In the end, you’ll have more or less the best possible linear regression model. The function step() implements the stepwise algorithm in R. This stepwise approach can also be done backwards, starting with a full model (all available variables) and removing one at a time. Or even with a back-and-forth approach, where you look at both including a new or removing an existing variable at each step (optimizing AIC). Furthermore, this algorithm can be applied to fitting a polynomial regression. We want to increase the degree of the polynomials unit by unit. For a model with categorical variables, interaction terms should only be considered after having the involved variables as “intercept only”. Multicollinearity exists when there is a correlation between multiple predictors in a multivariate regression model. This is problematic because it makes the estimated coefficients corresponding to the variables that are highly correlated very unstable. Since two highly correlated variables explain almost the same, it doesn’t matter whether we include one or the other in the model (the performance metrics will be similar) or even if we include both of them. Hence, it becomes difficult to say which variables actually influence the target. The variance inflation factor (VIF) is a score from economics that measures the amount of multicollinearity in regression, based on how the estimated variance of a coefficient is inflated due to its correlation with another predictor. It’s calculated as \\[\\text{VIF}_i = \\frac{1}{1 - R^2_i},\\] where \\(R^2_i\\) is the coefficient of determination for regressing the i\\(^{th}\\) predictor on the remaining ones. A VIF\\(_i\\) is computed for each predictor in the multivariate regression model we are evaluating, meaning: if \\(\\text{VIF}_i = 1\\) variables are not correlated; if \\(1 &lt; \\text{VIF}_i &lt; 5\\) there is moderate collinearity; and if \\(\\text{VIF}_i \\geq 5\\) they are highly correlated. Because they can be almost fully explained by all the other predictors (high \\(R^2_i\\)), these variables are redundant in our final model. To remedy collinearity, you may choose to use only one or two of those correlated variables. Another option would be to use Principal Component Analysis (PCA), which you may read more about here. What PCA does is to map the space of predictors into another of smaller dimension, leading to a smaller set of predictor variables \\(\\{Z_1, ... , Z_q\\}\\), each of them being a linear combination of all the initial available predictors, that is \\(Z_1 = \\alpha^1_0 + \\alpha^1_1 X_1 + ... + \\alpha^1_p X_p\\), etc. If you have collinearity, those highly correlated variables would be summarized into one single new variable, called principal component. When we work with high-dimensional data (that is, we have more variables than observations) there are better techniques to do variable selection than stepwise regression. Since the predictors space is so large, we could fit a line that passes through all the observations (a perfect fit), but does the model generalize? We don’t know. For example, Lasso and Ridge regression incorporate variable selection in the fitting process (you can check this post if you’re curious). 8.2.4.3 Outlier detection Detecting outliers is important, because they can affect the fit of a model and render the model fitting not robust. When the data is large, individual points have less influence on the model fitting. Therefore only outliers that are very far from normal values will affect the model fit (see below). Outliers are particularly problematic in the context of small data, because every value has a big influence on the fitted model. Take a look at the two linear regressions below and how one single weird observation can throw off the fit. Whenever an observation is very distant from the center of the predictor’s distribution, it becomes very influential (it has a big leverage). If the observed response for that data point is in harmony with the rest of points, nothing happens, but if it’s also off, the regression model will be affected greatly. set.seed(2023) hhdf_small &lt;- hhdf |&gt; sample_n(100) |&gt; # reduce dataset select(SW_IN_F, GPP_NT_VUT_REF) gg3 &lt;- hhdf_small |&gt; ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(size = 0.75) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, fullrange = TRUE) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() + ylim(-20, 40) + xlim(0, 2000) gg4 &lt;- hhdf_small |&gt; add_row(SW_IN_F = 2000, GPP_NT_VUT_REF = -20) |&gt; # add outlier ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(size = 0.75) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, fullrange = TRUE) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() + geom_point(aes(x = 2000, y = -20), colour=&#39;blue&#39;) + ylim(-20, 40) + xlim(0, 2000) cowplot::plot_grid(gg3, gg4) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; The first step to identifying outliers is to look at your data, one variable at a time. Plot a histogram to see the rough distribution of a variable. This will help identify what kind of values to expect. In Chapters 3 and 4 it was introduced how to identify values that fell out of this distribution using histograms and boxplots. Checking in the histogram if the distribution has fat tails helps to discern whether the values that pop out of a boxplot should be considered outliers or not. gg5 &lt;- hhdf_small |&gt; add_row(SW_IN_F = 2000, GPP_NT_VUT_REF = -20) |&gt; # add outlier ggplot(aes(x = GPP_NT_VUT_REF, y = after_stat(density))) + geom_histogram(fill = &quot;grey70&quot;, color = &quot;black&quot;) + geom_density(color = &#39;red&#39;)+ labs(title = &#39;Histogram, density and boxplot&#39;, x = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() gg6 &lt;- hhdf_small |&gt; add_row(SW_IN_F = 2000, GPP_NT_VUT_REF = -20) |&gt; # add outlier ggplot(aes(x = &quot;&quot;, y = GPP_NT_VUT_REF)) + geom_boxplot(fill = &quot;grey70&quot;, color = &quot;black&quot;) + coord_flip() + theme_classic() + theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) + labs(y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) gg7 &lt;- hhdf_small |&gt; add_row(SW_IN_F = 2000, GPP_NT_VUT_REF = -20) |&gt; # add outlier ggplot(aes(x = SW_IN_F, y = after_stat(density))) + geom_histogram(fill = &quot;grey70&quot;, color = &quot;black&quot;) + geom_density(color = &#39;red&#39;)+ labs(title = &#39;Histogram, density and boxplot&#39;, x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;))) + theme_classic() gg8 &lt;- hhdf_small |&gt; add_row(SW_IN_F = 2000, GPP_NT_VUT_REF = -20) |&gt; # add outlier ggplot(aes(x = &quot;&quot;, y = SW_IN_F)) + geom_boxplot(fill = &quot;grey70&quot;, color = &quot;black&quot;) + coord_flip() + theme_classic() + theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) + labs(y = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;))) cowplot::plot_grid(gg5, gg7, gg6, gg8, ncol = 2, rel_heights = c(2,1), align = &#39;v&#39;, axis = &#39;lr&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. A Q-Q Plot depicts the sample quantiles of a variable against the theoretical quantiles of a distribution of our choice, usually a normal distribution. In the histograms above, GPP looks somewhat Gaussian but with fatter tails and slightly skewed to the right, while shortwave radiation is skewed to the right, resembling an exponential distribution. This is also visible in the Q-Q plots below, because outliers deviate greatly from the straight line (which represents a match between the observed values and the theoretical distribution): gg9 &lt;- hhdf_small |&gt; add_row(SW_IN_F = 2000, GPP_NT_VUT_REF = -20) |&gt; # add outlier |&gt; ggplot(aes(sample = GPP_NT_VUT_REF)) + geom_qq() + geom_qq_line() + labs(y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), x = &quot;Theoretical normal quantiles&quot;) + theme_classic() gg10 &lt;- hhdf_small |&gt; add_row(SW_IN_F = 2000, GPP_NT_VUT_REF = -20) |&gt; # add outlier |&gt; ggplot(aes(sample = SW_IN_F)) + geom_qq() + geom_qq_line() + labs(y = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), x = &quot;Theoretical normal quantiles&quot;) + theme_classic() cowplot::plot_grid(gg9, gg10, ncol=2) For linear (and logistic) regression, we would like predictor variables to look as normal, i.e. gaussian, as possible. You’ve probably learned some of the reasons for this in quantitative methods courses, but are beyond the scope of this class. It’s common to study the distribution of the regression residuals with QQ-plots to assess if model assumptions are met. Above, you can see the distributions of our target and predictor (with outliers). And it’s very easy to see the weird value for the shortwave radiation but for GPP it doesn’t stick out so much. This already points to how important it is to check their multivariate distribution. R provides some useful plots from the fitted regression objects, in particular the “Residuals vs Leverage” plot: # Fit regression with outlier linmod_outlier &lt;- lm(GPP_NT_VUT_REF ~ SW_IN_F, data = add_row(hhdf_small, SW_IN_F = 2000, GPP_NT_VUT_REF = -20)) plot(linmod_outlier, 5) This plot shows the leverage (see the mathematical definition here) of each observation against the corresponding residual from the fitted linear regression. Points with high leverage, i.e., points that are far from the center of the predictor distribution, and large residuals, i.e., points that are far from the fitted regression line, are very influential. Cook’s distance (definition here) is an estimate of the influence of a data point in a linear regression and observations with Cook’s distance &gt; 1 are candidates for being outliers. See in the plot above how the point with index 101 (our added outlier) has a very large Cook’s distance. Boundary regions for Cook’s distance equal to 0.5 (suspicious) and 1 (certainly influential) are drawn with a dashed line. Finally, it’s very important that, before you remove a value because it may be an outlier, you understand where the data came from and if such an abnormal observation is possible. If it depicts an extraordinary but possible situation, this information can be very valuable and it’s wiser to keep it in the model. Interesting research questions arise when data doesn’t align with our preconceptions, so keep looking into it and potentially collect more data. 8.3 Report Exercise This tutorial’s exercise is a bit special. Although there is a lot of helpful information out there, you will often have to write your own data analysis routine. This requires good understanding of statistical knowledge, algorithmic thinking and problem-solving skills. While writing your code, you will face many questions and bugs that you need to solve. And knowing where and how to ask for help properly are crucial parts of this process (see second chapter for more on getting help). To learn these skills, you will attempt to write your own step-wise forward regression from scratch using the theory and code provided this and previous chapters (see task statement and hints below). Important: We do not expect that all of you solve this exercise without any issues - which is completely fine! Thus, in your report, you will add the code up to the point you got stuck and you will have to add a minimum reproducible example (MRE), following this guideline. Write this MRE as if you were addressing an online audience; describe in detail what you goal is, where you got stuck, what the error message is, provide a code example that is runnable without needing the .csv files that you have locally (see dput()), etc. If you can solve this problem without any issues, congratulations! But it is likely that you did not do so without facing any error messages. So, take one of these errors that you faced and write a MRE. Code errors can break knitting your report. To avoid having issues knitting your report from RMarkdown to HTML, make sure that you set the chunk settings, that hold erroneous code, to eror = TRUE, or skip running them entirely using eval = FALSE (more info here). Important tools to code a step-wise regression are nested for-loops, if-statements, and finding the best predictor for a given target variable. Instead of trying all of this at once, it is advisable to test your code in separate pieces that can then be stitched together. To help you out with these concepts, we provide two “warm-up exercises” (solutions are at the end of this book). 8.3.1 Task Statement Re-read the section on stepwise regression of this tutorial and write a pseudo-code for how you would encode a stepwise regression, write-out using coding terms like “I have to loop over all predictors and for each predictor I have to check…”. For picking the next-best variable to add, you can pick your metric of choice: \\(R^2\\), adjusted-\\(R^2\\), the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC). Visualise the result of the step-wise regression so that it shows how the chosen metric varies with the number of predictors, preferably showing the name of the additional predictor (not that you don’t always have to use graphs, you can also create a simple table for this). Use the code chunk below to start with an already cleaned dataset. df &lt;- readr::read_csv(&quot;data/df_for_stepwise_regression.csv&quot;) ## Rows: 9975 Columns: 17 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): siteid ## dbl (15): TA_F, SW_IN_F, LW_IN_F, VPD_F, PA_F, P_F, WS_F, TA_F_MDS, SW_IN_F... ## date (1): TIMESTAMP ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Hints: Really try to think of the blueprint (pseudo-code) first: How to go through different models in each forward step? How to store predictors added to the model and how to update candidate predictors? You will have to add predictors sequentally to your final model as you search through all predictors. You can use new_list &lt;- list() to create an empty vector, and then add elements to that list by new_list &lt;- c(new_list, new_element). It may be helpful to explicitly define a set of “candidate predictors” that are potentially added to the model (e.g., preds_candidate), and define predictors retained in the model from the previous step in a separate vector (e.g., preds_retained). In each step, search through preds_candidate, select the best predictor, add it to preds_retained and remove it from preds_candidate. At each step, record the metrics and store them in a data frame for later plots. As in the first “warm-up” exercise, you may record metrics at each step as a single-row data frame and sequentially stack (bind) them together. You will have to create new formulas for models dynamically, based on the candidate predictors. A clever way how to construct formulas dynamically is described in this stackoverflow post. For visualising: To display a table nicely as part of the RMarkdown html output, use the function knitr::kable(). To avoid reordering the list of variable names when plotting, change the type of variable names from “character” to “factor” by pred &lt;- factor(pred, levels = pred) 8.3.2 Warm-Up Exercises 8.3.2.1 Nested loops and if-statements Given a matrix A and a vector B (see below), do the following tasks: Replace the missing values (NA) in the first row of A by the largest value of B. After using that element of B for imputing A, drop that element from the vector B and proceed with imputing the second row of A, using the (now) largest value of the updated vector B, and drop that element from B after using it for imputing A. Repeat the same procedure for all four rows in A. After imputing (replacing) in each step, calculate the mean of the remaining values in B and record it as a single-row data frame with two columns row_number and avg, where row_number is the row number of A where the value was imputed, and avg is the mean of remaining values in B. As the algorithm proceeds through rows in A, sequentially bind the single-row data frame together so that after completion of the algorithm, the data frame contains four rows (corresponding to the number of rows in A). A &lt;- matrix(c(6, 7, 3, NA, 15, 6, 7, 8, 9, 12, 6, 11, NA, 3, 9, 4, 7, 3, 21, NA, 6, 7, 19, 6, NA, 15, 8, 10), nrow = 4, byrow = TRUE) B &lt;- c(8, 4, 12, 9, 15, 6) Before implementing these tasks, try to write down a pseudo code. This is code-like text that may not be executable, but describes the structure of real code and details where and how major steps are implemented. Next, you’ll need to write actual R code. For this, you will need to find answers to the following questions: How to go through each of the element in matrix? How to detect NA value? How to drop an element of a given value from a vector? How to add a row to an existing data frame? 8.3.2.2 Find the single best predictor The first step of a stepwise forward regression is to find the single most powerful predictor in a univariate linear regression model for the target variable GPP_NT_VUT_REF among all available predictors in our data set (all except those of type date or character). Implement this first part of the search, using the definition of the stepwise-forward algorithm below. Remove all rows with at least one missing value before starting the predictor search. Algorithm: Let \\(\\mathcal{M_0}\\) denote the null model, which contains no predictors. For \\(k=0,..,p-1\\): Consider all \\(p − k\\) models that augment \\(\\mathcal{M}_k\\) with one additional predictor. Choose the best model among these \\(p − k\\) models, and call it \\(\\mathcal{M}_{k+1}\\). Here best is defined as having the highest \\(R^2\\) . Hints: The “counter” variables in the for loop can be provided as a vector, and the counter will sequentially take on the value of each element in that vector. For example: all_predictors &lt;- df |&gt; dplyr::select(-GPP_NT_VUT_REF) |&gt; names() cat(&quot;Predictor variables and their first 3 entries in df: \\n&quot;) ## Predictor variables and their first 3 entries in df: for (var in all_predictors){ cat(&quot;\\n&quot;, var, &quot;: &quot;) cat(head(df[[var]], 3)) # cat(&quot;\\n&quot;) } ## ## siteid : CH-Dav CH-Dav CH-Dav ## TIMESTAMP : 13298 13299 13300 ## TA_F : -0.776 -1.081 -0.671 ## SW_IN_F : 183.144 266.34 165.87 ## LW_IN_F : 270.707 257.157 272.287 ## VPD_F : 1.077 0.77 0.938 ## PA_F : 82.815 83.214 83.667 ## P_F : 5 1.6 1.9 ## WS_F : 3.357 2.568 2.24 ## TA_F_MDS : -0.776 -1.081 -0.671 ## SW_IN_F_MDS : 183.144 266.34 165.87 ## LW_IN_F_MDS : 295.751 289.458 278.013 ## VPD_F_MDS : 1.077 0.77 0.938 ## CO2_F_MDS : 360.869 355.607 351.407 ## PPFD_IN : 372.752 499.9783 342.5253 ## USTAR : 0.65121 0.4633587 0.4450758 To record \\(R^2\\) values for the different models, you may start by creating an empty vector (vec &lt;- c()) before the loop and then sequentially add elements to that vector inside the loop (vec &lt;- c(vec, new_element)). Alternatively, you can do something similar, but with a data frame (initialising with df_rsq &lt;- data.frame() before the loop, and adding rows by df_rsq &lt;- bind_rows(df_rsq, data.frame(pred = predictor_name, rsq = rsq_result)) inside the loop). "],["supervisedmli.html", "Chapter 9 Supervised machine learning I 9.1 Learning objectives 9.2 Tutorial 9.3 Report Exercises", " Chapter 9 Supervised machine learning I Chapter lead author: Benjamin Stocker 9.1 Learning objectives Machine learning may appear magical. The ability of machine learning algorithms to detect patterns and make predictions is fascinating. However, several challenges have to be met in the process of formulating, training, and evaluating the models. In this and the next chapter (Chapter 10), we will discuss some basics of supervised machine learning and how to achieve best predictive results. Basic steps of the implementation of supervised machine learning are introduced, including data splitting, pre-processing, model formulation, and the implementation of these steps using the {caret} and {recipes} R packages. A focus is put on learning the concept of the bias-variance trade-off and overfitting. Contents of this Chapter are inspired and partly adopted by the excellent book Hands-On Machine Learning in R by Boehmke &amp; Greenwell. 9.2 Tutorial 9.2.1 What is supervised machine learning? Supervised machine learning is a type of machine learning where the model is trained using labeled data and the goal is to predict the output for new, unseen data. In contrast, unsupervised machine learning is a type of machine learning where the algorithms learn from data without being provided with labeled targets. The algorithms aim to identify patterns and relationships in the data without any guidance. Examples include clustering and dimensionality reduction. In supervised machine learning, we use a set of predictors \\(X\\) (also known as features, or labels, or independent variables) and observed values of a target variable \\(Y\\) that are recorded in parallel, to find a model \\(f(X) = \\hat{Y}\\) that yields a good match between \\(Y\\) and \\(\\hat{Y}\\) and that can be used for reliably predicting \\(Y\\) for new (“unseen”) data points \\(X_\\text{new}\\) - data that has not been used during model fitting/training. The hat on \\(\\hat{Y}\\) denotes an estimate. Some algorithms can even handle predictions of multiple target variables simultaneously (e.g., neural networks). From above definitions, we can note a few key ingredients of supervised machine learning: Input data (predictors) Target data recorded in parallel with predictors A model that estimates \\(f(X) = \\hat{Y}\\), made of mathematical operations relating \\(X\\) to \\(\\hat{Y}\\) and of model parameters (coefficients) that are calibrated to yield the best match of \\(Y\\) and \\(\\hat{Y}\\) A metric measuring how good the match between \\(Y\\) and \\(\\hat{Y}\\) is - the loss function An algorithm (the optimiser) to find the best set of parameters that minimize the loss Supervised machine learning ingredients, adopted from Chollet &amp; Allaire (2018) Deep Learning with R The type of modelling approach of supervised machine learning is very similar to fitting regression models as we did in Chapter @ref{#regressionclassification}. In a sense, supervised machine learning is just another empirical (or statistical) modelling approach. However, you may not want to call linear regression a machine learning algorithm because there is no iterative learning involved. Furthermore, machine learning differs from traditional statistical modelling methods in that it makes no assumptions regarding the data generation process and underlying distributions (Breiman, 2001). Nevertheless, contrasting a bivariate linear regression model with a complex machine learning algorithm is instructive. Also linear regression provides a prediction \\(\\hat{Y} = f(X)\\), just like other (proper) machine learning algorithms do. The functional form of a bivariate linear regression is not particularly flexible (just a straight line for the best fit between predictors and targets) and it has only two parameters (slope and intercept). At the other extreme are, for example, deep neural networks. They are extremely flexible, can learn highly non-linear relationships and deal with interactions between a large number of predictors. They also contain very large numbers of parameters (typically on the order of \\(10^4 - 10^7\\)). You can imagine that their high flexibility allows these types of algorithms to very effectively learn from the data, but also bears the risk of overfitting. What is overfitting? 9.2.2 Overfitting This example is based on this example from scikit-learn. Let’s assume that there is some true underlying relationship between a single predictor \\(X\\) and the target variable \\(Y\\). We don’t know this relationship (in the code below, this is true_fun()) and the observations contain a (normally distributed) error (y = true_fun(x) + 0.1 * rnorm(n_samples)). Based on our training data (df_train), we fit three polynomial models that differ with respect to their complexity. We fit a polynomial of degree 1, 4, and 15 to the observations. A polynomial of degree \\(N\\) is given by: \\[ y = \\sum_{n=0}^N a_n x^n \\] \\(a_n\\) are the coefficients, i.e., model parameters. The goal of the training is to find the coefficients \\(a_n\\) so that the predicted \\(\\hat{Y}\\) fits observed \\(Y\\) best. From the above definition, the polynomial of degree 15 has 16 parameters, while the polynomial of degree 1 has two parameters (and corresponds to a simple bivariate linear regression). You can imagine that the polynomial of degree 15 is much more flexible and should thus yield the closest fit to the training data. This is indeed the case. We can use the same fitted models on data that was not used for model fitting - the test data. This is what’s done below. Again, the same true underlying relationship is used, but we sample a new set of data points \\(X\\) and add a new sample of errors on top of the true relationship. You see that, using the test set, we find that “poly4” actually performs the best - it has a much lower RMSE than “poly15”. Apparently, “poly15” was overfitted. Apparently, it used its flexibility to fit not only the shape of the true underlying relationship, but also the observation errors on top of it. This has the implication that, when this model is used for making predictions for data that was not used for training (modl calibration, model fitting), it will yield misguided predictions that are affected by the errors in the training set. This is the reason why “poly15” performed worse on the test set than the other models. From the figures above, we can also conclude that “poly1” was underfitted - it performed worse than “poly4” also on the validation set. The out-of-sample performance of “poly15” gets even worse when applying the fitted polynomial models to data that extends beyond the range in \\(X\\) that was used for model training. Here, we’re extending just 20% to the right. You see that the RMSE for “poly15” literally explodes. The model is hopelessly overfitted and completely useless for prediction, although it looked like it fitted the data best when we considered only the training results. This is a fundamental challenge in machine learning - finding the model with the best generalisability. That is, a model that not only fits the training data well, but also performs well on unseen data. The phenomenon of fitting and overfitting as a function of the model complexity is also referred to as the bias-variance trade-off. The bias describes how well a model matches the training set (average error). A model with low bias will match the data set closely and vice versa. The variance describes how much a model changes when you train it using different portions of your data set. “poly15” has a high variance, but much of its variance is the result of misled training on observation errors. On the other extreme, “poly1” has a high bias. It’s not affected by the noise in observations, but its predictions are also far off the observations. In ML, we are challenged to balance this trade-off. This chapter and the next chapter introduce the methods for achieveing the best model generalisability and find the sweet spot between high bias and high variance. One of the key steps of the machine learning modelling process is motivated by the example above: the separation of the data into a training and a testing set (data splitting). Only by withholding part of the data from the model training, we have a good basis for testing the model on that unseen data for evaluating its generalisability. Additional steps that may be required or beneficial for effective model training and their implementation in R are introduced in this and the next Chapter. Depending on your application or research question, it may also be of interest to evaluate the relationships embodied in \\(f(X)\\) or to quantify the importance of different predictors in our model. This is referred to as model interpretation and is introduced in Chapter @ref(interpretable_ml). Of course, a plethora of algorithms exist that do the job of \\(y = f(X)\\). Each of them has its own strengths and limitations. It is beyond the scope of this course to introduce a larger number of ML algorithms. For illustration purposes in this chapter, we will use and introduce the K-nearest-Neighbors (KNN) algorithm and compare its performance to a multivariate linear regression for illustration purposes. Chapter @ref(supervised_ml_b) introduces Random Forest. 9.2.3 Data and the modelling challenge We’re returning to ecosystem flux data that we’ve used Chapters @ref(data_wrangling) and @ref(data_vis). Here, we’re using daily data from the evergreen site in Davos, Switzerland (CH-Dav) to avoid effects of seasonally varying foliage cover for which the data does not contain information. To address such additional effects, we would have to, for example, combine the flux and meteorological data with remotely sensed surface greenness data. The data set FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv contains a time series of the ecosystem gross primary production (GPP) and a range of meteorological variables, measured in parallel. In this chapter, we formulate a model for predicting GPP from a set of covariates (other variables that vary in parallel, here the meteorological variables). This is to say that GPP_NT_VUT_REF is the target variable, and other variables that are available in our dataset are the predictors. Let’s read the data, select suitable variables, and interpret missing value codes, and select only good-quality data (where at least 80% of the underlying half-hourly data was good quality measured data, and not gap-filled). ddf &lt;- read_csv(&quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot;) |&gt; # select only the variables we are interested in dplyr::select(TIMESTAMP, GPP_NT_VUT_REF, # the target ends_with(&quot;_QC&quot;), # quality control info ends_with(&quot;_F&quot;), # includes all all meteorological covariates -contains(&quot;JSB&quot;) # weird useless variable ) |&gt; # convert to a nice date object dplyr::mutate(TIMESTAMP = ymd(TIMESTAMP)) |&gt; # set all -9999 to NA dplyr::na_if(-9999) |&gt; # retain only data based on &gt;=80% good-quality measurements # overwrite bad data with NA (not dropping rows) dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC &lt; 0.8, NA, GPP_NT_VUT_REF), TA_F = ifelse(TA_F_QC &lt; 0.8, NA, TA_F), SW_IN_F = ifelse(SW_IN_F_QC &lt; 0.8, NA, SW_IN_F), LW_IN_F = ifelse(LW_IN_F_QC &lt; 0.8, NA, LW_IN_F), VPD_F = ifelse(VPD_F_QC &lt; 0.8, NA, VPD_F), PA_F = ifelse(PA_F_QC &lt; 0.8, NA, PA_F), P_F = ifelse(P_F_QC &lt; 0.8, NA, P_F), WS_F = ifelse(WS_F_QC &lt; 0.8, NA, WS_F)) |&gt; # drop QC variables (no longer needed) dplyr::select(-ends_with(&quot;_QC&quot;)) The steps above are considered data wrangling and are not part of the modelling process. After completing this tutorial, you will understand this distinction. (-&gt; exercise explain the distinction) 9.2.4 K-nearest neighbours Before we start with the model training workflow, let’s introduce the K-nearest neighbour (KNN) algorithm which is used here for demonstration purposes. It serves the purpose of demonstrating the bias-variance trade-off (see below). As the name suggests, KNN uses the \\(k\\) observations that are “nearest” to the new record for which we want to make a prediction. It then calculates their average (for regression) or most frequent value (for classification) and uses it as the prediction of the target value. “Nearest” is determined by some distance metric evaluated based on the values of the predictors. In our example (GPP_NT_VUT_REF ~ .), KNN would determine the \\(k\\) days where conditions, given by our set of predictors, were most similar (nearest) to the day for which we seek a prediction. Then, it calculates the prediction as the average (mean) GPP value of these days. Determining “nearest” neighbors is commonly based on either the Euclidean or Manhattan distances between two data points \\(x_a\\) and \\(x_b\\), considering all \\(P\\) predictors \\(j\\). Euclidean distance: \\[ \\sqrt{ \\sum_{j=1}^P (x_{a,j} - x_{b,j})^2 } \\\\ \\] Manhattan distance: \\[ \\sum_{j=1}^P | x_{a,j} - x_{b,j} | \\] In two-dimensional space, the Euclidean distance measures the length of a straight line between two points (remember Pythagoras!). The Manhattan distance is called this way because it measures the distance you would have to walk to get from point \\(a\\) to point \\(b\\) in Manhattan, New York, where you cannot cut corners but have to follow a rectangular grid of streets. \\(|x|\\) is the absolute value of \\(X\\) ( \\(|-x| = x\\)). KNN is a simple algorithm that uses knowledge of the “local” data structure for prediction. A drawback is that the model training has to be done for each prediction step and the computation time of the training increases with \\(x \\times p\\). KNNs are used, for example, to impute values (fill missing values) and have the advantage that predicted values are always within the range of observed values of the target variable. 9.2.5 Model formulation The aim of supervised ML is to find a model \\(\\hat{Y} = f(X)\\) so that \\(\\hat{Y}\\) agrees well with observations \\(Y\\). We typically start with a research question where \\(Y\\) is given - naturally - by the problem we are addressing and we have a data set at hand where one or multiple predictors (or “features”) \\(X\\) are recorded along with \\(Y\\). From our data, we have information about how GPP (ecosystem-level photosynthesis) depends on set of abiotic factors, mostly meteorological measurements. 9.2.5.1 Formula notation In R, it is common to use the formula notation to specify the target and predictor variables. You have encountered formulas before, e.g., for a linear regression using the lm() function. To specify a linear regression model for GPP_NT_VUT_REF with three predictors SW_IN_F, VPD_F, and TA_F, to be fitted to data ddf, we write: lm(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = ddf) 9.2.5.2 The generic train() The way we formulate a model can be understood as being independent of the algorithm, or engine, that takes care of fitting \\(f(X)\\). The R package caret provides a unified interface for using different ML algorithms implemented in separate packages. In other words, it acts as a wrapper for multiple different model fitting, or ML algorithms. This has the advantage that it unifies the interface - the way arguments are provided and outputs are returned. caret also provides implementations for a set of commonly used tools for data processing, model training, and evaluation. We’ll use caret here for model training with the function train(). Note however, that using a specific algorithm, which is implemented in a specific package outside caret, also requires that the respective package be installed and loaded. Using caret for specifying the same linear regression model as above, the base-R lm() function, can be done with caret in a generalized form as: caret::train( form = GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = ddf |&gt; drop_na(), # drop missing values trControl = caret::trainControl(method = &quot;none&quot;), # no resampling method = &quot;lm&quot; ) ## Linear Regression ## ## 2729 samples ## 3 predictor ## ## No pre-processing ## Resampling: None Note the argument specified as trControl = trainControl(method = \"none\"). This suppresses the default approach to model fitting in caret - to resample using bootstrapping. More on that below. Note also that we dropped all rows that contained at least one missing value - necessary to apply the least squares method for the linear regression model fitting. It’s advisable to apply this data removal step only at the very last point of the data processing and modelling workflow. Alternative algorithms may be able to deal with missing values and we want to avoid losing information along the workflow. Of course, it is an overkill compared to write this as in the chunk above compared to just writing lm(...). But the advantage of the unified interface is that we can simply replace the method argument to use a different model fitting algorithm. For example, to use KNN, we just can write: caret::train( form = GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = ddf |&gt; drop_na(), trControl = caret::trainControl(method = &quot;none&quot;), method = &quot;knn&quot; ) ## k-Nearest Neighbors ## ## 2729 samples ## 3 predictor ## ## No pre-processing ## Resampling: None 9.2.6 Data splitting The introductory example demonstrated the importance of validating the fitted model with data that was not used for training. Thus, we can test the model’s generalisability to new (“unseen”) data. The essential step that enables us to assess the model’s generalization error is to hold out part of the data from training and set it aside (leaving it absolutely untouched!) for testing. There is no fixed rule for how much data are to be used for training and testing, respectively. We have to balance a trade-off: Spending too much data for training will leave us with too little data for testing and the test results may not be robust. In this case, the sample size for getting robust validation statistics is not sufficiently large and we don’t know for sure whether we are safe from an over-fit model. Spending too much data for validation will leave us with too little data for training. In this case, the ML algorithm may not be successful at finding real relationships due to insufficient amounts of training data. Typical splits are between 60-80% for training. However, in cases where the number of data points is very large, the gains from having more training data are marginal, but come at the cost of adding to the already high computational burden of model training. In environmental sciences, the number of predictors is often smaller than the sample size (\\(p &lt; n\\)), because it is typically easier to collect repeated observations of a particular variable than to expand the set of variables being observed. Nevertheless, in cases where the number \\(p\\) gets large, it is important, and for some algorithms mandatory, to maintain \\(p &lt; n\\) for model training. An important aspect to consider when splitting the data is to make sure that all “states” of the system for which we have data are well represented in training and testing sets. A particularly challenging case is posed when it is of particular interest that the algorithm learns relationships \\(f(X)\\) under rare conditions \\(X\\), for example meteorological extreme events. If not addressed with particular measures, model training tends to achieve good model performance for the most common conditions. A simple way to put more emphasis for model training on extreme conditions is to compensate by sampling overly proportional from such cases for the training data set. Several alternative functions for the data splitting step are available from different packages in R. We use the the rsample package here as it allows to additionally make sure that data from the full range of a given variable’s values (VPD_F in the example below) are well covered in both training and testing sets. set.seed(123) # for reproducibility split &lt;- rsample::initial_split(ddf, prop = 0.7, strata = &quot;VPD_F&quot;) ddf_train &lt;- rsample::training(split) ddf_test &lt;- rsample::testing(split) Plot the distribution of values in the training and testing sets. ddf_train |&gt; dplyr::mutate(split = &quot;train&quot;) |&gt; dplyr::bind_rows(ddf_test |&gt; dplyr::mutate(split = &quot;test&quot;)) |&gt; tidyr::pivot_longer(cols = 2:9, names_to = &quot;variable&quot;, values_to = &quot;value&quot;) |&gt; ggplot(aes(x = value, y = ..density.., color = split)) + geom_density() + facet_wrap(~variable, scales = &quot;free&quot;) 9.2.7 Pre-processing Data pre-processing is aimed at preparing the data for use in a specific model fitting procedure and at improving the effectiveness of model training. The splitting of the data into a training and test set makes sure that no information from the test set is used during or before model training. It is important that absolutely no information from the test set finds its way into the training set (data leakage). In a general sense, pre-processing involve data transformations where the transformation functions use parameters that are determined on the data itself. Consider, for example, the standardization. That is, the linear transformation of a vector of values to have zero mean (data is centered, \\(\\mu = 0\\)) and a standard deviation of 1 (data is scaled to \\(\\sigma = 1\\)). In order to avoid data leakage, the mean and standard deviation have to be determined on the training set only. Then, the normalization of the training and the test sets both use set of (\\(\\mu, \\sigma\\)) determined on the training set. Data leakage would occur if the (\\(\\mu, \\sigma\\)) would be determined on data containing values from the test set. Often, multiple splits of the data are considered during model training. Hence, an even larger number of data transformation parameters (\\(\\mu, \\sigma\\) in the example of normalization) have to be determined and transformations applied to the multiple splits of the data. caret deals with this for you and the transformations do not have to be “manually” applied before applying the train() function call. Instead, the data pre-processing is considered an integral step of model training and instructions are specified as part of the train() function call and along with the un-transformed data. The recipes package provides an even more powerful way for specifying the formula and pre-processing steps in one go. It is compatible with the train() function of {caret}. For the same formula as above, and an example where the data ddf_train is to be normalized (centered and scaled), we can specify a “recipe” using the pipe operator as: pp &lt;- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = ddf_train) |&gt; recipes::step_center(all_numeric(), -all_outcomes()) |&gt; recipes::step_scale(all_numeric(), -all_outcomes()) The first line with the recipe() function call assigns roles to the different variables. GPP_NT_VUT_REF is an outcome (in “{recipes} speak”). Then, we used selectors to apply the recipe step to several variables at once. The first selector, all_numeric(), selects all variables that are either integers or real values. The second selector, -all_outcomes() removes any outcome (target) variables from this recipe step. The returned object pp does not contain a normalized version of the data frame ddf_train, but rather the information that allows us to apply a specific set of pre-processing steps also to any data set. The object pp can then be supplied to train() as its first argument: caret::train( pp, data = ddf_train, method = &quot;knn&quot;, trControl = caret::trainControl(method = &quot;none&quot;) ) The example above showed data standardization as a data pre-processing step. Data pre-processing may be done with different aims, as described in sub-sections below. 9.2.7.1 Standardization Several algorithms explicitly require data to be standardized so that values of all predictors vary within a comparable range. The necessity of this step becomes obvious when considering KNN, where the magnitude of the distance is strongly influenced by the order of magnitude of the predictor values. To get a quick overview of the distribution of all variables (columns) in our data frame, we can use the {skimr} package. ddf |&gt; summarise(across(where(is.numeric), ~quantile(.x, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE))) |&gt; t() |&gt; as_tibble(rownames = &quot;variable&quot;) |&gt; setNames(c(&quot;variable&quot;, &quot;min&quot;, &quot;q25&quot;, &quot;q50&quot;, &quot;q75&quot;, &quot;max&quot;)) ## # A tibble: 8 × 6 ## variable min q25 q50 q75 max ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GPP_NT_VUT_REF -4.23 0.773 2.87 5.45 12.3 ## 2 TA_F -21.9 -1.47 3.51 8.72 20.7 ## 3 SW_IN_F 3.30 77.8 135. 214. 366. ## 4 LW_IN_F 138. 243. 279. 308. 365. ## 5 VPD_F 0.001 0.959 2.23 4.06 16.6 ## 6 PA_F 80.4 83.2 83.7 84.1 85.6 ## 7 P_F 0 0 0 1.6 92.1 ## 8 WS_F 0.405 1.56 1.93 2.34 6.54 We see for example, that typical values of LW_IN_F are by a factor 100 larger than values of VPD_F. A distance calculated based on these raw values would therefore be strongly dominated by the difference in LW_IN_F values, and differences in VPD_F would hardly affect the distance. Therefore, the data must be standardized before using it with the KNN algorithm (and other algorithms, including Neural Networks). Standardization is done to each variable separately, by centering and scaling each to have \\(\\mu = 0\\) and \\(\\sigma = 1\\). The steps for centering and scaling using the recipes package are described above. Standardization can be done not only by centering and scaling (as described above), but also by scaling to within range, where values are scaled such that the minimum value within each variable (column) is 0 and the maximum is 1. As seen above for the feature engineering example, the object pp does not contain a standardized version of the data frame ddf_train, but rather the information that allows us to apply the same standardization also to other data. In other words, recipe(..., data = ddf_train) |&gt; step_center(...) |&gt; step_scale(...) doesn’t actually transform ddf_train. There are two more steps involved to get there. This might seem bothersome at first but their separation is critical in the context of model training and data leakage, and translates the conception of the pre-processing as a “recipe” into the way we write the code. To actually transform the data, we first have to “prepare” the recipe: pp_prep &lt;- recipes::prep(pp, training = ddf_train) Finally we can actually transform the data. That is, “juice” the prepared recipe. ddf_juiced &lt;- recipes::juice(pp_prep) Note, if we are to apply the prepared recipe to new data, we’ll have to bake() it. ddf_baked &lt;- recipes::bake(pp_prep, new_data = ddf_train) all_equal(ddf_juiced, ddf_baked) ## [1] TRUE The effect is of standardization is illustrated by comparing original and transformed variables: gg1 &lt;- ddf_train |&gt; dplyr::select(one_of(c(&quot;SW_IN_F&quot;, &quot;VPD_F&quot;, &quot;TA_F&quot;))) |&gt; tidyr::pivot_longer(cols = c(SW_IN_F, VPD_F, TA_F), names_to = &quot;var&quot;, values_to = &quot;val&quot;) |&gt; ggplot(aes(val, ..density..)) + geom_density() + facet_wrap(~var) gg2 &lt;- ddf_juiced |&gt; dplyr::select(one_of(c(&quot;SW_IN_F&quot;, &quot;VPD_F&quot;, &quot;TA_F&quot;))) |&gt; tidyr::pivot_longer(cols = c(SW_IN_F, VPD_F, TA_F), names_to = &quot;var&quot;, values_to = &quot;val&quot;) |&gt; ggplot(aes(val, ..density..)) + geom_density() + facet_wrap(~var) cowplot::plot_grid(gg1, gg2, nrow = 2) 9.2.7.2 Handling missing data Several machine learning algorithms require missing values to be removed. That is, if any of the cells in one row has a missing value, the entire cell gets removed. This can lead to severe data loss. In cases where missing values appear predominantly in only a few variables, it may be advantageous to drop the affected variable from the data for modelling. In other cases, it may be advantageous to fill missing values (data imputation, see next section). Although such imputed data is “fake”, it may be preferred to impute values than to drop entire rows and thus get the benefit of being able to use the information contained in available (real) values of affected rows. Whether or not imputation is preferred should be determined based on the model skill for an an out-of-sample test (more on that later). Visualizing missing data is the essential first step in making decisions about dropping rows with missing data versus removing predictors from the model (which would imply too much data removal). visdat::vis_miss( ddf, cluster = FALSE, warn_large_data = FALSE ) Here, the variable LW_IN_F (longwave radiation) if affected by a lot of missing data. Note that we applied a data cleaning step along with the data read-in at the very top of this Chapter. There, we applied a filtering criterion where values are only retained if at least 80% of the underlying half-hourly data is actual measured (and not gap-filled) data. Whether to drop the variable for further modelling should be informed also by our understanding of the data and the processes relevant for the modelling task. Here, the modelling target is GPP and the carbon cycle specialists among the readers may know that longwave radiation is not a known important control on GPP (ecosystem photosynthesis). Therefore, we may consider dropping this variable from the dataset for our modelling task. The remaining variables are affected by less frequent missingness with which we will deal otherwise. 9.2.7.3 Imputation Imputation refers to the replacement of missing values with with a “best guess” value (Boehmke &amp; Greenwell). Different approaches exist for determining that best guess. The most basic approach is to impute missing values with the mean or median of the available values of the same variable, which can be implemented using step_impute_*() from the {recipes} package. For example, to impute the median for all predictors separately: pp |&gt; step_impute_median(all_predictors()) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 3 ## ## Operations: ## ## Centering for all_numeric(), -all_outcomes() ## Scaling for all_numeric(), -all_outcomes() ## Median imputation for all_predictors() Imputing by the mean or median is “uninformative”. We may use information about the co-variation of multiple variables for imputing missing values. For example, for imputing missing VPD values, we may consider the fact that VPD tends to be high when air temperature is high. Therefore, missing VPD values can be modeled as a function of other co-variates (predictors). Several approaches to modelling missing values are available through the recipes package (see here). For example, we can use KNN with five neighbors as: pp |&gt; step_impute_knn(all_predictors(), neighbors = 5) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 3 ## ## Operations: ## ## Centering for all_numeric(), -all_outcomes() ## Scaling for all_numeric(), -all_outcomes() ## K-nearest neighbor imputation for all_predictors() 9.2.7.4 One-hot encoding For ML algorithms that require that all predictors be numerical (e.g., neural networks, or KNN), categorical predictors have to be pre-processed and converted into new numerical predictors. The most common such transformation is one-hot encoding, where a categorical predictor variable that has \\(N\\) levels is replaced by \\(N\\) new variables that contain either zeros or ones depending whether the value of the categorical feature corresponds to the respective column. Because this creates perfect collinearity between these new column, we can also drop one of them. This is referred to as dummy encoding. # original data frame df &lt;- tibble(id = 1:4, color = c(&quot;red&quot;, &quot;red&quot;, &quot;green&quot;, &quot;blue&quot;)) df ## # A tibble: 4 × 2 ## id color ## &lt;int&gt; &lt;chr&gt; ## 1 1 red ## 2 2 red ## 3 3 green ## 4 4 blue # after one-hot encoding dmy &lt;- dummyVars(&quot;~ .&quot;, data = df, sep = &quot;_&quot;) data.frame(predict(dmy, newdata = df)) ## id colorblue colorgreen colorred ## 1 1 0 0 1 ## 2 2 0 0 1 ## 3 3 0 1 0 ## 4 4 1 0 0 Note that in a case where color is strictly one of c(\"red\", \"red\", \"green\", \"blue\") (and not, for example, \"yellow\"), then one of the columns added by dummyVars() is obsolete (if it’s neither \"red\", nor \"green\", it must be \"blue\") - columns are collinear. This can be avoided by setting fullRank = FALSE. Using the recipes package, one-hot encoding is implemented by: recipe(GPP_NT_VUT_REF ~ ., data = ddf) |&gt; step_dummy(all_nominal(), one_hot = TRUE) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 8 ## ## Operations: ## ## Dummy variables from all_nominal() 9.2.7.5 Zero-variance predictors Sometimes, the data generation process yields variables that have the same value in each observation. And sometimes this is due to failure of the measurement device or some other bug in the data collection pipeline. Either way, this may cause some algorithms to crash or become unstable. Such “zero-variance” predictors are usually removed altogether. The same applies also to variables with “near-zero variance”. That is, variables where only a few unique values occur with a high frequency in the entire data set. The danger is that, when data is split into training and testing sets, the variable may effectively become a “zero-variance” variable within the training subset. We can test for zero-variance or near-zero variance predictors by quantifying the following metrics: Frequency ratio: Ratio of the frequency of the most common predictor over the second most common predictor. This should be near 1 for well-behaved predictors and get very large for problematic ones. Percent unique values: The number of unique values divided by the total number of rows in the data set (times 100). For problematic variables, this ratio gets small (approaches 1/100). The function nearZeroVar of the {caret} package flags suspicious variables (zeroVar = TRUE or nzv = TRUE). In our data set, we don’t find any: caret::nearZeroVar(ddf, saveMetrics = TRUE) ## freqRatio percentUnique zeroVar nzv ## TIMESTAMP 1.000000 100.000000 FALSE FALSE ## GPP_NT_VUT_REF 1.000000 93.732887 FALSE FALSE ## TA_F 1.000000 83.951932 FALSE FALSE ## SW_IN_F 1.500000 95.375723 FALSE FALSE ## LW_IN_F 1.000000 43.170064 FALSE FALSE ## VPD_F 1.142857 60.450259 FALSE FALSE ## PA_F 1.090909 37.906906 FALSE FALSE ## P_F 10.268072 5.978096 FALSE FALSE ## WS_F 1.083333 34.758138 FALSE FALSE Using the recipes package, we can add a step that removes zero-variance predictors by: pp |&gt; step_zv(all_predictors()) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 3 ## ## Operations: ## ## Centering for all_numeric(), -all_outcomes() ## Scaling for all_numeric(), -all_outcomes() ## Zero variance filter on all_predictors() 9.2.7.6 Target engineering Target engineering refers to pre-processing of the target variable. Its application can enable improved predictions, particularly for models that make assumptions about prediction errors or when the target variable follows a “special” distribution (e.g., heavily skewed distribution, or where the target variable is a fraction that is naturally bounded by 0 and 1). A simple log-transformation of the target variable can often resolve issues with skewed distributions. An implication of a log-transformation is that errors in predicting values in the upper end of the observed range are “discounted” in their weight compared to errors in the lower range. In our data set, the variable WS_F (wind speed) is skewed. The target variable that we have considered so far (GPP_NT_VUT_REF) is not skewed. In a case where we would consider WS_F to be our target variable, we would thus consider applying a log-transformation. gg1 &lt;- ddf |&gt; ggplot(aes(x = WS_F, y = ..density..)) + geom_histogram() + labs(title = &quot;Original&quot;) gg2 &lt;- ddf |&gt; ggplot(aes(x = log(WS_F), y = ..density..)) + geom_histogram() + labs(title = &quot;Log-transformed&quot;) cowplot::plot_grid(gg1, gg2) Log transformation as part of the pre-processing is specified using the step_log() function, here applied to the model target variable (all_outcomes()). recipes::recipe(WS_F ~ ., data = ddf) |&gt; # it&#39;s of course non-sense to model wind speed like this recipes::step_log(all_outcomes()) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 8 ## ## Operations: ## ## Log transformation on all_outcomes() A log-transformation doesn’t necessarily result in a perfect normal distribution of transformed values. The Box-Cox can get us closer. It can be considered a generalization of the log-transformation. Values are transformed according to the following function: \\[ y(\\lambda) = \\begin{cases} \\frac{Y^\\lambda-1}{\\lambda}, &amp;\\; y \\neq 0\\\\ \\log(Y), &amp;\\; y = 0 \\end{cases} \\] \\(\\lambda\\) is treated as a parameter that is fitted such that the resulting distribution of values \\(Y\\) approaches the normal distribution. To specify a Box-Cox-transformation as part of the pre-processing, we can use step_BoxCox() from the {recipes} package. pp &lt;- recipe(WS_F ~ ., data = ddf_train) |&gt; step_BoxCox(all_outcomes()) How do transformed values look like? prep_pp &lt;- prep(pp, training = ddf_train |&gt; drop_na()) ddf_baked &lt;- bake(prep_pp, new_data = ddf_test |&gt; drop_na()) ddf_baked |&gt; ggplot(aes(x = WS_F, y = ..density..)) + geom_histogram() + labs(title = &quot;Box-Cox-transformed&quot;) Note that the Box-Cox-transformation can only be applied to values that are strictly positive. In our example, wind speed (WS_F) is. If this is not satisfied, a Yeo-Johnson transformation can be applied. recipe(WS_F ~ ., data = ddf) |&gt; step_YeoJohnson(all_outcomes()) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 8 ## ## Operations: ## ## Yeo-Johnson transformation on all_outcomes() 9.2.8 Putting it all together (half-way) Let’s recap. We have a dataset ddf and we want to predict ecosystem GPP (GPP_NT_VUT_REF) from a set of predictors - environmental covariates that were measured in parallel to GPP. Let’s compare the performance of a multivariate linear regression and KNN model in terms of its generalisation to data that was not used for model fitting. The following pieces are implemented: Missing data: We’ve seen that the predictor LW_IN_F has lots of missing values and - given a priori knowledge is not critical for predicting GPP and we’ll drop it. Data cleaning: Data (ddf) was cleaned based on quality control information upon reading the data at the beginning of this Chapter. Before modelling, we’re checking the distribution of the target value here again to make sure it is “well-behaved”. Imputation: We drop rows with missing data for model training, instead of imputing them. Some of the predictors are distintively not normally distributed. Let’s Box-Cox transform all predictors as a pre-processing step. We have to standardize the data in order to use it for KNN. We have no variable where zero-variance was detected and we have no categorical variables that have to be transformed by one-hot encoding to be used in KNN. We use a data split, whithholding 30% for testing. Take \\(k=10\\) for the KNN model. Other choices are possible and will affect the prediction error on the training and the testing data in different manners. We’ll learn more about the optimal choice of \\(k\\) (hyperparameter tuning) in the next chapter. Fit models to minimize the root mean square error (RMSE) between predictions and observations. More on the choice of the metric argument in train() (loss function) in the next chapter. For the KNN model, use \\(k=5\\). These steps are implemented by the code below. # Data cleaning: looks ok, no obviously bad data # no long tail, therefore no further target engineering ddf |&gt; ggplot(aes(x = GPP_NT_VUT_REF, y = ..count..)) + geom_histogram() # Data splitting set.seed(1982) # for reproducibility split &lt;- rsample::initial_split(ddf, prop = 0.7, strata = &quot;VPD_F&quot;) ddf_train &lt;- rsample::training(split) ddf_test &lt;- rsample::testing(split) # Model and pre-processing formulation, use all variables but LW_IN_F pp &lt;- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F + PA_F + P_F + WS_F, data = ddf_train |&gt; drop_na()) |&gt; recipes::step_BoxCox(all_predictors()) |&gt; recipes::step_center(all_numeric(), -all_outcomes()) |&gt; recipes::step_scale(all_numeric(), -all_outcomes()) # Fit linear regression model mod_lm &lt;- caret::train( pp, data = ddf_train |&gt; drop_na(), method = &quot;lm&quot;, trControl = caret::trainControl(method = &quot;none&quot;), metric = &quot;RMSE&quot; ) # Fit KNN model mod_knn &lt;- caret::train( pp, data = ddf_train |&gt; drop_na(), method = &quot;knn&quot;, trControl = caret::trainControl(method = &quot;none&quot;), tuneGrid = data.frame(k = 5), metric = &quot;RMSE&quot; ) We can use the model objects mod_lm and mod_knn to add the fitted values to the training data and add the predicted values to the test data, both using the generic function predict(..., newdata = ...). The code below implements the prediction step, the measuring of the prediction skill, and the visualisation of predicted versus observed values on the test and training sets, bundled into one function - eval_model() - which we will re-use for each fitted model object. # make model evaluation into a function to reuse code eval_model &lt;- function(mod, df_train, df_test){ # add predictions to the data frames df_train &lt;- df_train %&gt;% drop_na() %&gt;% mutate(fitted = predict(mod, newdata = .)) df_test &lt;- df_test %&gt;% drop_na() %&gt;% mutate(fitted = predict(mod, newdata = .)) # get metrics tables metrics_train &lt;- df_train %&gt;% yardstick::metrics(GPP_NT_VUT_REF, fitted) metrics_test &lt;- df_test %&gt;% yardstick::metrics(GPP_NT_VUT_REF, fitted) # extract values from metrics tables rmse_train &lt;- metrics_train %&gt;% filter(.metric == &quot;rmse&quot;) %&gt;% pull(.estimate) rsq_train &lt;- metrics_train %&gt;% filter(.metric == &quot;rsq&quot;) %&gt;% pull(.estimate) rmse_test &lt;- metrics_test %&gt;% filter(.metric == &quot;rmse&quot;) %&gt;% pull(.estimate) rsq_test &lt;- metrics_test %&gt;% filter(.metric == &quot;rsq&quot;) %&gt;% pull(.estimate) # visualise as a scatterplot # adding information of metrics as sub-titles gg1 &lt;- df_train %&gt;% ggplot(aes(GPP_NT_VUT_REF, fitted)) + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;) + geom_abline(slope = 1, intercept = 0, linetype = &quot;dotted&quot;) + labs(subtitle = bquote( italic(R)^2 == .(format(rsq_train, digits = 2)) ~~ RMSE == .(format(rmse_train, digits = 3))), title = &quot;Training set&quot;) + theme_classic() gg2 &lt;- df_test %&gt;% ggplot(aes(GPP_NT_VUT_REF, fitted)) + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;) + geom_abline(slope = 1, intercept = 0, linetype = &quot;dotted&quot;) + labs(subtitle = bquote( italic(R)^2 == .(format(rsq_test, digits = 2)) ~~ RMSE == .(format(rmse_test, digits = 3))), title = &quot;Test set&quot;) + theme_classic() out &lt;- cowplot::plot_grid(gg1, gg2) return(out) } # linear regression model eval_model(mod = mod_lm, df_train = ddf_train, df_test = ddf_test) # KNN eval_model(mod = mod_knn, df_train = ddf_train, df_test = ddf_test) It is advisable to keep workflow notebooks (this RMarkdown file) light and legible. Therefore, code chunks should not be excessively long and functions should be kept in a ./R/*.R file, which can be loaded. This also facilitates debugging code inside the function. Here, the function eval_model() is part of the book’s git repository, stored in the sub-directory ./R/, and used also in later chapters. 9.3 Report Exercises The last figures of this tutorial show the training of testing of a linear model and a KNN model. Compare the four figures and answer to following questions: How does the point cloud differ between the two training set plots? Why does the \\(R^2\\) value increase from LM to KNN? Use your knowledge on the fundamental difference between the two fitted models! How does the performance of both models (LM and KNN) change when predicting onto the test set? Why do they change in that manner? The test set plots for both models show a tendency to have more points on the right side of the dotted one-to-one line. What does that tell you about the performance of the model? How do you interpret this model performance in an ecophysiological manner? Note that we are comparing predicted (y-axis) against observed (x-axis) values of gross primary production. Let’s look at the role of \\(k\\) in a KNN. Answer the following questions: Write a hypothesis based on the following questions: What do you expect, how do the model metrics of both sets, the training set and the test set, change if you increase or decrease \\(k\\)? Connect this question to what you learned about the bias-variance trade-off! Put your hypothesis to the test! Write a code script that repeats fitting different values for \\(k\\) as done in the tutorial. Use a simple bivariate model, regressing temperature against the day-of-year in the ddf dataset (DOY ~ TA_F). Visualise your results in an intuitive plot that is understandable without additional explanation. To get the DOY from a date, you can use lubridate::yday(). Compare your results against your hypothesis from 2a. Is the result as you expected? If your hypothesis was right, can you explain further, why your hypothesis was correct, using the results. If your hypothesis was wrong, can you explain why? What surprised you, what did you not expect to see? "],["supervisedmlii.html", "Chapter 10 Supervised machine learning II 10.1 Learning objectives 10.2 Tutorial 10.3 Report Exercises", " Chapter 10 Supervised machine learning II Chapter lead author: Benjamin Stocker 10.1 Learning objectives In the Chapter 9, you learned how the data are pre-processed, the model fitted, and how the model’s generasbility to unseen data is tested. In the exercises of Chapter 9, you learned how the bias-variance trade-off of a model, a KNN, can be controlled and that the choice of model complexity has different implications of the model’s performance on the training and the test sets. A “good” model generalises well. That is, it performs well on unseen data. In this chapter, you will learn more about the process of model training, the concept of the loss, and how we can chose the right level of model complexity for optimal model generalisability as part of the model training step. This completes your set of skills for your first implementations a supervised machine learning workflow. 10.2 Tutorial 10.2.1 Data and the modelling challenge We’re using the same data and address the same modelling challenge as in Chapter 9. Let’s load the data, wrangle a bit, specify the same model formulation, and the same pre-processing steps as in the previous Chapter 9. ddf &lt;- read_csv(&quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot;) |&gt; # select only the variables we are interested in dplyr::select(TIMESTAMP, GPP_NT_VUT_REF, # the target ends_with(&quot;_QC&quot;), # quality control info ends_with(&quot;_F&quot;), # includes all all meteorological covariates -contains(&quot;JSB&quot;) # weird useless variable ) |&gt; # convert to a nice date object dplyr::mutate(TIMESTAMP = ymd(TIMESTAMP)) |&gt; # set all -9999 to NA dplyr::na_if(-9999) |&gt; # retain only data based on &gt;=80% good-quality measurements # overwrite bad data with NA (not dropping rows) dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC &lt; 0.8, NA, GPP_NT_VUT_REF), TA_F = ifelse(TA_F_QC &lt; 0.8, NA, TA_F), SW_IN_F = ifelse(SW_IN_F_QC &lt; 0.8, NA, SW_IN_F), LW_IN_F = ifelse(LW_IN_F_QC &lt; 0.8, NA, LW_IN_F), VPD_F = ifelse(VPD_F_QC &lt; 0.8, NA, VPD_F), PA_F = ifelse(PA_F_QC &lt; 0.8, NA, PA_F), P_F = ifelse(P_F_QC &lt; 0.8, NA, P_F), WS_F = ifelse(WS_F_QC &lt; 0.8, NA, WS_F)) |&gt; # drop QC variables (no longer needed) dplyr::select(-ends_with(&quot;_QC&quot;)) # Data splitting set.seed(123) # for reproducibility split &lt;- rsample::initial_split(ddf, prop = 0.7, strata = &quot;VPD_F&quot;) ddf_train &lt;- rsample::training(split) ddf_test &lt;- rsample::testing(split) # The same model formulation is in the previous chapter pp &lt;- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = ddf_train) |&gt; recipes::step_center(all_numeric(), -all_outcomes()) |&gt; recipes::step_scale(all_numeric(), -all_outcomes()) 10.2.2 Loss function Model training in supervised machine learning is guided by the match (or mismatch) between the predicted and observed target variable(s), that is, between \\(\\hat{Y}\\) and \\(Y\\). The loss function quantifies this mismatch (\\(L(\\hat{Y}, Y)\\)), and the algorithm (“optimiser” in Fig. XXX of Chapter 9) takes care of progressively reducing the loss during model training. Let’s say the machine learning model contains two parameters and predictions can be considered a function of the two (\\(\\hat{Y}(w_1, w_2)\\)). \\(Y\\) is actually constant. Thus, the loss function is effectively a function \\(L(w_1, w_2)\\). Therefore, we can consider the model training as a search of the parameter space to find the minimum of the loss. The parameter space spanned by all possible combinations of \\((w_1, w_2)\\). Common loss functions are the root mean square error (RMSE), or the mean square error (MSE), or the mean absolute error (MAE). Loss minimization is a general feature of ML model training. Visualization of a loss function Model training is implemented in R for different machine learning algorithms in different packages. Some algorithms are even implemented by multiple packages. As described in Chapter @ref(#supervisedmli), the {caret} package provides “wrappers” that handle a large selection of different machine learning model implementations in different packages with a unified interface (see here for an overview of available models). The {caret} function train() is the center piece also in terms of specifying the loss function as the argument metric. It defaults to the RMSE for regression models and the accuracy for classification. 10.2.3 Hyperparameters Practically all machine learning algorithms have some “knobs” to turn for controlling a model’s complexity and other features of the model training. The optimal choice of these “knobs” is to be found for efficient model performance. Such “knobs” are the hyperparameters. Each algorithm comes with its own, specific hyperparameters. For KNN, k is the (only) hyperparameter. It specifies the number of neighbours to consider for determining distances. There is always an optimum \\(k\\). Obviously, if \\(k = n\\), we consider all observations as neighbours and each prediction is simply the mean of all observed target values \\(Y\\), irrespective of the predictor values. This cannot be optimal and such a model is likely underfit. On the other extreme, with \\(k = 1\\), the model will be strongly affected by the noise in the single nearest neighbour and its generalisability will suffer. This should be reflected in a poor performance on the validation data. Hyperparameters usually have to be “tuned”. The optimal setting depends on the data and can therefore not be known a priori. # load our custom evaluation function source(&quot;R/eval_model.R&quot;) # specify the set of K df_k &lt;- data.frame(k = c(2, 5, 10, 15, 20, 25, 30, 35, 40, 60, 100, 200, 300)) |&gt; mutate(idx = 1:n()) # model training for the specified set of K list_mod_knn &lt;- purrr::map( df_k$k, ~caret::train(pp, data = ddf_train |&gt; drop_na(), method = &quot;knn&quot;, trControl = caret::trainControl(method = &quot;none&quot;), tuneGrid = data.frame(k = .), # &#39;.&#39; evaulates k metric = &quot;RMSE&quot;)) list_metrics &lt;- purrr::map( list_mod_knn, ~eval_model(., ddf_train |&gt; drop_na(), ddf_test, return_metrics = TRUE)) # extract metrics on training data list_metrics_train &lt;- purrr::map( list_metrics, &quot;train&quot;) |&gt; # add K to the data frame bind_rows(.id = &quot;idx&quot;) |&gt; mutate(idx = as.numeric(idx)) |&gt; left_join(df_k, by = &quot;idx&quot;) # extract metrics on testing data list_metrics_test &lt;- purrr::map( list_metrics, &quot;test&quot;) |&gt; # add K to the data frame bind_rows(.id = &quot;idx&quot;) |&gt; mutate(idx = as.numeric(idx)) |&gt; left_join(df_k, by = &quot;idx&quot;) # prepare for visualisation df_mae &lt;- list_metrics_train |&gt; filter(.metric == &quot;mae&quot;) |&gt; mutate(set = &quot;train&quot;) |&gt; bind_rows( list_metrics_test |&gt; filter(.metric == &quot;mae&quot;) |&gt; mutate(set = &quot;test&quot;) ) # visualise df_mae |&gt; ggplot(aes(x = k, y = .estimate, color = set)) + geom_point() + geom_line() The workflow implemented above should remind you of your the exercise in Chapter 9. It demonstrates that the model performance on the training set keeps improving as the model variance (as opposed to bias) increases - here as we go towards smaller \\(k\\). However, what counts for measuring out-of-sample model performance is the evaluation on the test set, which deteriorates with increasing model variance beyond a certain point. Although decisive for the generalisability of the model, we cannot evaluate its performance on the test set during model training. We have set that data aside and must leave it untouched to have a basis for evaluating the model performance on unseen data after training. What can we do to determine the optimal hyperparameter choice during model training, estimating its performance on the test set? 10.2.4 Resampling The solution is to split the training data again - now into a training and a validation set. Using the validation set, we can “mimick” out-of-sample predictions during training by determining the loss on the validation set and use that for guiding the model training. However, depending on the volume of data we have access to, evaluation metrics determined on the validation set may not be robust. Results may vary depending on the split into training and validation data. This makes it challenging for reliably estimate out-of-sample performance. A solution for this problem is to resample the training into a number training-validation splits, yielding several pairs of training and validation data. Model training is then guided by minimising the average loss determined on the different resamples. Having multiple resamples (multiple folds of training-validation splits) avoids the loss minimization from being misguided by random peculiarities in the training and/or validation data. Whether or not a resampling is applied depends on the data volume and computational costs of the model training which increase linearly with the number of resamples. For models that take days-weeks to train, resampling is not a realistic option. However, for many machine learning applications in Geography and Environmental Sciences, models are much less costly and resampling is viable and desirable approach to model training. The most important methods of resampling are bootstrapping (not explained here, but see Boehmke &amp; Greenwell (2020)) and k-fold cross validation. An advantage of bootstrap is that it provides an estimation of the distribution of the training error (without the need for data distribution assumptions because it’s non parametric), which informs not only the magnitude of the training error but also the variance of such estimate. Nevertheless, this statistical approach can become very computationally intensive because it needs many resamples with replacement and model runs. Hence cross validation lends itself more to model training. In k-fold cross validation, the training data is split into k equally sized subsets (folds). (Don’t confuse this k with the k in KNN.) Then, there will be k iterations, where each fold is used for validation once (while the remaining folds are used for training). An extreme case is leave-one-out cross validation, where k corresponds to the number of data points. K-fold cross validation. From Boehmke &amp; Greenwell (2020). To do a k-fold cross validation during model training in R, we don’t have to implement the loops around folds ourselves. The resampling procedure can be specified in the {caret} function train() with the argument trControl. The object that this argument takes is the output of a function call to trainControl(). This can be implemented in two steps. For example, to do a 10-fold cross-validation, we can write: set.seed(1982) mod_cv &lt;- caret::train(pp, data = ddf_train |&gt; drop_na(), method = &quot;knn&quot;, trControl = caret::trainControl(method = &quot;cv&quot;, number = 10), tuneGrid = data.frame(k = c(2, 5, 10, 15, 20, 25, 30, 35, 40, 60, 100)), metric = &quot;MAE&quot;) # generic plot of the caret model object ggplot(mod_cv) # generic print print(mod_cv) ## k-Nearest Neighbors ## ## 1910 samples ## 8 predictor ## ## Recipe steps: center, scale ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 1719, 1718, 1718, 1719, 1720, 1718, ... ## Resampling results across tuning parameters: ## ## k RMSE Rsquared MAE ## 2 1.758390 0.5643266 1.331563 ## 5 1.579795 0.6321707 1.199857 ## 10 1.539660 0.6480432 1.163468 ## 15 1.522924 0.6549458 1.155488 ## 20 1.518210 0.6568319 1.152947 ## 25 1.516940 0.6574409 1.151223 ## 30 1.517698 0.6570899 1.153631 ## 35 1.517916 0.6570457 1.153421 ## 40 1.517559 0.6573507 1.153832 ## 60 1.522451 0.6556145 1.161452 ## 100 1.535410 0.6508217 1.177483 ## ## MAE was used to select the optimal model using the smallest value. ## The final value used for the model was k = 25. From the output of print(mod_cv), we get information about model performance for each hyperparameter choice. The values reported are means of respective metrics determined across the ten folds. Also the optimal choice of \\(k\\) (25) is reported. Does this correspond to the \\(k\\) with the best performance determined on the test set? If resampling was a good approach to estimating out-of-sample model performance, then it should! df_mae |&gt; filter(set == &quot;test&quot;) |&gt; filter(.estimate == min(.estimate)) ## # A tibble: 1 × 6 ## idx .metric .estimator .estimate k set ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 7 mae standard 1.15 30 test The evaluation on the test set suggests that \\(k=30\\) is optimal, while 10-fold cross-validation yielded an optimal \\(k = 25\\). Apparently, cross-validation suggested a model that is slightly more on the variance side along the bias-variance trade-off than the evaluation on the test set did. This (relatively small) mismatch is primarily a result of randomness in the data. Let’s look at the results as we did in Chapter 9. The model object mod_cv contains information about the whole hyperparameter search and also about the choice of the best hyperparameter value. When using the object in the predict() function (as used inside eval_model()), it automatically uses the model trained with the optimal \\(k\\). eval_model(mod = mod_cv, df_train = ddf_train, df_test = ddf_test) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; Remember that in Chapter 9, we used \\(k=5\\) and got an \\(R^2=0.74\\) on the training set and \\(R^2=0.61\\) on the test set. The results with the optimal choice of \\(k=25\\) yield an \\(R^2=0.69\\) on the training set and \\(R^2=0.65\\) on the test set - poorer than with \\(k=5\\) on the training set but better on the test set. 10.2.5 Validation versus testing data A source of confusion can be the distinction of validation and testing data. They are different things. The validation data is used during model training. The model fitting and the selection of the optimal hyperparameters is based on comparing predictions with the validation data. Hence, a evaluations of true out-of-sample predictions should be done with a portion of the data that has never been used during the model training process (see Figure below). Figure adopted form Google Machine Learning Crash Course 10.2.6 Modeling with structured data A fundamental assumption underlying many machine learning algorithms and their training setup is that the data are independent and identically distributed (iid). This means that each observation is generated by the same process, follows the same distribution, and is independent from its neighboring point or any other data point. This assumption is often made in statistical models to simplify the analysis and to ensure the validity of various mathematical results used in machine learning algorithms. In reality, this is often not satisfied. In Chapter 3, we dealt with structure in the data in the context of formatting and aggregating data frames. Such structures are often also relevant for modelling and what it means for a model to “generalize well”. Remember that structure in data arises from similarity of the subjects generating the data. Such structures and their relation to the modelling task should be considered when choosing the model algorithm, formulating the model, and when implementing the training a testing setup. Consider, for example, the time series of ecosystem fluxes and meterological covariates in our dataset ddf. When using this data to train a KNN or a linear regression model, we implicitly assume that the data is iid. We assumed that there is a true function \\(f(X)\\) that generates the target data \\(Y\\) and that can be used to predict under any novel condition \\(X_\\text{new}\\). However, in reality, this may not be the case. \\(f(X)\\) may change over time. For example, over the course of a season, the physiology of plants changes (think phenology) and may lead to temporally varying relationships between \\(X\\) and \\(Y\\) that are not captured by temporal variations in \\(X\\) itself - the relationships are not stationary. Working with geographic and environmental data, we often deal with temporal dependencies between predictors and the target data. In our data ddf of ecosystem fluxes and meteorological covariates, this may be, as mentioned, arising from phenological changes in plant physiology and structure, or caused by a lasting effects weather extremes (e.g., a late frost event in spring). In hydrological data, temporal dependencies between weather and streamflow are generated by catchment characteristics and the runoff generation processes. Such temporal dependencies violate the independence assumption. Certain machine learning algorithms (e.g., recurrent neural networks) offer a solution for such cases and are suited for modelling temporal dependencies or, in general, sequential data where the order of the records matters (e.g., language models that consider the order of words in a text). Training-testing and cross-validation splits for sequential data have to preserve the order of the data in the subsets. In this case, the splits have to be done by blocks. That is, model generalisability is to be assessed by training on one block of the time series and testing on the remaining block. Note that the splitting method introduced in Chapter 9 using rsample::initial_split() assumes that the data is iid. In the function, data points are randomly drawn and allocated to either the test or the training subset. It is therefore not applicable for splitting data with temporal dependencies. Modelling temporal dependencies will be dealt in future (not currently available) chapters of this book. Other dependencies may arise from the spatial context. For example, a model for classifying an atmospheric pressure field as a high or a low pressure system uses information about the spatial arrangement of the data - in this case raster data. A model predicts one value (‘high pressure system’ or ‘low pressure system’) per pressure field. Such modelling tasks are dealt with yet another class of algorithms (e.g., convolutional neural networks). Spatial or group-related structure in the data may arise if, in general, the processes generating the data, cannot be assumed to be identical and lead to identically distributed data across groups. For example, in the data ddf_allsites_nested_joined from Chapter 3, time series of ecosystem fluxes and meteorological covariates are provided for a set of different sites. There, the group structure is linked to site identity. Similarly, streamflow data may be available for multiple catchments. However, considering the between-catchment variations in soil, terrain, vegetation, and geology, a model may not yield accurate predictions when trained by data from one catchment and applied to a different catchment. To evaluate model generalisability to a new site or catchment (not just a new time step within a single site or catchment), this structure has to be taken into consideration. In this, case, data splits of training and validation or testing subsets are to be separated along blocks, delineated by the similar groups of data points (by sites, or by catchments). That is, training data from a given site (or catchment) should be either in the training set or in the test (or validation) set, but not in both. This illustrates that the data structure and the modelling aim (generalisability in what respect?) have to be accounted for when designing the data split and resampling strategy. The {caret} function groupKFold() offers the solution for such cases, creating folds for cross-validation that respect group delineations. In other cases, such grouping structure may not be evident and may not be reflected by information accessible for modelling. For example, we may be able to separate time series from different sites but we don’t know whether sites are sufficiently independent to be able to consider the test metric to reflect the true uncertainty in predicting to an entirely new location which is neither in the test nor training set. In such cases, creative solutions have to be found and appropriate cross-validations have to be implemented with a view to the data structure and modelling aim. 10.3 Report Exercises In this tutorial’s exercise, you will explore the key idea of generalisability! The tutorial holds all steps to train and test on data from the Davos FLUXNET site (FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv). Repeat these steps but with data from the Laegern site (FLX_CH-Lae_FLUXNET2015_FULLSET_DD_2004-2014_1-4.csv). Now, train a model on the Davos site and test it on the Laegern site, and vice versa (train on Laegern, test on Davos). Answer the following questions: How do the model metric on the test set change, when you are training and testing on the same or on different datasets? What are the differences between Davos and Laegern that could cause the pattern that you found? What do you expect when training and testing on Davos and Laegern simultaneously? Is this a valid approach? Use your new knowledge to reason for or against a situation where you would train on both sites. "],["randomforest.html", "Chapter 11 Random Forest 11.1 Learning objectives 11.2 Tutorial 11.3 Exercises", " Chapter 11 Random Forest Chapter lead author: Benjamin Stocker 11.1 Learning objectives TBC 11.2 Tutorial 11.2.1 Decision trees Just as a forest consists of trees, a Random Forest model consists of decision trees. Therefore, let’s start with a decision tree, also known as CART (classification and regression tree). Consider a similar example as in Chapter 9 where we fitted a function \\(Y = f(X)\\) with polynomials. Instead, here we fit it with a decision tree. The tree grows by successively splitting (branching) the predictor range (\\(X\\)) into binary classes (two regions along \\(X\\)) and predicting a different and constant value \\(c\\) for the target variable on either side of the split. The location of the split is chosen such that the overall error between the observed response (\\(Y_i\\)) and the predicted constant (\\(c_i\\)) is minimized. The error is determined based on whether we’re dealing with a regression or a classification problem. For regression problems, the sum of square errors is minimized. \\[ \\text{SSE} = \\sum_i{(\\hat{Y_i}-Y_i)^2} \\] For classification problems, the cross-entropy or the Gini index are typically maximized. As a tree grows, splits are recursively added. In our example, only one predictor variable is available. The splits are therefore performed always on the same variable, splitting up \\(X\\) further. In the visualisation of the decision tree above, the uppermost decision is the root node. From there, two branches connect to internal nodes, and at the bottom of the tree are the leaf nodes. (The nature-aware reader may note that leaves are typically at the top, and roots at the bottom of a tree. Nevermind.) Typically, multiple predictor variables are available. For each split, the variable and the location of the split along that variable is determined to satisfy the respective criteria for regression and classification. Decision trees are high variance learners. That is, as the maximum tree depth is increased, the variance of predictions increases. In other words, the depth of a tree controls the model complexity and the bias-variance trade-off. With excessive depth, decision trees tend to overfit. An advantage of decision trees is that they require minimal pre-processing of the data and they are robust to outliers. This is thanks to their approach of converting continuous variables into binary classes for predictions. Hence, they can naturally handle a mix of continuous and categorical predictor variables. Furthermore, predictions are not sensitive to the distance of a predictor variable’s value to a respective variable’s split location. This makes decision trees robust to outliers. It also implies that predictions to unseen data points that lie outside the range of values in the training data (extrapolation) are conservative. The disadvantage is, as demonstrated above, the tendency towards high variance of predictions when models get complex (deep trees). And thus, decision trees tend to be outperformed by other algorithms. 11.2.2 Bagging The approach of bagging is to smooth out the high variance of decision tree predictions by averaging over multiple, slightly differet trees. Differences between the trees are introduced by bagging, that is, by training each individual tree only on a bootstrapped sample of the full training data. Here, a decision tree has the role of a base learner and bagging takes an ensemble approach. Final predictions are then taken as the average (for regression) or the most frequent class (for classification) across all trees’ predictions. Bagging is particularly effective when the base learner tends to have a high variance. The variance of predictions is continuously reduced with an increasing number of decision trees, over which averages are taken, and no tendency to overfit results from increasing the number of trees. However, the computational cost linearly increases with the number of trees and the gain in model performance levels out. Bagging also limits the interpretability. We can no longer visualise the fitted model with an intuitive graphical decision tree as done above. 11.2.3 From trees to a forest While bagging reduces the variance in predictions, limits to predictive performance remain. This is linked to the fact that, although a certain degree of randomness is introduced by sub-sampling the training data for each tree, individual trees often remain relatively similar. This is particularly expressed if variations in the target variable are controlled primarily by variations in a small number of predictor variables. In this case, decision trees tend to be built by splits on the same variable, irrespective of which bootstrapped sample the individual tree is trained with. Random Forest solves this problem by introducing an additional source of randomness: Only a subset of the predictor variables are considered at each split. This strongly reduces the similarity of individual trees (and also reduces computational costs) and leads to improved predictive performance. The number of variables to consider at each split is a hyperparameter of the Random Forest algorithm, commonly named \\(m_\\text{try}\\). In the example below, we use the implementation in the {ranger} package (wrapped with {caret}), where the respective model fitting function has a hyperparameter mtry. Common default values are \\(m_\\text{try} = P/3\\) for regression and \\(m_\\text{try} = \\sqrt{P}\\) for classification, where \\(P\\) is the number of predictors. Model complexity is controlled by the depth of the trees. Depending on the implementation of the Random Forest algorithm, this is governed not by explicitly specifying the tree depth, but by setting the number of observations in the leaf node. In the {ranger} package, the respective hyperparamter is min.node.size. The number of trees is another hyperparameter and affects predictions similarly as described above for bagging. A great strength of Random Forest is, inherited by the characteristics of its underlying decision trees, its minimal requirement of data pre-processing, its capability of dealing with continuous and categorical variables, and its robustness to outliers. With the default choices of \\(m_\\text{try}\\), Random Forest provides very good out-of-the-box performance. However, the hyperparameters of Random Forest have interactive effects and should be searched systematically. source(&quot;R/eval_model.R&quot;) ddf &lt;- read_csv(&quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot;) |&gt; # select only the variables we are interested in dplyr::select(TIMESTAMP, GPP_NT_VUT_REF, # the target ends_with(&quot;_QC&quot;), # quality control info ends_with(&quot;_F&quot;), # includes all all meteorological covariates -contains(&quot;JSB&quot;) # weird useless variable ) |&gt; # convert to a nice date object dplyr::mutate(TIMESTAMP = ymd(TIMESTAMP)) |&gt; # set all -9999 to NA dplyr::na_if(-9999) |&gt; # retain only data based on &gt;=80% good-quality measurements # overwrite bad data with NA (not dropping rows) dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC &lt; 0.8, NA, GPP_NT_VUT_REF), TA_F = ifelse(TA_F_QC &lt; 0.8, NA, TA_F), SW_IN_F = ifelse(SW_IN_F_QC &lt; 0.8, NA, SW_IN_F), LW_IN_F = ifelse(LW_IN_F_QC &lt; 0.8, NA, LW_IN_F), VPD_F = ifelse(VPD_F_QC &lt; 0.8, NA, VPD_F), PA_F = ifelse(PA_F_QC &lt; 0.8, NA, PA_F), P_F = ifelse(P_F_QC &lt; 0.8, NA, P_F), WS_F = ifelse(WS_F_QC &lt; 0.8, NA, WS_F)) |&gt; # drop QC variables (no longer needed) dplyr::select(-ends_with(&quot;_QC&quot;)) ## Rows: 6574 Columns: 334 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (334): TIMESTAMP, TA_F_MDS, TA_F_MDS_QC, TA_F_MDS_NIGHT, TA_F_MDS_NIGHT_... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Data splitting set.seed(123) # for reproducibility split &lt;- rsample::initial_split(ddf, prop = 0.7, strata = &quot;VPD_F&quot;) ddf_train &lt;- rsample::training(split) ddf_test &lt;- rsample::testing(split) # The same model formulation is in the previous chapter pp &lt;- recipes::recipe(GPP_NT_VUT_REF ~ TA_F + SW_IN_F + LW_IN_F + VPD_F + P_F + WS_F, data = ddf_train) |&gt; recipes::step_center(all_numeric(), -all_outcomes()) |&gt; recipes::step_scale(all_numeric(), -all_outcomes()) mod &lt;- train( pp, data = ddf_train %&gt;% drop_na(), method = &quot;ranger&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5, savePredictions = &quot;final&quot;), tuneGrid = expand.grid( .mtry = floor(6 / 3), .min.node.size = 5, .splitrule = &quot;variance&quot;), metric = &quot;RMSE&quot;, replace = FALSE, sample.fraction = 0.5, num.trees = 2000, # high number ok since no hperparam tuning seed = 1982 # for reproducibility ) ## ## Attaching package: &#39;e1071&#39; ## ## The following object is masked from &#39;package:rsample&#39;: ## ## permutations ## ## The following object is masked from &#39;package:terra&#39;: ## ## interpolate # generic print print(mod) ## Random Forest ## ## 1910 samples ## 8 predictor ## ## Recipe steps: center, scale ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 1528, 1528, 1529, 1527, 1528 ## Resampling results: ## ## RMSE Rsquared MAE ## 1.411155 0.7047382 1.070285 ## ## Tuning parameter &#39;mtry&#39; was held constant at a value of 2 ## Tuning ## parameter &#39;splitrule&#39; was held constant at a value of variance ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 eval_model(mod = mod, df_train = ddf_train, df_test = ddf_test) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; 11.3 Exercises XXX Exercise: implement bagging 12 decision trees, each with a maximum depth of 4, for the same data as used in the demonstration above. XXX Exercise: fit a random forest model to the flux data used in previous chapters. Use a single set of hyperparameters. Same modelling task, different engine. XXXXXXXXX For random forests from the {ranger} package, hyperparameters are: mtry: the number of variables to consider to make decisions, often taken as \\(p/3\\), where \\(p\\) is the number of predictors. min.node.size: the number of data points at the “bottom” of each decision tree splitrule: the function applied to data in each branch of a tree, used for determining the goodness of a decision "],["solutions.html", "A Solutions A.1 Getting Started", " A Solutions A.1 Getting Started Dimensions of a circle Given the radius of a circle write a few lines of code that calculates its area and its circumference. Run your code with different values assigned of the radius. radius &lt;- 1 area &lt;- pi * radius^2 circum &lt;- 2 * pi * radius Print the solution as text. print(paste(&quot;Radius:&quot;, radius, &quot; Circumference:&quot;, circum)) ## [1] &quot;Radius: 1 Circumference: 6.28318530717959&quot; Sequence of numbers Generate a sequence of numbers from 0 and \\(\\pi\\) as a vector with length 5. seq(0, pi, length.out = 5) ## [1] 0.0000000 0.7853982 1.5707963 2.3561945 3.1415927 Gauss sum Rumors have it that young Carl Friedrich Gauss was asked in primary school to calculate the sum of all natural numbers between 1 and 100. He did it in his head in no time. We’re very likely not as intelligent as young Gauss. But we have R. What’s the solution? sum(1:100) ## [1] 5050 Gauss calculated the sum with a trick. The sum of 100 and 1 is 101. The sum of 99 and 2 is 101. You do this 50 times, and you get \\(50 \\times 101\\). Demonstrate Gauss’ trick with vectors in R. vec_a &lt;- 1:50 vec_b &lt;- 100:51 vec_c &lt;- vec_a + vec_b # each element is 101 vec_c ## [1] 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 ## [20] 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 ## [39] 101 101 101 101 101 101 101 101 101 101 101 101 # the length of vectors is fifty. 50 * 101 sum(vec_c) ## [1] 5050 Magic trick algorithm Define a variable named x that contains an integer value and perform the following operations in sequence: Redefine x by adding 1. Double the resulting number, over-writing x. Add 4 to x and save the result as x. Redefine x as half of the previous value of x. Subtract the originally chosen arbitrary number from x. Print x. Restart the algorithm defined above by choosing a new arbitrary natural number. x &lt;- -999 # arbitrary number between 1 and 20 x_save &lt;- x # save for the last step x &lt;- x + 1 x &lt;- x * 2 x &lt;- x + 4 x &lt;- x / 2 x - x_save ## [1] 3 Vectors Print the object datasets::rivers and consult the manual of this object. What is the class of the object? What is the length of the object? Calculate the mean, median, minimum, maximum, and the 33%-quantile across all values. class(datasets::rivers) ## [1] &quot;numeric&quot; length(datasets::rivers) ## [1] 141 mean(datasets::rivers) ## [1] 591.1844 # other functions can easily be found on the internet ;-) Data frames Print the object datasets::quakes and consult the manual of this object. Determine the dimensions of the data frame using the respective function in R. Extract the vector of values in the data frame that contain information about the Richter Magnitude. Determine the value largest value in the vector of event magnitudes. Determine the geographic position of the epicenter of the largest event. dim(datasets::quakes) ## [1] 1000 5 vec &lt;- datasets::quakes$mag max(vec) ## [1] 6.4 idx &lt;- which.max(vec) # index of largest value # geographic positions defined by longitude and latitude (columns long and lat) datasets::quakes$long[idx] ## [1] 167.62 datasets::quakes$lat[idx] ## [1] -15.56 RMarkdown Create an RMarkdown file and implement your solutions to above exercises in it. Give the file a title, implement some structure in the document, and write some text explaining what your code does. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
